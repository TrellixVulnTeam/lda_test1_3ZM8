1,Artemy-Mellanox,I would pass qp_loop struct to all this functions and store  CI's and stuff inside.  Also why inline it?
2,alex-mikheev,imho it is better to change nvme_squeue_t *nvme_sq to something like uint32_t db_offset  so that low level api does not depend on high level definitions
3,Artemy-Mellanox,Yes  but we may want to pass some struct - there will be more nvme SQ parameters and also need out parameter  id  to use it in  unmap  command.
4,alex-mikheev,Lets pass a struct instead of nvme_sq
5,Artemy-Mellanox,print as  d
6,orendu,Let's call it bar registers space.
7,orendu,Eventually the address range needs to be configurable...
8,orendu,Why do we need db_addr if a SQ specific context is already supplied?
9,orendu,Should get this from device capabilities
10,orendu,...and go through init flow of the nvme function(s)?
11,orendu,what actually happens here?
12,orendu,s/worder/worker/
13,orendu,e?
14,orendu,maybe just give sqe pointer?
15,orendu,This is not used... I think the doorbell write callback should be property of creating sq.  Imagine multiple types of SQs that you'd want different handlers for them.  For instance  admin_q vs. io_q  (not must in nvme  there is another way to diff between admin/io queues  but i think it is nice)  
16,alex-mikheev,i will add attribute to the mlx_emu_dev_attr_t
17,alex-mikheev,you are right... 
18,alex-mikheev,so may be i should add something mlx_emu_dev_query(char *dev_name  mlx_emu_dev_qattr_t *) ?   It will return number of functions  max bar register space size etc
19,alex-mikheev,yes it will also do an init flow
20,alex-mikheev,revert mlx_emu_dev_open()  try to gracefully close all open resources and teardown emulated device
21,alex-mikheev,Yeah it is more generic. I will update .h file
22,orendu,the question was on worker_create()...
23,Artemy-Mellanox,Let's allow this to pass - otherwise will need to reboot after each execution.
24,alex-mikheev,let's move this the separate function ex sq_ring_db()
25,alex-mikheev,lets have a separate function for the cq db ringing (cq_ring_db)  also it is a good time to have a separate function that checks for completion(s) and optionally can ring cq doorbell  
26,alex-mikheev,also lets add a test in a separate PR that does following:  - submit full SQ of commands  - polls/checks for completions  but do not ring cq doorbell  - submit another full SQ of commands.   - check for completions - there should be none because there is no room in CQ  - ring cq doorbell  - check for completions - all commands must complete 
27,alex-mikheev,I think that you still have to do process_sq() after taking care of req_list
28,orendu,Are we going to have a complicated DB callback? otherwise let's drop the  simple ...
29,orendu,Let's drop the  only . We have  DB mode . Since it is used in the cb_type field  let's name this MLX_EMU_CB_TYPE_DB
30,orendu,Let's call it mlx_emu_db_mode_cb_t or alike. I want to drop all the  basic  and  simple' adjectives...
31,orendu,Is this used? I think FW did not implement registers change event  did they?
32,orendu,Is that how many QPs can be mapped to doorbells FW limit?  If so  it will be true also for SQE QPs  etc. So maybe it should be names  max mapped qps ?
33,orendu,Assuming 32bit doorbells?
34,orendu,Why do we need these on/off? If queue was created  with db mode  it will start sending DBs. Isn't that enough?  
35,mike-dubman,rm -f 
36,mike-dubman,do you want to override default  install  target ? why? if it is only to avoid build gtest which may be unavailable - you need to borrow from ucx ```./configure --enable-gtest``` logic
37,mike-dubman,tarball 
38,mike-dubman,```sh -eE``` to catch silent errors
39,mike-dubman,what does it mean? ``` 1 -eq 2``` ?
40,mike-dubman,please do not use single letter var names  it is hard to search it in the vim  root_dir?
41,mike-dubman,do we pack well-knows ssh keys into image? why?  can be security issue.
42,mike-dubman,-eE
43,mike-dubman,where cross compile is installed?  does it mean it can run only from that machine?  maybe need to ask michael to install it into /hpc/local?  also need check if cross compile path exit and error/die if not.
44,mike-dubman,-eE
45,mike-dubman,does it make sense to take size from getenv and default to 512?
46,alex-mikheev,Ucx does not configure gtest as git submodule. So install target is not applied to gtest.   So far it is my best idea is to override install for the gtest.    Also there seems to be a problem with the make check as it descends into gtest. Probably it is best to skip it in jenkins
47,alex-mikheev,duh it is ( if 0) equivalent to skip some steps during test runs
48,alex-mikheev,those are the standard host keys that come on the original image. I am doing it to enable ssh into the machine by default
49,alex-mikheev,Currently i only have it locally on rsws36. I guess it is a good idea to install free electrons cross compilers in //hpc/local. UCX can also use it.
50,alex-mikheev,i would not bother as this code will be replaced with a more generic backend/block device layer.  
51,mike-dubman,I think this method https://github.com/openucx/ucx/blob/master/config/m4/gtest.m4 should work and use HAVE_GTEST macro to enable/disable gtest folder addon to SUBDIRS?
52,mike-dubman,@MrBr-github - please note  lets add it to  non-urgent  tasks
53,yshestakov,`rm -rf` what?
54,yshestakov,`sshd` could re-generate new SSH keys on the first start.  So it's better to not include SSH host keys into the archive
55,yshestakov,hardcoded kernel version. Is it OK?
56,alex-mikheev,yeah  but i suppose it should be addressed by the Yevgeny Petrilin. 
57,alex-mikheev,+1 good one 
58,alex-mikheev,at this stage yes. 
59,alex-mikheev,typo: ^iemulated^emulated
60,alex-mikheev,^NVME_ERR^MLX_ERR
61,orendu,Remove 'rx' from comment
62,alex-mikheev,please change 'dw11:' to something more descriptive
63,alex-mikheev,dw11 -> to smth more descriptive
64,alex-mikheev,add a comment that describes what exactly is done here
65,alex-mikheev,0x3 | (64 << 7) | (64 << 15) | (64 << 23) seems to be a common pattern. May be use a common function
66,alex-mikheev,please us a descriptive macro for the capabilities bits
67,alex-mikheev,pls rename to arbitration
68,alex-mikheev,remove packed attribute from the places that already have NVME_PACKED  please use NVME_PACKED macro when such attribute is needed
69,orendu,O_SYNC  yes please
70,orendu,Rephrase: should be set when this queue is a completion queue
71,orendu, The callbacks normally used to deliver commands (such as mlx_emu_q_nvmf_cmd_cb_t) will never be called. When external QP is used  there is no sense in having more than one worker that will handle async events and registers events  so all queues can be created using the same worker context. 
72,orendu,I think we agreed we need indication if this is an SQ or CQ.  In case of SQ  need to have CQ number to report to.
73,orendu,To support it like that  we need bitmap in the attributes to indicate which fields to modify. For now we will only support updating the QPN field.
74,orendu,I think we should have a flag  ext_qp_mode  as well as ext_qp_num.  I'd like to support the option for having no QP configured (i.e. ext_qp_num==0)  but still would not like to get the commands to the callbacks... For instance  in case both by active and backup controllers are down...
75,orendu,Actually we don't allow modifying the MLX_EMU_Q_EXTERNAL_QP bit within the flags field. We allow modifying the ext_qp_num field only. 
76,orendu,Just add that the field is valid only when MLX_EMU_Q_EXTERNAL_QP is set in flags.
77,alex-mikheev,the intention here to change ext_qp_num field. Do you want a new flags or to reword the description ?
78,mike-dubman,what about port? no need to specify? ```mlx5_2:1``` ?
79,alex-mikheev,no need for the port  it is a 'special hca' with one port
80,mike-dubman,Should be called as part of jenkins  Also documened in readme how to call it
81,alex-mikheev,We will have to make sure that we are taking ibverbs and rdmacm from rdma-core and not ofed. May be add a configure check
82,alex-mikheev,Do we need a separate thread polling CQ ? I have an impression that we are always going to work with the event_loop ?
83,alex-mikheev,Does controller gtest really depends on ibverbs and rdmacm ?
84,alex-mikheev,@Artemy-Mellanox  should it be a part of devx actually ?
85,alex-mikheev,it is better to have a more descriptive name than nvmf_cc
86,alex-mikheev,may be a good idea to add a test that also checks that connect command is received.
87,alex-mikheev,naming: either all fields should have nvme_ prefix or go without it.  do you thing it worth to group attributes to structs. For example cq_attrs  sq_attrs  common_attrs etc..
88,galshachaf,this is the command name in the PRM  do you think it's better to change it to  command capsule ?
89,galshachaf,The names are all according to prm  except legacy fields like  q_length  that appeared in the existing code. I think we should change them all to prm conventions  do you agree?  I can leave the common attributes without a struct  and use a union for cq_attrs  sq_attrs. What do you think about this solution?
90,galshachaf,I will add this test.
91,alex-mikheev,That would be a very long name. Let's add short comment that describes what this function does. 
92,alex-mikheev,I agree  lets change to the prm conventions. Union looks like a good solution
93,alex-mikheev,need to change printf to our logger nvmx_debug etc...
94,maxgurtovoy,This might be needed earlier. so let's parse this file during nvme_ctrl_init after allocating the ctrl.  Where is the call to the function that deallocate values ? should be called also in nvme_ctrl_destroy and in error flows during nvme_ctrl_init 
95,alex-mikheev,If json parser license is BSD please move it from sample to the src
96,maxgurtovoy,can this command fail ?  please add error flow to this one as well.
97,maxgurtovoy,what is this config.h file ?  I can't find it in this PR.
98,maxgurtovoy,please remove the src notation.
99,maxgurtovoy,please don't define json config inside nvme_controller.  we will use it from src/* code as well.
100,vasilyMellanox,it doesn't part of this PR  it exists before
101,alex-mikheev,Please move this check eslewhere. Either to main configure.ac or create your own json.m4
102,mike-dubman,todo: need to wrap all getenv/setenv usages by nvmx_getconf/nvmx_setconf and use conf file/env sources  also type checking
103,mike-dubman,why name is hardcoded? probably best take it from conf/env
104,alex-mikheev,we are in the process of having all this taken from the config. Not in this PR though ;)
105,aviadye,In general you are right. We are moving to conf file mechanism so this will be removed anyway.
106,aviadye,In general you are right. We are moving to conf file mechanism so this will be removed anyway.
107,maxgurtovoy,we'll add support for conf file in different PR.
108,mike-dubman,please update jenkins script to set target_ip var so test will kick in CI  @yshestakov - please note
109,nitzancarmi,We currently don't check that we read exactly the same buffer that we have written before (No data corruption).  Maybe instead of using the req->payload as a buffer  it is better to pass our own write_buf to  write  send_nvmf_cmd  and another (blank) read_buf to  read  send_nvmf_cmd  and see if they're indeed identical.
110,alex-mikheev,devx_qp_t itself is quite small you can save the malloc/free and have it as a part of the struct
111,maxgurtovoy,done
112,maxgurtovoy,you don't need this local variable. Just use this dereference during the RNR state.
113,maxgurtovoy,can be nvmx_debug or printed once after ctrl establishment and not per queue.
114,maxgurtovoy,changing nr_io_queues can be in a separate commit and also the renaming of the traddr and subsys name.  This commit should be only for making the rdma/cm establishment with devx_qp.  Also please mention in the commit message that we are based on the new modify_qp api in rdma_cm that still haven't pushed upstream and this functionality can be changed.
115,nitzancarmi,It is already after ctrl establishment. It is at the end of nvmf_rdma_create_ctrl func  right before  return  ctrl->ctrl . Should I still set it to debug?
116,mike-dubman,next line  if  should be included into ```else``` from line 1064
117,nitzancarmi,Applied
118,alex-mikheev,It assumes that gtest is run from the top level of nvmx source. Perhaps a better approach is to use absolute name for the sample config
119,alex-mikheev,default should depend on the  --sysconfdir value ( {prefix}/etc) 
120,yshestakov,I don't like hardcoded path to the config like `src/json/sample.json`  Could we have default one `/etc/nvmx/config.json`  and be able to redefine it with shell env like  NVMX_CONFIG=./src/json/sample.json`
121,vasilyMellanox,No  as you can see it s for test needs only   gtest tests the  ./src/json/sample.json  only and nothing else  the test doesn t make sense if user re-configures the json file 
122,alex-mikheev,@yshestakov it is ok to have a fixed path for the test only file. I agree with you about the installable predefined config file. See my next comment.
123,alex-mikheev,@yshestakov 
124,yshestakov,@alex-mikheev got it  thanks.
125,yshestakov,Thanks
126,alex-mikheev,Let's change it to NVME_CONFIG to be consistent with the rest of environment variables ?
127,alex-mikheev,it still uses a relative path ? 
128,alex-mikheev,Do you need to change sysconf_DATA ?  It may have side effects on the other things
129,alex-mikheev,I don't follow the logic here.   Lets say I run configure --prefix=/usr/local  In this case config file should got to /usr/local/etc/mlnx_nvmx_sample.json  If I run configure --prefix=/usr/local --with-conf=/nvmx/my_conf.json config file should go there   Is this the case ?  
130,alex-mikheev,@yshestakov @miked-mellanox @maxgurtovoy  guys if i do ./configure --prefix=/usr/local --with-conf=/nvmx/my_conf.json where should the config file go on make install:  - /usr/local/etc/my_conf.json  - /nvmx/my_conf.json  - /usr/local/etc/nvmx/my_conf.json  - non of the above    Please vote 
131,alex-mikheev,@Artemy-Mellanox 
132,Artemy-Mellanox,IMHO --with-conf should be able to choose file out of  prefix/ sysconfdir  like  /my_conf.json
133,mike-dubman,+1
134,alex-mikheev,It looks like a generic script which can be used without gtest. Let's move it to ./scripts 
135,alex-mikheev,May be it is better to have a separate class (ex NvmfControllerTest) that will run nvmf related test(s) ?
136,alex-mikheev,I talked with @yshestakov and:  - there seems to be no value in the ability to choose a different configuration file name during ./configure step  - configure already gives option of setting custom sysconfdir  - Athough we don't distribute gtests  Yury build a separate RPM for them and he wants an ability to choose different config for tests at runtime    Bottom line @vasilyMellanox   - remove --with-conf option  - Define nvme_controller_emu_DATA = nvme_controller_config.json in the samples/Makefile.am   - At the moment just copy sample file from the src
137,mike-dubman,why device is hardcoded? what if it is diff than mlx5_2? can it be autodetected or taken from config file?
138,mike-dubman,same here
139,yshestakov,1. of course  define could be auto-detected: run `nvme_bar_parse` and detect the controller.  2. I'm going to create a shell wrapper to do so.
140,yshestakov,I wrote a wrapper script `nvmx_run_ctrl.sh` which auto-detects mlx5_2 by calling `nvme_bar_parse  dev`
141,mike-dubman,need to add check if system has systemd  ```  if [ command -v systemctl >/dev/null ]  then  else  fi  ```  or require it in spec deps section
142,mike-dubman,best not to invoke commands  but check sysfs  ```   ls -1 /sys/class/infiniband/  mlx5_0  mlx5_1  mlx5_2  ```
143,mike-dubman,add -eE to let script fail on failures.
144,mike-dubman,/opt/mellanox/nvmx should be taken from _prefix or  {prefix}  not hardcoded
145,yshestakov,I assume we run our software on RHEL7+ or Ubuntu16+ OS. Both have systemd / systemctl.  Not sure about Yocto/BlueOS Linux. I guess it is based on recent Debian userspace  which also uses systemd since Debian 8 (jessie)
146,yshestakov,Added sanity check that `systemctl` command is present
147,mike-dubman,need to add to start it on boot with  ```systemctl enable nvmx```    Also  need  preun section which will disable service before it is removed.
148,mike-dubman,currently  it will start *last* device found  was it intention?  maybe nvmx start and emu commands from line 13 should be executed for every found dev?  or add break to start on 1st found?
149,yshestakov,Which directory should I use to put tests scripts into? assuming ` {_prefix}     /usr`  `/usr/share/nvmx/tests` -> ` {_prefix}/share/nvmx/tests` -> ` {_datarootdir}/nvmx/tests`
150,yshestakov,Removed hardcoded paths  replaced with ` {_datarootdir}`  ` {_sysconfdir}`
151,yshestakov,Well. At this moment `nvme_bar_parse` works only for one device - `mlx5_2` despite 2  nvme ssd  functions enabled in the FW (we see mlx5_2 and mlx5_3 on the ARM side).  Of course I could add `break` to let the script start on the 1st detected  nvme  device
152,yshestakov,added `break` - changeset 329486b
153,yshestakov,added  changeset 07d9ec2
154,mike-dubman,btw  since this script is part of systemd  maybe need use ```logger  instead of ```echo```  Also would add  found_dev that was picked by script for debug purposes.
155,mike-dubman,also ```enable``` to start on boot
156,yshestakov,SystemD handles output to STDERR properly: it marks the text with  red  color.  Value of ` found_dev` is visible in the output of `systemctl status nvmx` and `journalctl -u nvmx` commands:    ```  -- Logs begin at Wed 1969-12-31 19:00:02 EST  end at Wed 2018-10-03 09:19:25 EDT. --  Dec 31 19:07:22 l-snic006.mtl.labs.mlnx systemd[1]: Starting NVMx EMU controller...  Dec 31 19:07:23 l-snic006.mtl.labs.mlnx nvme_start[1834]: NVME emulation is active on mlx5_2  Dec 31 19:07:23 l-snic006.mtl.labs.mlnx systemd[1]: Started NVMx EMU controller.  Dec 31 19:07:24 l-snic006.mtl.labs.mlnx nvme_controller_emu[1837]: nvme_emu_io_driver.c:148 INFO single_ep posix_io disk  Dec 31 19:07:45 l-snic006.mtl.labs.mlnx nvme_controller_emu[1837]: NVME controller is ready on mlx5_2  Dec 31 19:07:45 l-snic006.mtl.labs.mlnx nvme_controller_emu[1837]: ACQ    [0x30..0x38] : 0x86d17c000  [Admin Completion  Dec 31 19:07:45 l-snic006.mtl.labs.mlnx nvme_controller_emu[1837]: ASQ    [0x28..0x30] : 0x86d17b000  [Admin Submissiong  Dec 31 19:07:45 l-snic006.mtl.labs.mlnx nvme_controller_emu[1837]: AQA    [0x24..0x28] : 0x1f001f ASQS:31 ACQS:31 [Admin  Dec 31 19:07:45 l-snic006.mtl.labs.mlnx nvme_controller_emu[1837]: CC     [0x14..0x18] : 0x460001 EN:1 CSS:0 MPS:0 AMS:0  Dec 31 19:07:45 l-snic006.mtl.labs.mlnx nvme_controller_emu[1837]: devx_verbs.c:738 INFO devx_ctx 0xa680480 created regu  Dec 31 19:07:45 l-snic006.mtl.labs.mlnx nvme_controller_emu[1837]: devx_verbs.c:738 INFO devx_ctx 0xa680480 created on_b  Dec 31 19:07:45 l-snic006.mtl.labs.mlnx nvme_controller_emu[1837]: nvme_prm.c:217 ERROR ctx 0xa680480 map QP to NVME db_  Dec 31 19:07:45 l-snic006.mtl.labs.mlnx nvme_controller_emu[1837]: devx_verbs.c:738 INFO devx_ctx 0xa680480 created regu  Dec 31 19:07:45 l-snic006.mtl.labs.mlnx nvme_controller_emu[1837]: devx_verbs.c:738 INFO devx_ctx 0xa680480 created on_b  Dec 31 19:07:45 l-snic006.mtl.labs.mlnx nvme_controller_emu[1837]: nvme_prm.c:217 ERROR ctx 0xa680480 map QP to NVME db_  Dec 31 19:08:46 l-snic006.mtl.labs.mlnx systemd[1]: nvmx.service: main process exited  code=killed  status=11/SEGV  Dec 31 19:08:46 l-snic006.mtl.labs.mlnx systemd[1]: Unit nvmx.service entered failed state.  Dec 31 19:08:46 l-snic006.mtl.labs.mlnx systemd[1]: nvmx.service failed.  ```  
157,yshestakov,I guess it's too early to let  nvmx  start on the boot implicitly with default settings (512M posix_io backend).  I plan to improve `nvmx.spec` and `nvmx.service` when the new code will be merged.
158,mike-dubman,btw  /opt//mellanox.nvmx still appears in the spec  it should not.  only _prefix
159,yshestakov,thank you. Fixed  see changeset af7edc5
160,mike-dubman,I think you should use  DESTDIR instead of  RPM_BUILD_ROOT
161,yshestakov,We're installing files NOT into the FS root  but into the ` RPM_BUILD_ROOT` directory like ` /rpmbuild/BUILDROOT/nvmx-0.2-9/` for example.  A content of this directory will be packed into the RPM file.
162,alex-mikheev,imho it will make code mode readable if j is set to zero just before the inner while() loop. Or may be change inner while() loop to the for()
163,yshestakov,could we name it `mlnx_nvmx_config.json`?  IMHO  sample  is not good as default config file name
164,alex-mikheev,remove else { } because you already have return in if()
165,alex-mikheev,no need to cast len here  only in prints where size_t is converted to  llu
166,alex-mikheev,indentation
167,alex-mikheev,may be add assert that num_of_list_entries < 64 ?
168,alex-mikheev,It seems that sleeping on fatal instead of aborting is a very useful feature during debug  So can you add configure --enable-debug-sleep-on-fatal as a separate PR ? 
169,alex-mikheev,Shouldnt it be actually 64 ? If first page starts in the middle of prp1 than all 64 entries should be used right ? 
170,alex-mikheev,Change debug to sleep-on-fatal  In help string change sleep-fatal to sleep-on-fatal 
171,alex-mikheev,You can probably do while(NVME_SLEEP_FATAL) instead of  if def;) 
172,alex-mikheev,you can move actual query code to the devx initialization
173,alex-mikheev,Please add } else { clause to the devx_is_ob_ib() check at line 529 and move the check there.
174,aviadye,@alex-mikheev is it ok it is hardcoded like this?  
175,aviadye,can we take it from the define of worker?
176,aviadye,why?  please fix
177,alex-mikheev,@aviadye it is ok because  we always create qps with DEVX_RQ_SIZE. See devx_qp_init()
178,alex-mikheev,May be add an env variable so that we can change the value at runtime ?    Ideally (not in this PR) the value should be configurable and we can pick max value from mlx_emu_dev_query()
179,alex-mikheev,remove getchar() and printf()
180,alex-mikheev,remove printf()
181,alex-mikheev,remove printf+getchar and printfs in the loop
182,kaomri,I will do it in the next commit
183,mike-dubman,is there doc on ansyble playbook with howto build/rebuild such docker image?
184,yshestakov,@miked-mellanox  I have a set of scripts and READMEs on the NFS share:      `/.autodirect/mtrswgwork/yuriis/swx-devops-containers`    I could at it to the  nvmx  git repo if it is suitable. However  I prefer to create a new Git repo at Github or Gitlab  
185,mike-dubman,maybe best create wiki page under gitub/nvmx and mention it here for howto re-create image
186,yshestakov,`swx-devops-containers` contains both README and shell and Dockerfile scripts. And I do update these scripts.    ```  swx-devops-containers  tree -f  .    ./centos75-core      ./centos75-core/mkimage-yum.sh      ./centos75-core/README.md    ./centos75-devel      ./centos75-devel/build_image.sh      ./centos75-devel/Dockerfile      ./centos75-devel/README.md    ./centos75-rdma-core        ./centos75-rdma-core/build_image.sh        ./centos75-rdma-core/rdma-core.repo        ./centos75-rdma-core/README.md  ```
187,yshestakov,I mean publishing only instructions on nvmx/Wiki is not enough
188,mike-dubman,why version is set from outside of the src tree?  It is defined in the autotools https://github.com/Mellanox/nvmx/blob/master/configure.ac L2 and once configured appears in config.h. nvmx.spec file can be nvmx.spec.in and use var substitute with  VERSION to appear correctly.    see example here:  https://github.com/openucx/ucx/blob/master/configure.ac L21 (lines 11-13 as well)    and here: https://github.com/openucx/ucx/blob/master/ucx.spec.in L4    and here https://github.com/openucx/ucx/blob/master/src/ucp/api/ucp_version.h.in as nvmx should know its version internally and print it in the log to ease support in the field
189,yshestakov,It is a good idea to use `autotool`. I can modify `build_scripts/build_rpm.sh` to generate `nvmx.spec` by `autogen.sh`. Going to try it
190,mike-dubman,but it is a good start  smth like  tools and conf is here  run this and get brand new image.
191,mike-dubman,should be set by substitution
192,mike-dubman,can be grepped from configure.ac or similar
193,yshestakov,@miked-mellanox I've reworked build scripts to take the package version from `AC_INIT` of `configure.ac`
194,yshestakov,done
195,yshestakov,done
196,mike-dubman,-eE
197,yshestakov,I can update `build_scripts/README.md` and copy instructions from `/.autodirect/mtrswgwork/yuriis/swx-devops-containers`  add a reference to this NFS directory
198,mike-dubman,-eE
199,mike-dubman,support@mellanox.com
201,mike-dubman,why not convert nvmx.spec into nvmx.spec.in and use autogen substitute? 
202,yshestakov,Updated `build_scripts/README.md` with a reference to `swx-devops-containers` and short instructions how to rebuild `centos75-rdma-core` docker iamge.
203,mike-dubman,best to NOT keep it in personal project folder  but group level. @MrBr-github please advise on location. maybe /hpc/local/oss/devops/.... ?
204,yshestakov,Fixed
205,yshestakov,run of `./autogen.sh` from `build_rpm.sh` take a few minutes. It's very slow. Much easier is to  grep  the version from `configure.ac`
206,yshestakov,@miked-mellanox   ```    time ./autogen.sh  ...  real    0m47.202s  user    0m27.512s  sys     0m2.966s  ```    Well  for some reason I'm not able to make `configure.ac` create `nvmx.spec` by `nvmx.spec.in`. Let's postpone this change to the next PR.
207,mike-dubman,u need to add spec file into configure.ac as well  see example:  https://github.com/openucx/ucx/blob/master/configure.ac L274  
208,yshestakov,Thanks. I tried to do so. I guess I made some mistake.
209,umanskymax,Perhaps  `mft.repo` file (and other in `ansible/files/*`) need move to  `ansible/roles/<roles-name>/files/`  The idea of Ansible roles is an independent unit of configuration without dependencies.
210,mike-dubman,do u change sudoers file in snic to allow no pw root exec?
211,mike-dubman,IMO  best to put all mellanox provided files/utils under /opt/mellanox to avoid looking entire filesystem for distro vs. mlnx provided files/pkgs
212,mike-dubman,btw  why use it from automount and not copy all needed?  what if BF card is taken to non-mlnx network?
213,yshestakov,No  I don't. I use this file on the host only.  To be honest  it's a tool for internal use. I would like to have standalone version which could be shared with customers
214,yshestakov,I agree. Need to review the content of `. fwvalias`: could be share it with customers as is?  In fact  I need only 2 functions:  - `mcra`  - `fwresetfunc` -- `fw_reset_wrapper_shared.sh  __CHIP --unbind_flow  @`  
215,mike-dubman,* mcra is part of mft.rpm so it will be there  * fwresetfunc - is it a bash func or util? where  __CHIP comes from?    
216,mike-dubman,so u need to change /etc/sudoers on host from ansible  maybe best to copy it during ansible and use by copy.  please start thread with tamir and karem to include this file into mft (add me too)
217,yshestakov,`fwresetfunc` is a local function in the script (reset_bf.sh)  I've copied it from `.fwalias`  where `__CHIPID` is an argument passed to this function like *bluefield*:         fwresetfunc bluefield    ```  fwresetfunc ()  {      __CHIP= 1;      shift;      sudo /.autodirect/mswg/projects/fw/fw_ver/hca_fw_tools/fw_reset_wrapper_shared.sh  __CHIP --unbind_flow  @  }  ```
218,yshestakov,Probably  it makes sense to add `fw_reset_wrapper_shared.sh` to some RPM package like `nvmx-ansible`  which will be installed on the host machine (x86_64) and provide all needed content to setup both the host and SNIC.  
219,mike-dubman,AFAIR  fwreset is part of mft.  best to have it as local copy  lets start thread w/ fw folks on this.
220,yshestakov,Ok  thanks. I've sent an email
221,yshestakov,I agree. Will fix it.
222,yshestakov,`fwreset` is not the same as `reset bluefield`.  In case I run `fwreset` I need to do so on both the host (x86_64) and SNIC (aarch64) sides.  `reset_bf.sh` doesn't depend on state of SNIC/arm side
223,yshestakov,I found a workaround how to reset SNIC FW w/o waiting for the peer host:  ```     SNIC   mlxfwreset -d 03:00.0 --yes -s --skip_fsm_sync r  ```
224,mike-dubman,cool  shall we close FR in the redmine?
225,yshestakov,@miked-mellanox  Yes  we can close it. The same command works from the host. I'm going to modify `reset_bf.sh` script to use `mlxfwreset` only
226,umanskymax,You can use `ansible_os_family` instead `ansible_distribution`. Example:  ```suggestion    when: ansible_os_family ==  RedHat  ```
227,umanskymax,The same for Debian family:  ```suggestion    when: ansible_os_family ==  Debian  ```
228,umanskymax,Service restart better provide via Ansible handlers    and move all handlers into `roles/<roles_name>/handlers/`  
229,yshestakov,Fixed
230,yshestakov,Got it  thanks :)
231,alex-mikheev,stylew: add space between include and <
232,alex-mikheev,can you add getopt() support since there will be more than one option ex --srcip and --srciface to test binding from the specific iface/ip
233,aviadye,nvme_cq_type_cqe - why query is marked as PASS in excel?  nvme_cq_type_nvmf - why query is marked as PASS in excel?    This should be indicated in excel 
234,aviadye,same for nvme_sq_sqe_create_modify_query_destroy  and for nvme_sq_cc_create_modify_query_destroy    you should mark as RED in excel 
235,aviadye,Please rename nvme_sq_sqe_create_modify_query_destroy to nvme_sq_tyep_sqe
236,aviadye,Please rename nvme_sq_nvmf_cc_create_modify_query_destroy to nvme_sq_type_nvmf_cc
237,aviadye,4?
238,galshachaf,OK
239,aviadye,remove dead code
240,aviadye,remove dead code
241,galshachaf,probably stale code. changed back to 12.
242,aviadye,is it working?
243,aviadye,Leave a comment that this test is missing query
244,aviadye,Leave a comment that this test is missing query
245,galshachaf,like magic
246,aviadye,dev_ctx and umem_id already printed above
247,aviadye,didn't u remove this dead code?
248,galshachaf,done
249,galshachaf,done
250,alex-mikheev,a small nitpicking  when comparing with  a constant it is better to put it first. That way  gtest makes a prettier output when the test fails ;)
251,galshachaf,tnx alex :)
252,aviadye,please add a comment that need to fix kernel
253,galshachaf,added
254,maxgurtovoy,typo M -> N
255,maxgurtovoy,The size of every cmd should be 64B. here you have 60B in my count.
256,maxgurtovoy,use a definition as all other env variables in the code.
257,kaomri,It's like NVME_EMU_TYPE
258,mike-dubman,i think you should override rpm build root dir to host local FS.  What if you run ansible on number machines in parallel and they all building inside same dir in   ?
259,mike-dubman,same here
260,mike-dubman,what if already loaded? maybe need to modprobe -r 1st?
261,mike-dubman,what is this IP? 
262,yshestakov,@miked-mellanox `rpmbuild --rebuild` uses ` HOME/rpmbuild/` directory. In case of  root  user it uses local FS on the host.  
263,yshestakov,We run ansible playbook(s) with  become_user = root . So  ` HOME == /root`
264,yshestakov,We need to ensure that  rhsim  module is loaded. No need to unload it (it causes removal of  tmfifo_net0  network interface). Also  `modprobe XXX` doesn't fail if the module is loaded already.
265,yshestakov,It's assumed by developers of BlueField software stack that we use 192.168.100.0/24 network for  Ethernet over RSHIM  interface.  - 192.168.100.1 is assigned to the host  tmfifo_net0  - 192.168.100.2 is assigned to the SNIC  tmfifo_net0  
266,alex-mikheev,add error check
267,maxgurtovoy,what is this  if 0  needed for ?
268,nitzancarmi,I left it since it works  just no longer needed. Removed it now to avoid confusions.
269,alex-mikheev,can you add a nvmx_debug() print to the nvmf_rdma_route_resolved_handler() that will show an actual device ?  ex: nvmx_debug( route resolved: rdma device  s   queue->cm_id->verbs->device->name);
270,alex-mikheev,@yshestakov NVMX_SCRIPTS_DIR is set to the  {ac_pwd}/scripts. (current build directory). So this will break when nvmx is installed from the binary RPM.   In addition NVMX_SCRIPTS_DIR is used in the nvmf controller tests. So it will break if we relocate tests  
271,alex-mikheev,Cannot hardcode device name here because tests may be run of different devices.  Let's use get_dev_name() helper function here and a table with a predefined dev to iface mappings.
272,nitzancarmi,@alex-mikheev I'm not sure how you see this patch being run?  We either need to install (copy) it on the snic  or to use it from the scripts dir.
273,yshestakov,@alex-mikheev Let's don't specify an absolute path to the script and assume it's placed somewhere into  PATH  We can add needed scripts from `scripts/` directory into `/usr/bin/` if needed
274,nitzancarmi,After consulting with Yurii  we will just use  ibdev2netdev  as it is  and the script will be added to bin path from the rpm build.
275,maxgurtovoy,this is error. and false means that this is not a representor.
276,maxgurtovoy,where do we use this code convention ?
277,mike-dubman,why needed? this script is part of mofed/ofed
278,mike-dubman,@nitzancarmi   @yshestakov 
279,mike-dubman,taken from hcoll src code
280,mike-dubman,we should not use system calls to extract dev info  see example below for the right way:  ```   define MAX_STR_LEN 128  static int port_from_file(char *port_file) {      char buf1[MAX_STR_LEN]  buf2[MAX_STR_LEN];      FILE *fp;      int res = -1;        if ((fp = fopen(port_file   r )) == NULL)          return -1;        if (fgets(buf1  MAX_STR_LEN - 1  fp) == NULL)          goto out;        int len = strlen(buf1) - 2;      strncpy(buf2  buf1 + 2  len);      buf2[len] = 0;      res = atoi(buf2);    out:      fclose(fp);      return res;  }     define PREF  /sys/class/net/   define SUFF  /device/resource    static int dev2if(char *dev_name  char *port  char *if_name) {      char dev_file[MAX_STR_LEN]  port_file[MAX_STR_LEN]  net_file[MAX_STR_LEN];      glob_t glob_el = {0 };      int found = 0;        sprintf(dev_file   /sys/class/infiniband/ s SUFF  dev_name);        glob(PREF *   0  0   glob_el);      char **p = glob_el.gl_pathv;        if (glob_el.gl_pathc >= 1)          for(int i = 0; i < glob_el.gl_pathc; i++  p++)          {              sprintf(port_file   s/dev_id   *p);              sprintf(net_file    s SUFF     *p);              if(cmp_files(net_file  dev_file)   port  = NULL                  port_from_file(port_file) == atoi(port) - 1)              {                  found = 1;                  break;              }          }        globfree( glob_el);        if(found)      {          int len = strlen(net_file) - strlen(PREF) - strlen(SUFF);          strncpy(if_name  net_file + strlen(PREF)  len);          if_name[len] = 0;      }      else          strcpy(if_name   );        return found;  }  ```
281,nitzancarmi,We don't want to install ofed on the snic  but we still need to use ibdev2netdev (called from within the code).
282,mike-dubman,@nitzancarmi - we should try not to call  system(script)  from code - please see my inline comment about it in the merged PR w/ code example to achieve same effect.
283,mike-dubman,here: https://github.com/Mellanox/nvmx/pull/169 pullrequestreview-170768777
284,mike-dubman,nice catch. the proper fix is that iface_to_ipaddr and this line will use same constant  like IP_ADDR_LEN (256 now)  https://github.com/Mellanox/nvmx/blob/3013a3045ed773221e09fc71fee7a06a1697c2c1/src/utils.c L115    once we will start support ipv6 the len will become 128bit and it will break again.  need to enable coverity asap to catch such and similar errors during development.
285,yshestakov,> nice catch. the proper fix is that iface_to_ipaddr and this line will use same constant  like IP_ADDR_LEN (256 now)  > ...  > once we will start support ipv6 the len will become 128bit and it will break again.  > need to enable coverity asap to catch such and similar errors during development.    I'm going to rework this code to pass string size explicitly and call `inet_ntop()` instead of `inet_ntoa()`.  BTW  `inet_ntop()` supports `AF_INET6`    Please take a look at the next commit to this PR.     UPD. Please don't merge: it fails to compile
286,mike-dubman,why need last param? src_addr is null terminated and one can calc len dynamically.
287,mike-dubman,or use INET6_ADDRSTRLEN constant
288,yshestakov,> why need last param? src_addr is null terminated and one can calc len dynamically.    default null-terminated `src_addr` has `strlen(src_addr) == 0`. It's not what we expect :)  Also  it's a good practice to pass the buffer size to underlying call  like`inet_ntop()` has:    ```  const char *inet_ntop(int af  const void *src                                char *dst  socklen_t size);  ```
289,yshestakov,> or use INET6_ADDRSTRLEN constant    `INET_ADDRSTRLEN` in case of IPV4 (we support only it). I've put the constant here
290,yshestakov,@miked-mellanox  please re-approve this PR. I've tested the build on aarch64/docker and it passed.
291,aviadye,u probably mean AF_INET and AF_INET6
292,yshestakov,Yep  correct :)
293,yshestakov,> u probably mean AF_INET and AF_INET6    I've fixed this comment. Please review
294,aviadye,let add function id and add this to show_gids new utility. It will is small effort with huge ROI
295,orendu,is an example of the configuration syntax
296,orendu,obvious - remove
297,orendu,obvious - remove
298,orendu,In the ctrl object you refer to 'localfstgt'  better have such backend example
299,orendu,why did you add 'xxx'? this shows how the same backend can be used twice  as different namespaces  with different local block path
300,alex-mikheev,add ASSERT_TRUE(values  = NULL)
301,alex-mikheev,Lets add NVME_DEFAULT_CONFIG to the config.h with AC_DEFINE
302,alex-mikheev,add check that values is not NULL
303,maxgurtovoy,where is the check that values  = null ? I remember you had it..
304,maxgurtovoy,please add mn value also to json file. We got a request for this from customers.
305,maxgurtovoy,as I wrote above  please add default mn  MLX NVMe Ctrl 
306,maxgurtovoy,nsid 0 is not valid in the spec.  We also support only 1 namespace per subsystem currently (with nsid == 1)
307,maxgurtovoy,here too.
308,maxgurtovoy,please use an  if  clause instead of __func( a ? dsd : dss  attr2).  let's have a readable code.
309,maxgurtovoy,maybe change signature to return values or NULL in case of failure ?  Try to make code as simple as we can.
310,maxgurtovoy,I don't understand what all the fixes of white space and commas related to json object commit ?  Should be separate.
311,vasilyMellanox,It's impossible to get NULLL here
312,vasilyMellanox,I prefer this notation
313,vasilyMellanox,Is it really issue if it will be submit here ?
314,vasilyMellanox,Didn't get it... Why it should be NULL ? 
315,vasilyMellanox,it is not supposed to return a pointer
316,vasilyMellanox,but fixed it anyway
317,alex-mikheev,Let's add an nvmx_assert_always just in case.
318,maxgurtovoy,lets create a unit_tests.json with needed definitions.  In this mlnx_nvmx_config we should have only parsed attributes (this is the example for customers)
319,maxgurtovoy,please fix.
320,maxgurtovoy,please change func name.  I also don't think we use __name convention for static function in this file.
321,alex-mikheev,Use PATH_MAX for the path max length
322,alex-mikheev,nvmx_error()  also there is a nice  m format option to printf
323,alex-mikheev,nvmx_error()
324,alex-mikheev,substitute 128 by sizeof(dev_port_path)
325,alex-mikheev,nvmx_error()
326,alex-mikheev,PATH_MAX
327,alex-mikheev,snprintf
328,alex-mikheev,we can do some magic like  define member_size(type  member) sizeof(((type *)0)->member)
329,alex-mikheev,lets use IFACE_MAX_LEN
330,alex-mikheev,IFACE_MAX_LEN  also there is not need for -1 here:   The snprintf() and vsnprintf() functions will write at most size-1 of the characters... 
331,alex-mikheev,Let's put it in the compiler.h near the offset_of and container_of macros
332,aviadye,put IF 0 here - exception for dead code
333,galshachaf,should we add a macro for this?
334,alex-mikheev,imho if the destroy flow is working we don't need ifdef here  just remove the sleep
335,galshachaf,@aviadye requested this  please decide
336,alex-mikheev,should switch to json config file
337,alex-mikheev,IMHO we need to add create_destroy_twice test for other queue types ? Do you want to handle it in another PR ?
338,galshachaf,yes  it is not connected to this pr. there are create n_cqs  n_sqs tests.
339,alex-mikheev,Please use some kind of define instead of 0x3
340,kaomri,Done
341,alex-mikheev,fclose(fp);
342,yshestakov,ah  thanks. Fixed. 
343,alex-mikheev,Please fix the indentation here
344,galshachaf,fixed
345,maxgurtovoy,maybe we'll make it configurable ? I don't want to break the possibility to use 1 NS.    Also please fix typos in the commit message
346,alex-mikheev,Lets use normal // or /* */ style comments. Json parser has no problems with that
347,maxgurtovoy,no need to change the create_cq callback. we don't use it anyway now
348,maxgurtovoy,I prefer to check if the num_entries > ctrl->max_io_queue_size instead of the define.
349,maxgurtovoy,no need
350,maxgurtovoy,change io_queue_size to max_io_queue_size and leave it here.
351,maxgurtovoy,you can leave the max_io_queue_size.  if target support upto 512 and we'll ask 1024 it will work in your code.
352,maxgurtovoy,why additional commit for this ?
353,maxgurtovoy,why don't you use the ctrl->max_io_queue_size ?
354,alex-mikheev,please add nvmx_error()
355,alex-mikheev,since you already have a nvme_config_t it make sense to change nvme_json_get_value to accept is a first argument instead of a (values  num_values)
356,alex-mikheev,please remove trailing  \n  in messages in all nvmx_error/debug... macros
357,alex-mikheev,Let's add a function to get controller config ? 
358,alex-mikheev,also driver should hold nvme_config_t
359,orendu,c-style comments are not standard json.
360,orendu,Why do we need this param? Just look how many EPs are in the endpoints array...
361,orendu,This is not the last example we discussed for the config...
362,alex-mikheev,afaik json has no comments at all ;). It is much better to have comments that are human readable. Check our those guys: http://hjson.org/  
363,alex-mikheev,can you move config file reading to the test setup ? We will need this  pattern in other places too. 
364,galshachaf,seems like you're introducing an indentation problem?
365,galshachaf,remove this line
366,alex-mikheev,fixing the existing one. Most of the code is done with the     set ts=4  set sw=4  set expandtab    
367,alex-mikheev,It is better to put the code in that if in a separate function
368,alex-mikheev,Too many parameters. May be create a type for the mkey ? Something like  struct devx_mkey_t { uint32_t mkey  devx_obj_handle *mkey_objh  struct devx_obj_handle *psv_objh  uint32_t psv_index }  
369,maxgurtovoy,I'll do it next week.
370,maxgurtovoy,ok
371,alex-mikheev,Let's also change g_malloc0 here to a regular malloc and check the allocation instead of failing on assertion. 
372,alex-mikheev,Let's also change g_malloc0 here to a regular malloc and check the allocation instead of failing on assertion. 
373,galshachaf,change to sigint_handler (conventions)
374,galshachaf,change to keep_running (conventions)
375,alex-mikheev,NVME_XXXX environment variables are no longer supported. You need to create custom test config and use NVME_CONFIG=.. var
376,alex-mikheev,this is wrong  it should return 31 (33 - 2) for the 32 io queues.  SPEC 5.14.1.7
377,maxgurtovoy,num_queues = 32 and not 33. and this is zero based
378,maxgurtovoy,ctrl->num_queues = NVME_EMU_NUM_QUEUES (=32)
379,alex-mikheev,you are right. Yet i don't like two different values for the number of queues and working with the zero based value and C arrays
380,alex-mikheev,It is enough to return NVMX_ERR here because driver only sees bar registers at this stage of controller init
381,maxgurtovoy,fixed. also same for the nvme_sq_init.
382,vasilyMellanox,The last line is indented too far to the left  please align with the previous ones
383,vasilyMellanox,U should remove 'ep' 'ctrl' from the class declaration if you decided to use local pointers to 'ep'   'ctrl'
384,nitzancarmi,fixed
385,nitzancarmi,fixed
386,alex-mikheev,please change printf to nvmx_error
387,alex-mikheev,please add ? 1 : 0 to improve readability and people won't think that == should be changed to = ;)
388,alex-mikheev,missing return code
389,alex-mikheev,add error check or change nvmf_rdma_init_queue() to void
390,alex-mikheev,please remove  \n 
391,alex-mikheev,Let's add a description of the each state and short description of the state machine. That is when transition between states happen
392,alex-mikheev,Please add /* TODO: use mlx_emu_q_modify() once it is implemented */
393,nitzancarmi,Will send seperate commit to clean all `\n` appearances in nvmx_*() formats
394,alex-mikheev,pls remote  \n 
395,nitzancarmi,Done
396,alex-mikheev,please use snprintf everywhere instead of sprintf
397,alex-mikheev,typo: ^patth^path
398,alex-mikheev,can array size be zero ?
399,alex-mikheev,a better pattern is snprintf(namespace  sizeof(namespace)  ...);
401,vasilyMellanox,No  it's error json format + user MUST define at least one ns - in order to know which  backend/driver  will e raised
402,alex-mikheev,what happens if user does not define one ns ?
403,maxgurtovoy,code conventions:  1. no need to have a new line after variables definitions.  2. definition + assignment first  only definitions second.
404,maxgurtovoy,please use a function for this logic. Its not a good practice to have so long functions.
405,maxgurtovoy,again code conventions
406,maxgurtovoy,don't we have define for 0x02 ?  BTW  why is this test part of the commit ?  I prefer to put new tests in different commits
407,vasilyMellanox,This is not a new one - the current commit breaks it  so it's just a repairing - 0x02 originally used there
408,vasilyMellanox,Sorry but it's impossible - the logic depends on loop index and calculates the values that used almost across the each line of the loop (like prev  disk size etc.)
409,maxgurtovoy,please explain what will happen ?
410,maxgurtovoy,please write a correct error log
411,maxgurtovoy,don't write code in the error log
412,maxgurtovoy,it;s worth adding a comment about the name
413,maxgurtovoy,your example now is cc_remote with nvmf_rdma. let's add a port and addr example instead of NULL
414,maxgurtovoy,wrong error log
415,maxgurtovoy,see above comment
416,maxgurtovoy,If I set size == 0 ? please fail it. Users/QA will do it for sure.
417,maxgurtovoy,we support only block order 12 and 9.  please make sure to fail otherwise.  Error flows  
418,maxgurtovoy,you can save a pointer to config in the driver. We'll need it in the future. And we can use ctrl code in our BSD code
419,maxgurtovoy,please use this enum:    enum NvmeIdentifyCns {      NVME_IDENTIFY_CNS_NAMESPACE             = 0x00       NVME_IDENTIFY_CNS_CTRL                  = 0x01       NVME_IDENTIFY_CNS_ACTIVE_NS_LIST        = 0x02       NVME_IDENTIFY_CND_NS_ID_DESCRIPTOR_LIST = 0x03   };    from src/nvme.h.  also you can fix other place in different commit as well.
420,maxgurtovoy,???
421,maxgurtovoy,can you use our standard NVMX_ERR ?
422,maxgurtovoy,??
423,maxgurtovoy,if metadata is 0  the integrity should be None
424,maxgurtovoy,here too
425,maxgurtovoy,please change the order of comparison:  disk_block_order  = 9    disk_block_order  = 12
426,maxgurtovoy,NVMX_ERR
427,maxgurtovoy,do you save the pointer ?
428,maxgurtovoy,I don't understand this strange logic. The code should not be adjusted to tests.
429,maxgurtovoy,need to check if we really need q_type
430,vasilyMellanox,There is a comment above      /* Get the number of namespaces */    The func description is over its prototype...
431,vasilyMellanox,talk to ALexM  he will explain it better than me
432,vasilyMellanox,inside ctrl  see NITZAN commit
433,galshachaf,does queue_reset describe this well enough? because what we really do is make sure there isn't any garbage data in the struct.. what about queue_clear?
434,alex-mikheev,i will change it to clear  np
435,maxgurtovoy,you shouldn't call this function with q == NULL.  why is this happen ?
436,galshachaf,you can call it with q == NULL from nvme_stop_ctrl  this is what happened to Yurii.  Maybe there is another bug in nvme controller  but it's best practice to have our code more defensive.
437,mike-dubman,typo: netwrok
438,mike-dubman,add note about what user user should do in this case  like manual config file  this and that
439,mike-dubman,typo: provision
440,yshestakov,thanks  fixed (going to push all changes in a minute)
441,yshestakov,Added  > There is a *NVMe emu controller configuration* section in the Wikinox page above.  > Please use it as a reference.
442,yshestakov,fixed
443,alex-mikheev,Can we actually have a human readable values here in decimal and do all necessary magic when we read them ?
444,alex-mikheev,sorry to open this again but since we do not support sq entry size  = 64 bytes and cq entry size  = 16 bytes may be we should to remove  sqes  and  cqes  from the config ?  @maxgurtovoy @orendu what do you think ?
445,maxgurtovoy,this function is used:     if HAVE_RDMA_CM_MODIFY_QP_CB          param.qp_num = queue->devx_qp.super.id;          param.modify_qp =  nvmf_rdma_modify_qp;   else          nvmx_fatal( Cannot connect to the NVMF target because rdma cm has no                      modify_qp callback. );   endif    Maybe need a backport there too.
446,alex-mikheev,I can put nvmf_rdma_modify_qp under  HAVE_RDMA_CM_MODIFY_QP_CB but then the function will not be compiled on the systems without the latest rdma-core. I prefered to mark it unused so that it always compiles.
447,alex-mikheev,fixed
448,maxgurtovoy,please add error flows goto.    out_free_qp:      devx_qp_reset( nctx->worker   queue->devx_qp);  out_err:      return ret;
449,maxgurtovoy,ret = -errno;  goto out_free_qp;
450,maxgurtovoy,let's add goto and re-use code.  goto out_err;
451,maxgurtovoy,goto out_free_qp;
452,alex-mikheev,you are off by one here because num queues includes admin queue
453,maxgurtovoy,here too.  Also don't forget code conventions:  if {  } else if {  } else {  }
454,maxgurtovoy,you also need to replace its location in the json file conf as well.
455,nitzancarmi,Done
456,nitzancarmi,@maxgurtovoy I think it is already located in-place in config file as backend (io_driver) attribute  and not path (endpoint) attribute. We were wrong for parsing it in code into ep attribute in the first place.
457,nitzancarmi,@alex-mikheev So after your change  ctrl->num_queues here should be nr_io_queues + 1  right?
458,maxgurtovoy,@nitzancarmi please notice that the backend is pointing to array of backends (currently we support 1 backend but it can change in the future). it should be in the ctrl section of the json file since we don't want that any backend will have different nr_queues from what we expose to the host.
459,maxgurtovoy,you have more json files to update.
460,alex-mikheev,there is no point to have this function for a single line of code. Let's remove and move the line to nvme_ctrl_init()
461,alex-mikheev,please add an error message
462,alex-mikheev,please move  nr_io_queues  to the controller section so that you are compatible with the nitzan changes
463,maxgurtovoy,sqe_only can be used also with nvmf_rdma. please try it.  also there is a sqe_cc mode that need to add to the descriptions
464,maxgurtovoy,maybe you can mention here that this file is used for unit testing in DB mode to check ctrl functionality without real emulation.
465,alex-mikheev,please move md_size after the uint16_t size
466,maxgurtovoy,please leave this line as is below the if/else clause.
467,maxgurtovoy,what we actually need is to check if the expected amount on namespaces we configured to the host is <= nn.
468,mike-dubman,typo: runningn
469,mike-dubman,typo: controlelr 
470,galshachaf,I have a script called run_unvme.sh just for this  I can have it accept the NVME_CONFIG as argument and also run tests  would you like me to change that?
471,galshachaf,typo: tesst instead of tests
472,galshachaf,This is not true for unvme tests...
473,galshachaf,where are these tests? the jason parser sanity tests? because I get no failures with tests/snic_tests.json
474,galshachaf,again  tests/run_unvme.sh takes care of all of that
475,alex-mikheev,pls do
476,galshachaf,ok  so let's disregard this comment for this PR  approve it  and I'll change the doc regarding this in a separate PR.
477,nitzancarmi,You are missing a breaks here. Or if you intended to increment counter either way  please fix indentation.
478,alex-mikheev,please use  queue  d: reached t....  format. It make things ways easier to grep when looking for the problem with specific queue
479,alex-mikheev,typo: ^QUERYS^QUERIES
480,alex-mikheev,please add define for the time out value
481,alex-mikheev,code style: space after else
482,alex-mikheev,usleep takes timeout in micro seconds  please rename to xxx_IN_USECS
483,maxgurtovoy,no need the else here.  just continue the flow:    real_block_order =   real_metadata =     
484,maxgurtovoy,please use dedicated func to validate a value according to expected value and path in json.  In that way you won't duplicate code here for data/metadata format.
485,maxgurtovoy,i think it's better to have it as json method:    nvme_json_verify_config_value(config  config_path  expected_val)
486,maxgurtovoy,no need for the else.    just retrun 0;
487,maxgurtovoy,can we check also the namespaces num is <= target_ns_num
488,maxgurtovoy,this should be in the beginning of the func as well.
489,maxgurtovoy,please put undefined args bellow define args.
490,maxgurtovoy,?
491,maxgurtovoy,just return ns here and return NULL in the end of the func if you don't find the NS.  no need for break and another variable.
492,maxgurtovoy,see how we use comments in kernel.  /*    \* comment    */  
493,maxgurtovoy,never break the spec. Please fix that otherwise we can't push it.  You can use a sorted list instead of an array for example.
494,maxgurtovoy,check list allocation while we're here.
495,maxgurtovoy,code conventions:    if (bla_bla)      oneline;    no need {} arount 1 liner
496,nitzancarmi,Following Alex mail  should I always stick to kernel code convention? Because I looked at it before I pushed  and saw at least 4 different comment conventions in that file alone.
497,alex-mikheev,Can you take a fix from https://github.com/Mellanox/nvmx/pull/259/commits/6edc2067e7525a7e1d4f9347b536538275eb2713 and add it here ?
498,nitzancarmi,No problem.  Rebased and pushed on top of my commits.
499,maxgurtovoy,if (condition) and not if(condition)
501,maxgurtovoy,I don't understand the compare function.  you add LE namespace ID to the list. each element is the pointer to le(ns_id).  Here you compare the pointers.
502,maxgurtovoy,it's not a mandatory field so if it's not in the conf file  use 0 and 0 for both.
503,maxgurtovoy,please add error flows here.  we don't allow leaks in our code
504,maxgurtovoy,please use if/else here. also make sure your code is aligned.  actually I don't understand how active_ep can be >= nr_eps.
505,maxgurtovoy,where is deactivation ?  where do you delete second ep ?
506,maxgurtovoy,second ep?
507,maxgurtovoy,NVMX_ERR. use goto here
508,maxgurtovoy,same here.    also  don't you distinguish between create/activate states ?
509,maxgurtovoy,???
510,nitzancarmi,I prefer not to distinguish ctrl state based on io queues configurations.  As far as the nvmf ctrl is concerned  it is responsible only for keeping admin queue connected  and therefore stays connected as long as it has live admin queue.  The io driver may decide do create  destroy  activate  deactivate queues on that ctrl  it won't matter to him.
511,alex-mikheev,should be -EINVAL. We try to use -errno convention.  Please grep   fix 
512,alex-mikheev,nice catch 
513,mike-dubman,why hardcoded version here? can it be autodetected from configure.ac and substituted here?
514,yshestakov,Well  yes. `inventory.yaml` should be generated by `configure` from the template. Going to add a fix to this PR
515,nitzancarmi,While we're here  you can remove the  \n .
516,nitzancarmi,maybe it is more readable to just  return 0 . not sure.
517,alex-mikheev,should shift right by one because status contains phase bit plus use le16_to_cpu() to be portable  
518,alex-mikheev,can you also change error message so it starts with queue  d:  it makes grepping for particular queue much easier
519,alex-mikheev,in nvmx_info above:  queue  d: deleting 
520,alex-mikheev,also  queue  d: filed to modify to state .... 
521,alex-mikheev,^Queue  d^queue  d:
522,alex-mikheev,see comment above about cqe.status
523,alex-mikheev,add queue  d: to the error message
524,alex-mikheev,should not it be an error ? 
525,nitzancarmi,I preferred the  best effort  approach. But I'll change it to error.
526,maxgurtovoy,error flows ?
527,maxgurtovoy,are you sure we've 1 backend ?  
528,nitzancarmi,I tried to quickly add it without the function refactoring  but you're right.  So  I will just add this patch to a larger series of fixes in that function.
529,alex-mikheev,typo
530,alex-mikheev,typo
531,maxgurtovoy,new line ?
532,maxgurtovoy,is this nvme specific behaviour ? maybe better to update devX ifc.h ?
533,maxgurtovoy,is this related to this patchset ?
534,maxgurtovoy,can we use regular while instead of do/while ?
535,maxgurtovoy,i guess we don't need to initialize the ret arg.
536,maxgurtovoy,where is the cb implemented ?
537,maxgurtovoy,we can squash the previous and this commit.  also we can remove the check if (dev->flr_cb) and just call it.
538,maxgurtovoy,this commit can be squashed to the first one 
539,maxgurtovoy,you can run 32 times instead of 43 and do an inner for loop (j < i) for create_io_q(j + 1). will be harder test that will create 1 more queue in each loop.
540,maxgurtovoy,admin queue size is 32/33 in Linux 
541,alex-mikheev,fixed in the next commits
542,alex-mikheev,in the glue layer so that we can restore our custom bar on the FLR
543,alex-mikheev,At the moment there is no easy way to handle wrap around  and max queue size is limited to the 64 entries (page size).   Also until qp by name is in I prefer not to create extra queues.
544,alex-mikheev,During the PXE installation I see  AQA    [0x24..0x28] : 0xff00ff ASQS:255 ACQS:255 [Admin Queue Attributes]    This specific test reproduces a problem that we encountered in the FW so I tried to use queue sizes that I saw in the log.
545,alex-mikheev,unfortunately :(. I will submit another PR that merges two libraries. 
546,alex-mikheev,@Artemy-Mellanox is against it. He says that we should keep ifc changes or new structs as a part of our project
547,alex-mikheev,updated
548,alex-mikheev,do while fits better here because we always want to enable hca
549,maxgurtovoy,yes but you assign n = 0 and check n < MLX_FLR_RETRIES.  so you'll get in anyway :)
550,alex-mikheev,let's rename from nvme_driver_prp_t to the nvme_driver_req_t  and also prp to the req later in the code when you use this.
551,nitzancarmi,nsid here is the ctrl.namespaces[i].nsid.  What we really need to pass here is the ctrl.namespaces[i].backend.nsid.  We currently have limitation for them to be the same  but let's do it right from the start so we won't need to change it again later.
552,maxgurtovoy,my suggestion was to pass the entire nvme_rw cmd. why we need to create a new data structure and copy things to it instead of passing existing one ?
553,maxgurtovoy,we don't need extra data structures while we have all nvme protocol cmds already. see my comment above
554,maxgurtovoy,can you remove the unused fields in a preparation commit ? e.g req is not used at all...
555,maxgurtovoy,For good practice  please destroy in the opposite order of allocation.
556,maxgurtovoy,for one liner loop no need to use {}
557,maxgurtovoy,one line condition.  also please don't put logic before local params - code conventions.
558,maxgurtovoy,same here. please put above.
559,maxgurtovoy,can you combine a test to check the qid feature (instead of using 1 qid ?  for (ns) {       for (qid) {          read_write()      }  }
560,maxgurtovoy,what are these hard coded values mean ?  I guess define is needed.
561,maxgurtovoy,you shouldn't use sequential nsid anymore. We support now also non-seq namespaces. So need to take the nsid in ns_arr[i]
562,maxgurtovoy,where do you actually use data_buf ?
563,maxgurtovoy,where do you use prp_list ?
564,maxgurtovoy,don't you want to check the whole prp1 you wrote ?
565,maxgurtovoy,I'm not sure I understand the logic here. data transfer length is (nlb+1)  << block_order.  Are you counting on block_order == 9 ? What will happen in case I'll use json file with block_order = 12 ?
566,maxgurtovoy,why 9 ? can we use some random generator for traffic ?
567,alex-mikheev,At the moment ALL test infrastructure assumes block size 9. IMHO there should be a separate PR for the 4k blocks.  
568,alex-mikheev,At the moment we use fixed/constant patterns everywhere. 
569,vasilyMellanox,common in gtests   pls look other tests of other peoples
570,vasilyMellanox,It s like another test defines it 
571,vasilyMellanox,If you mean  nvme_cmd_rw_t rw;    it s C++  better to define where u actually use it 
572,vasilyMellanox,There is no define   I ve just copied it from NVMe tests - like AlexM uses it there 
573,maxgurtovoy,please check all the written data of 4k.
574,maxgurtovoy,In general I prefer never push code that I know that will be changed in the future.  It s better to write 1 test with right value to be as an example for others.  I guess we can ignore this call this time but not a common practice.
575,alex-mikheev,i think you don't need param to pass the string
576,nitzancarmi,you're right. fixed.
577,maxgurtovoy,if (likely(driver->active_endpoint >= 0   driver->active_endpoint < driver->nr_endpoints)
578,maxgurtovoy,1 line space after variables definitions
579,maxgurtovoy,here too
580,maxgurtovoy,*
581,maxgurtovoy,*
582,maxgurtovoy,be consistent when you write code.  if you use qid naming so use it in all funcs.  if you use sometimes sqid and somethimes cqid then don't use qid.  also regarding if conditions:    if (qid   blabla) of if (blabla   qid) and not both.    readability  
583,maxgurtovoy,in prm lba_size is 8 bit (log units) and md_size is 16 bit (regular units)
584,maxgurtovoy,this is not void function yet  Although we're regular that destroy function always succeed.  this is different refactor work.
585,maxgurtovoy,same here.
586,maxgurtovoy,please add size to the print + move nvmx_info to nvmx_debug
587,maxgurtovoy,same here 
588,maxgurtovoy,this comment is wrong
589,nitzancarmi,This is very risky with respect to the current state machine.  You unlock state_lock before moving state to DISCONNECTED.  So  another context might enter the function again  so we  might end up double freeing things.    What I think would more effective is to remove the state_lock from rdma_cm handler  (already done it successfully  I hope  in HA feature).    
590,alex-mikheev,You cannot hold the mutex after pthread_cancel because it causes a deadlock.   Also at the moment we only have two thread context: main and cm state machine. Once cm thread has exited no one is going to change the state.  Please correct me if there is another thread context.   
591,nitzancarmi,Have you seen that deadlock actually happens? because today we only support rdma_cm events that happen during queue creation process  and we should never reach destroy_queue for a queue not yet created (not until HA will be introduced).   Also  future speaking  we should support rdma_cm events that need to call destroy_queue by themselves (DISCONNECTED for example) so this patch will interfere there and will need to be reversed (actually  it will happen when I will push HA code).    Instead  I suggest that we just remove the state lock from rdma_cm_handler completely (only relevant for ESTABLISH handler)  and also remove the state unlock/lock around sem_wait() in nvmf_rdma_create_queue(). That way we save ourselves from any parallel create/destroy queue calls  it's deadlock free  and we won't need to worry about  too soon  unlocks.
592,nitzancarmi,Ok  I see how deadlock can happen in today's code.  But I still think the right solution is the one I suggested.  
593,alex-mikheev,Once rdma_disconnect() was uncommented the deadlock happens consistently.  I agree that this patch is going to interfere with HA pr so lets push it first and i revisit the code afterwards.
594,nitzancarmi,great.
595,maxgurtovoy,where ? only driver_delete_sq does it.
596,maxgurtovoy,please use j < i notation
597,nitzancarmi,nvmf_emu_ctrl_delete_sq does it.   I think you once had a commit to remove it that you abandoned.
598,nitzancarmi,Anyway  I changed it now to be more symmetric.
599,maxgurtovoy,If you want to set the queue state under the lock it's possible too. let's do it.  all the other functions are void.
600,maxgurtovoy,isn't there a similar function in rdma-core ?
601,alex-mikheev,i am waiting for HA then i will do rebasing   testing. 
602,alex-mikheev,@nitzancarmi @maxgurtovoy Any chance that you remember its name ? 
603,maxgurtovoy,let's add assertion in case sqid == 0 in all create/delete callback in nvmf_emu_ctrl
604,maxgurtovoy,what about the layering we talked about ?  this can be set in nvmf_emu_create_ctrl
605,maxgurtovoy,please explain regarding the nvmf_ctrl states. There wasn't a good explanation in the commit it want pushed.  Also let's make sure the mutex is unlocked after  create  is finished.
606,maxgurtovoy,should we check ctrl state here ?  why isn't it symmetric to the creation ?
607,maxgurtovoy,same here - increase the critical section
608,maxgurtovoy,same comment as for  destroy 
609,maxgurtovoy,I'm talking about this opposite logic in each commit.  Please take it into consideration in your next commits.
610,maxgurtovoy,no logic in the for clause please.  Try to ease the code.
611,maxgurtovoy,what is faulty ep ?  Where is all the logic we discussed regarding killing the ctrl and trying to create it again ?  you call recover after KA Timer expired  what is next ?  please explain in the commit message. 
612,maxgurtovoy,I really don't understand the logic.
613,maxgurtovoy,unlock + lock queue_state ?
614,nitzancarmi,I prefer the name `ka_timeout_ms` like in `size_mb` attribute. it will reduce bad configurations of people putting just  15  and gets wierd timeouts.
615,nitzancarmi,If ka_timeout has to be consistent across all endpoints  it should be attribute of io_driver (and parsed the same place as backend type  for example)  and just assinged here.  If we want to be more flexible (in HA for example I might see a reason of different keep-alives to different ports)  then it should appear in different place on config file (attribute of each path in backend.paths].    @maxgurtovoy @alex-mikheev what's your view for this?
616,vasilyMellanox,fixed
617,vasilyMellanox,fixed
618,maxgurtovoy,this shouldn't be mandatory param. Set default to 15000 in case it's not in json file
619,maxgurtovoy,just print a warning and set to minimal size.
620,maxgurtovoy,_MS
621,vasilyMellanox,fixed
622,vasilyMellanox,fixed
623,vasilyMellanox,fixed
624,maxgurtovoy,let's make sure before the for loop that we don't have multiple endpoints for non NVMF_RDMA ep_type.
625,maxgurtovoy,is this connected to this commit ?
626,alex-mikheev,can you please  add queue  d: before the messages to easy on the log analysis.  
627,alex-mikheev,may be it is worth to move swtch()... code to the small helper functions both here and when you do error handling
628,alex-mikheev,can you please add exact reference. like 5.1 for example
629,vasilyMellanox,fixed
630,maxgurtovoy,need to check that status is ok and result is failed
631,vasilyMellanox,in gtests the checking done by ASESERT macros  this what exactly happens inside check_completion function  so we don't check return value  please look on other tests 
632,alex-mikheev,why is emu_eanbled is here
633,nitzancarmi,DELETING state doesn't have any reference in commit message.  Maybe we can just delete it (if not used later in HA commits).
634,nitzancarmi,maybe we can skip this sem_post by just initializing semaphore to 1 in sem_init(). Not really important.
635,alex-mikheev,Can you please add a state machine description from the commit message here ?
636,nitzancarmi,please add comment that it must be called under lock
637,nitzancarmi,*
638,nitzancarmi,*
639,nitzancarmi,*
640,nitzancarmi,This will not be safe when moving to multiple endpoints.  consider a scenario where all endpoints fail at the same time (kill target):  1. first ep (active) assigns itself in driver->fail_ep  2. second ep (non-active) assigns itself in driver->fail_ep  overriding first one.  3. driver_thread looks at the value  sees only the second one  and doesn't handle the deactivation of active_ep.
641,nitzancarmi,maybe add explanation that this  if  is for handling configured (but not actually created) queues.
642,alex-mikheev,We can keep most of this code in nvmx. We only need patch to add fd to the devx context. 
643,alex-mikheev,lets have this code in a separate function. 
644,maxgurtovoy,I can't find it.  can you add this to functionality to libibverbs/enum_strs.c in upstream rdma-core ?  
645,mike-dubman,would avoid using perl where sed can be used (perl may not be part of BF install  also drags many deps for rpm.  ```  sed -i -e  s /tmp/nvmx-[0-9.-]+\.tar /tmp/nvmx- {scm_version}- {scm_rev}.tar   {RPM_BUILD_ROOT} {_datarootdir}/nvmx/ansible/inventory.yaml  ```
646,yshestakov,ok  going to fix it now
647,yshestakov,sed doesn't recognize enhanced regex like `[0-9.-]+` :(  need to replace '+' suffix by '*'
648,mike-dubman,why not to disable it in ```chkconfig```?
649,yshestakov,We don't control machines on the customer site. FirewallD could be enabled for some reason  like the machine is connected to  untrusted  network or so.
650,mike-dubman,is it running on host of BF machine?
651,yshestakov,It is running on the host to which BF is connected by RSHIM interface  it could be USB (BF is installed into the Windows server) or PCIe-RSHIM (the same host).  In the case of MSFT  8 BFs were connected to the same Linux host.
652,yshestakov,Well  we can provision BF over ETH network too  i.e. RSHIM connection is not mandatory.  However in suc case we need to control DHCP/PXE services for the ETH network to which BF port  1 is connected.
653,yshestakov,Please rename this from as `nvmx-fw.spec`.  `snic.spec` doesn't reflect exact purpose of this file
654,mike-dubman,would externalize /opt/mellanox/smartnic/fw into var ```_prefix``` and use it.  one will be able to support pkg rellocation.
655,mike-dubman,would add  /  to make sure if user provide path w/o trailing shash.  ```   {buildroot} {mlx_prefix}/smartnic/fw  ```
656,maxgurtovoy,please be consistent in all place change to:  sizeof(size) and not sizeof size
657,maxgurtovoy,*buf ??
658,maxgurtovoy,*
659,maxgurtovoy,*
660,maxgurtovoy,why are you changing coding style here ?
661,maxgurtovoy,be consistent. use this description for all uint*_t/int*_t.  maybe we would like to remote int/unsigned int option ?
662,maxgurtovoy,which code styling is used in this file ? tabs or spaces ?
663,maxgurtovoy,you can use casting instead.
664,maxgurtovoy,same here 
665,maxgurtovoy,sizeof(size_mb)
666,maxgurtovoy,what is the size of boolean ? 1 byte ?
667,maxgurtovoy,google:  -------------  The size of the pointer basically depends on the architecture of the system in which it is implemented. For example the size of a pointer in 32 bit is 4 bytes (32 bit ) and 8 bytes(64 bit ) in a 64 bit machines. The bit types in a machine are nothing but memory address  that it can have.
668,alex-mikheev,should be buf sizeof(*buf) will be 1
669,alex-mikheev,grep shows that we use sizeof() with braces in most cases. Lets stick to that
670,alex-mikheev,are you sure it should be value->len + 1 and not value->len because i think that snprintf takes trailing '\0' into account automatically
671,alex-mikheev,pls fix indentation settings so that they are inline with what is used in the file
672,alex-mikheev,This seems to be a religious issue. They are arguments for both ways. Have fun reading https://stackoverflow.com/questions/605845/do-i-cast-the-result-of-malloc    json code seems to be in the not casting camp.  
673,vasilyMellanox,buf is a char array - so no issue here
674,vasilyMellanox,no changes - it is different set of function -   nvme_json_number_to_uint64  = nvme_json_decode_uint64    nvme_json_decode_uint* - part of them were on the original SPDK parser  so I am using their original code style
675,vasilyMellanox,Alex is right - this template I took from the similar funcs in original SPDK parser
676,vasilyMellanox,nvme_json_decode_bool (this is not mine - SPDK implementation) returns it as bool ( include <stdbool.h>) - so yes  it is 1 byte
677,vasilyMellanox,This is not a pointer - it's char array  so no issue here
678,vasilyMellanox,Discussed with Alex  no issue here
679,vasilyMellanox,fixed
680,vasilyMellanox,The parser gets sizeof out buffer and cannot be aware if it uint32_t or unsigned
681,vasilyMellanox,changes as on origin - They use tabs there...
682,yshestakov,suggestion:     `_rev= pr ghprbPullId `
683,mlx3im,+
684,maxgurtovoy,code conventions.  
685,maxgurtovoy,maybe we need to memset the cdata in nvmf_emu_stop_ctrl before nvmf_emu_disable_ctrl (opposite to identify_ctrl).
686,maxgurtovoy,lets check ctrl->num_ns here and set it to 0 too.
687,maxgurtovoy,there is a typo here I guess
688,maxgurtovoy,this might be endless loop.
689,maxgurtovoy,alignment should be as in printf  to first char after (.  all the below also.
690,vasilyMellanox,fixed
691,vasilyMellanox,fixed
692,vasilyMellanox,fixed
693,mike-dubman,i would also add template to github nvmx repo  https://help.github.com/articles/creating-a-pull-request-template-for-your-repository/    see good example here: https://github.com/openucx/ucx/blob/master/PULL_REQUEST_TEMPLATE.md  
694,yshestakov,Ok  going to learn it
695,yshestakov,Done  added `pull_request_template.md` template to the repo
696,mike-dubman,(max 50 char)
697,mike-dubman,can remove for briefity
698,yshestakov,Ok  going to reduce from 60 to 50 chars per  subject  line
699,mike-dubman,can be removed as appearing in 1st line
701,yshestakov,ok  fixed
702,yshestakov,fixed
703,mike-dubman,redundant sentence  would just add max 72 char to 1st line
704,mike-dubman,seems redundant
705,yshestakov,done
706,yshestakov,fixed
707,maxgurtovoy,let's decide that default is 0 (not supported if not exist in the json).
708,maxgurtovoy,these 3 fixes/improvements should be pushed in separate commit.  It's not related to ONCS
709,maxgurtovoy,if only second bit allowed you should check if oncs  = DSM
710,maxgurtovoy,this is not informative comment for someone that is not the feature writer.
711,maxgurtovoy,please improve the comment here.  add explanation to all 6 bits and mention that only DSM is supported.
712,maxgurtovoy,same as above.
713,maxgurtovoy,no need to check DSM. just check if oncs support in target is >= wanted oncs
714,maxgurtovoy,see above for this too.
715,maxgurtovoy,see example in nvmf_ctrl_verify_namespaces_configured. there is too much un-needed code here.  It's also not the place to put this check in a basic cmd such as identify_ctrl.  add a function called nvmf_ctrl_verify_oncs_configuration.
716,maxgurtovoy,not really needed
717,maxgurtovoy,nvmf layer shouldn't care about DSM. This is validated in the controller layer.  Here you should care only about oncs (need wider perspective).    if (err < 0)      oncs = 0;    if (oncs    (oncs   ctrl->cdata.oncs)) {      nvmx_error();      return error;  }    return ok;
718,maxgurtovoy,there is a new error flow we need to add.  stop_io_queues in case we started it above.
719,maxgurtovoy,only:  if (start_io_queues)
720,vasilyMellanox,This is exactly what happened - please look at the code before
721,maxgurtovoy,No it's not.  you stop the IO queues unconditionally. you need to stop them only if you started them.
722,vasilyMellanox,fixed
723,alex-mikheev,let's rename to devx_qp_init_internal and reduce number of resources taken by the shadow_qp.  There is not need in sq  rq and cq
724,alex-mikheev,Lets move capabilit check for the device_emulation_manager  cmd_on_behalf and qp_by_name to a separate helper function which will be called from both nvme_init_emu_dev and mlx_emu_dev_query()
725,alex-mikheev,nvme_init_emu_dev shall fail  no need to call query here
726,alex-mikheev,add SKIP_TEST_ON_DEV(nctx) 
727,alex-mikheev,lets add a check that shadow qpn is equal to the qpn
728,alex-mikheev,Let's move changed structs to nvme_mlx5_ifc.h in order to avoid patching devx
729,alex-mikheev,Can you please check if DEVX_ADDR_OF() can be used to get offset of tha capability.cmd_hca_cap. And then use DEVX_GET()   Similart to what is done in devx_qp_init(). This way we will not have to carry 2 extra structs
730,vasilyMellanox,Impossible -  DEVX_ADDR_OF is boiled down to __devx_bit_off where I should specify the actual field name. In other words - if I don't have any struct that contains my new field how could I init that new field ? for example 'input_qpn' begins after reserved_at_40[0x8] (reserved_at_40[0x20] originally) 
731,vasilyMellanox,fixed
732,maxgurtovoy,please fix the buggy error flows in this function.
733,maxgurtovoy,Need to fix error flows here too.
734,maxgurtovoy,if we fail here  we'll leak..
735,maxgurtovoy,please add a comment with explanation what is shadow QP and why we need it 
736,maxgurtovoy,alignment
737,maxgurtovoy,here too
738,maxgurtovoy,here
739,maxgurtovoy,nvme_check_mandatory_dev_caps *  Also should be static probably.
740,maxgurtovoy,I'm not sure that this is the place to call this function from layers POV.  You also call it in different place..
741,maxgurtovoy,redundant
742,maxgurtovoy,free(qp->shadow_qp)
743,maxgurtovoy,you can move it to devx_verbs and call it devx_is_emulation_supported (or something similar)
744,maxgurtovoy,code conventions.  if (condition)
745,maxgurtovoy,let's return NVMX_OK in the functions called in nvme_ctrl_init and check if err  = NVMX_OK.  There is a big mess there (nvme_ctrl_id_init and nvme_ctrl_ns_init).
746,maxgurtovoy,please add nvme_ctrl_id_uninit function (might be empty) to all error flows.  please fix the order in nvme_ctrl_destroy function.
747,maxgurtovoy,I don't understand how this works ? nn should be in little endian.  id->nn = cpu_to_le32(nvmx_max(le32_to_cpu(id->nn)  ctrl->namespaces[i].id));
748,maxgurtovoy,maybe remove nvmx_log altogether ? and use nvmx_info instead ?  advice with AlexM.
749,maxgurtovoy,for bool use true/false no 1/0
750,galshachaf,fixed
751,maxgurtovoy,you should break lines *only* if you got 80 chars. Otherwise  stay in the same line.  This is also for all other similar breaks in this PR.
752,maxgurtovoy,don't assign arguments after non initialized args.
753,maxgurtovoy,see my fix to status value read
754,maxgurtovoy,same here for status
755,maxgurtovoy,rkey == 0 ??  IMO you should register the buffer here and put the rkey...
756,vasilyMellanox,if it's zero => it will be replaced with q->worker->dev_ctx->prm_ctx->dma_rkey (do_dma_xfer func)
757,alex-mikheev,please add a comment that 0xff is a vendor specific admin/nvme command
758,alex-mikheev,i guess there is no need to rename has_completion to completion
759,nitzancarmi,I didn't understand how this change (and the hard-coded change in the end) is related to the rest of commit.  Maybe it is better to seperate these commits (although actually  I don't understand the change at all - isn't the  whole point is to get this value from FW instead of  define)?
760,nitzancarmi,I think we don't need the -1 here anymore. originally NVME_EMU_NUM_QUEUES was 33  to mention 32 io queues + 1 admin queue. From FW perspective  there are only 32 queues (admin queue is managed by SW)  so cap->num_queues == nr_io_queues.
761,nitzancarmi,see comment above
762,nitzancarmi,please fix indentation.
763,nitzancarmi,I see this function behaves a little different.  IMO  `struct nvme_query_attr dev_caps` should be an attribute of nvme_emulator (and no nvme_ctrl_t)  same place as PCI bar for instance   e.g. queried and filled during nvme_emu_dev_open().    That way  we don't need another local instance of nvme_ctrl_t (a little odd to me)  and any future usage of these caps (like in setup_queues and id_ctrl) will be easily taken from ctrl->dev. 
764,nitzancarmi,Is there a reason why src/devx submodule is updated here?  most likely it sneaked into commit when doing `git add -u` or something :-)
765,nitzancarmi,explained by gal. It will be removed completely on later commits.
766,alex-mikheev,Is ctrl->reg_size used anywhere ?  If it does we better flag an error here if reg_size cannot accomodate number of queueus. 
767,alex-mikheev,Please move ifc bits to the src/nvme_mlx5_ifc.h
768,alex-mikheev,shouldn't it be max_dev_nr_queues-1 ? 
769,alex-mikheev,can you remove ctrl->reg_size ?
770,nitzancarmi,I think that we don't need the -1.  from FW POV  max_dev_nr_queues  is 32  which is effectively how many IO queues it can support   since we don't use FW for admin queue.  
771,alex-mikheev,Admin queue is also handled by the FW  the difference is that admin queue is always in the sqe_only mode.
772,galshachaf,changed
773,maxgurtovoy,your code is inconsistent. don't do this mixture between align and not aligned  = 
774,maxgurtovoy,if you return NVMX_ERR above  here you should use NVMX_OK
775,maxgurtovoy,Gal   please answer and resolve this issue.
776,maxgurtovoy,please fix the test.  I don't know what it's doing and it's not written clearly.
777,maxgurtovoy,nvmx_min ?
778,maxgurtovoy,I'm surprised it works.
779,maxgurtovoy,same here.
780,maxgurtovoy,need to comment it.
781,maxgurtovoy,80 chars per line is fine.
782,maxgurtovoy,if (rc) {    }
783,maxgurtovoy,why are you looking on json file in QEMU layer ?  nvme_emu_get_num_queues should be in common code.
784,maxgurtovoy,this should be the first commit in the series.
785,galshachaf,removed
786,galshachaf,ok  moved
787,galshachaf,something like  current values reported by FW ?
788,galshachaf,how come? gtest is cpp based
789,galshachaf,still based on cpp
790,galshachaf,fixed
791,galshachaf,ok
792,galshachaf,fixed
793,galshachaf,@alex-mikheev   could you please clarify how to fix this test?
794,galshachaf,ok
795,galshachaf,ok
796,galshachaf,fixed and added comments
797,galshachaf,done
798,maxgurtovoy,args alignment please.
799,maxgurtovoy,alignment + need to fix the comment above (or function name is wrong. should be set or return the number instead of void)
800,galshachaf,changed to set
801,galshachaf,aligned
802,maxgurtovoy,code convention.  int i  ret;
803,maxgurtovoy,maybe pass num iteration as argument to function ?
804,vasilyMellanox,Maybe make sense replace 0x1 with '1' as soon as u already did it in other places 
805,vasilyMellanox,So it means that SQ size must be the same as CQ size  otherwise the phase is calculed based om CQ counters
806,vasilyMellanox,I don't see where head counter is proceeded - could be an issue for phase calc
807,alex-mikheev,yes  sync commands assume 1:1 sq per cq mapping
808,alex-mikheev,right  that was fixed in later commits in this pr
809,vasilyMellanox,Shouldn't you care if overlapping is happened and change the phase in this case  otherwise  uint8_t  phase   = sq_ctrl(qid)->phase;  will give the wrong phase for next call for this func
810,alex-mikheev,command_prep() calls io_q->add_tail() advances sq_tail and phase  
811,alex-mikheev,ah you are right  i pushed the fix + admin q stress test
812,maxgurtovoy,*NVME_IDENTIFY_CNS_ACTIVE_NS_LIST  please use existing macros. Let's try to avoid using constants as 1/2/3...  We already have thousands on code lines and it's hard to maintain.
813,maxgurtovoy,didn't we agreed to use an argument for num iterations ? like in the func below ?
814,maxgurtovoy,NVME_CSTS_READY >> CSTS_RDY_SHIFT
815,maxgurtovoy,NVME_CSTS_SHST_COMPLETE >> CSTS_SHST_SHIFT
816,maxgurtovoy,can you mention the new test implementation in the commit message ?  I didn't understand how it's related to this commit...
817,maxgurtovoy,can you use num_cmds instead of N ?  are these commands are async ? if so  why are we sending more that q_depth ?  if not  I guess the test only sends admin cmds in a loop ?
818,maxgurtovoy,I really prefer a good planning of commit instead of having N commits that are  dirty  and the N+1 commit clean all the  dirty  stuff.
819,maxgurtovoy,maybe it's worth using the helper function I added for cqe status for code readability.
820,maxgurtovoy,why we need if 0 here ?  
821,maxgurtovoy,here you remove the if 0 from previous commit ?  It's not so interesting knowledge to keep in git...
822,alex-mikheev,commands are sync. Test checks that we can send 2*q_depth commands
823,yshestakov,On the first iteration value of ` ?` comes from a command above (outside of) `while [  ? -eq 0]` loop.  I guess it's not very correct on one hand. On another hand we have this scrip running with `-eE` options. So let's keep it as is.
824,nitzancarmi,please update comment to right explanation
825,galshachaf,updated  thanks
826,maxgurtovoy,*real_size_mb
827,maxgurtovoy,as we discussed  lets make it symmetrical. Every function should clear the error flow internally.  if nvmf_ctrl_update_namespaces failed we just free(ctrl->ns).  if nvmf_ctrl_verify_namespaces_configured(ctrl) failed we run nvmf_ctrl_clear_namespaces(ctrl); + free(ctrl->ns).  I prefer not to call the symmetrical function from the peer one (construct_a should not call destruct_a. It should call destruct functions of all the components of it that succeeded).  
828,nitzancarmi,Maybe we need to add a check of the matching CQ's state  since we don't want to create SQ without supporting CQ.  It is not really important since we create dummy CQs today and always succeed  but still.
829,vasilyMellanox,indentation issue
830,vasilyMellanox,indentation issue
831,alex-mikheev,Lets change queue depth to a define
832,maxgurtovoy,please use the status MACRO.
833,maxgurtovoy,nvme_ctrl_get_log_page_cmd
834,maxgurtovoy,fail if you get unsupported page_id.
835,maxgurtovoy,need to comment that we currently support only non-extended get-log-page interface (only Number of Dwords Lower (NUMDL))
836,maxgurtovoy,why true ?  it's async  right ?
837,maxgurtovoy,add a error print.  no need to return rc inside the if (see keep alive example).
838,maxgurtovoy,Did you ever get here ?
839,maxgurtovoy,how is this related ?  not needed assert.
840,maxgurtovoy,check here if ( supported_oaes) return 0;
841,maxgurtovoy,i don't understand this login to sem_wait in recv_completion.
842,maxgurtovoy,where do we use this naming convention ?
843,maxgurtovoy,please remove dead code in different commit
844,alex-mikheev,please rename value to exp_value and mention -EINVAL  return if conf > exp
845,maxgurtovoy,free_ns:      ctrl->num_ns = 0;      free(ctrl->ns);
846,maxgurtovoy,strange that we need this nvme_ctrl_qparams call only to get q_type for the next call.  Need to refactor the code to something more cleaner.
847,maxgurtovoy,same here.  q_type is save in ops.
848,maxgurtovoy,where do we free namespaces arr ?
849,nitzancarmi,I noticed that too.  There is a huge mess here with the q_ops.   I intended to push it as a seperate PR  but I'll just attach it here as a third commit.
850,maxgurtovoy,please add error flows for the below for loop.
851,maxgurtovoy,let's use 64/256 here for problematic users.
852,maxgurtovoy,no real need for these local variables  right ?
853,maxgurtovoy,what about ctrl_db_ops ? consult with Sasha please.
854,maxgurtovoy,see above
855,maxgurtovoy,same here. let's not use the stack if no real need.
856,maxgurtovoy,is this a mandatory param ? if so  we must fail.
857,yshestakov, please open nvmx.spec at line 52  ```   51 cd src/devx   52  patch0 -p1   53 cd ../..  ```  you need to add  2 lines  ```   patch1 -p1   patch2 -p1  ```    assuming you have added  Patch{1 2}  in the header at lines 35-36 above
858,maxgurtovoy,typo:   entries 
859,maxgurtovoy,please put the initialized param before uninitialized.
860,alex-mikheev,fixed
861,alex-mikheev,fixed
862,maxgurtovoy,redundant space ?
863,maxgurtovoy,use upto 80 chars per line.
864,maxgurtovoy,please explain in the commit message the SQ state machine.
865,maxgurtovoy,need to check with sqe_only and sqe_cc modes too.  Please add it to  Tests  section in commit message.
866,maxgurtovoy,not in this commit as agreed.
867,maxgurtovoy,redundant space
868,maxgurtovoy,I don't see where you modify state to ERROR and to INIT
869,maxgurtovoy,spaces.  something went wrong here.
870,maxgurtovoy,don't pass attr == NULL.  set it if needed and pass the right mask from the caller.  Also for other states as well
871,vasilyMellanox,nothing wrong - look at  man qp_modify  init stage
872,maxgurtovoy,why don't you use IBV_QP_RQ_PSN and IBV_QP_PATH_MTU mask ?  you should reflect each attr you set in the attr_mask instead of checking if the value is not 0
873,vasilyMellanox,PSN and MTU are necessary  it must be set - so I don't see a reason to redundant code to set mask manually each the time    My goal (from your own words) was to set  primary_address_path and not each param  There are no flags for tclass  sl  etc. - this is the reason why I check if it non-zero
874,maxgurtovoy,you need to memset the attr here.  I know this code works  but every code you write should be right and unbreakable (I can break it by adding wrong assignments in devx_qp_modify_to_rts. for example use the rq_psn or path_mtu).
875,vasilyMellanox,fixed
876,maxgurtovoy,| NVME_DNR
877,maxgurtovoy,can you be consistent ?  set all the 3 funcs below to non void ones (not only smart log)  we want readable code.
878,nitzancarmi,done
879,nitzancarmi,Done
880,maxgurtovoy,shadow qp should be given vhca_id as well ?
881,maxgurtovoy,don't assert here. just return error value with nvmx_error print that says that this device don't support it.
882,maxgurtovoy,please comment here that you support only 1 pf currently.
883,maxgurtovoy,please add debug print for all emulated_pf_info attributes.
884,maxgurtovoy,this should be in seperate commit (can be in this series)
885,maxgurtovoy,are you sure ? isn't it only for on_behalf stuff ?
886,maxgurtovoy,see above
887,maxgurtovoy,how is this related to renaming ?  please check git log why we put u32 here.
888,vasilyMellanox,no need  it's a regular QP - not on_behalf
889,vasilyMellanox,It's just a bug
890,maxgurtovoy,so why are you passing it ?
891,vasilyMellanox,It s right - the function doesn t use this param in not on_behalf case  but  it s on internal function logic   to decide when each param is used and when not   when I exploit the function prototype   on me to provide all params and let to function do its work  
892,vasilyMellanox,The param will be used in QP_ON_BEHALF case only
893,maxgurtovoy,no. you don't pass wrong argument to the function. and you also do use it in non on_behalf flow:  qp->vhca_id = vhca_id;
894,maxgurtovoy,don't pass wrong args.
895,vasilyMellanox,what's wrong in it ??? I  as a func user  cannot know how the func use its arguments (ADT principles)
896,vasilyMellanox,qp->vhca_id = vhca_id; - It i s not a usage  just some field init in user defined struct  this field will be used only if the qp is on_behalf
897,vasilyMellanox,Here I prefer leave as it - instead of adding additional if statement 
898,nitzancarmi,Note that since you called devx_init_dev()  at the end of this small application  FW will generate internal FLR.  So maybe we don't care about it  but if we do - we should include this patch along with the patch for  exclusive open  of interfaces.
899,alex-mikheev,devx_init_dev() does not touch nvme gvmi. There should be no FLR
900,nitzancarmi,Please initialize the semaphore before registerring logger_handle_sig that uses it.
901,nitzancarmi,Please follow code conventions for comments:  /*   * bla bla bla   */
902,nitzancarmi,Let's avoid using dynamic allocations from strdup  this is a little confusing  epsecially when using static pointers. It's more readable to stick with static buffers (256 size?) and strcpy. (same for log level).
903,mike-dubman,typo: s/defiend/defined
904,nitzancarmi,stdout  not stdio
905,nitzancarmi,This is boolean  not int.
906,nitzancarmi,please call that enum a name (nvmx_logger_log_level?) and use it as  a type instead of  int .
907,nitzancarmi,SUFFIX  instead of POSTFIX
908,nitzancarmi,maybe assert will be enough
909,nitzancarmi,  =  
910,yshestakov,Please add ` {RPM_BUILD_ROOT}` prefix to the above lines
911,yshestakov,Who will create `/var/log/logger_test` directory for you (nvme logger)?  I guess you need to put `/var/log/nvmx` there because in the `nvmx.spec` you're creating `/var/log/nvmx` directory
912,hellerguyh,you are right.
913,mike-dubman,plz add LOG_ERROR
914,mike-dubman,LOG_ERROR on fail exits to ease debugability
915,nitzancarmi,code conventions: nvmx_emu_log_level
916,nitzancarmi,I think it is more readable to merge this 2 prints into 1 ( Port  d state  s ( d) )
917,nitzancarmi,instead of the  if (ctx)  just use 2 different goto labels  e.g.  out_free_dev_list  and  out_close_device 
918,nitzancarmi,maybe it is better to use `continue` instead of `goto cleanup`
919,nitzancarmi,please use the same assert as used in other files (`nvmx_assertv_always`)
920,nitzancarmi,you don't need `\n` here (already done inside LOG_ERROR)
921,nitzancarmi,redundant `\n`
922,nitzancarmi,why is it attached here (uninitialized and not even printed in string)?
923,nitzancarmi,No `\n` needed in nvmx_*() functions. please go over all other strings too and verify it.
924,nitzancarmi,redundant `\n` here too
925,hellerguyh,are you sure? This code convention was used in src/nvme.h  that's why I used it.   
926,nitzancarmi,I see. AFAIK that was because we took those things out of APDK.  Anyway  not that critical  you can leave it that way also for now.
927,hellerguyh,smart assert calls nvmx_fatal  that will call again nvmx_logger
928,hellerguyh,you are right  fixed
929,nitzancarmi,ok
930,alex-mikheev,please change to  (IBVERBS_LDFLAGS) -lpthread  better way would be to add AC_CHECK_LIB(pthread_init  pthread)   
931,alex-mikheev,Better not to add libs to CFLAGS
932,alex-mikheev,please don't use double underscore __. Single is ok. See https://stackoverflow.com/questions/10687053/meaning-of-double-underscore-in-the-beginning
933,alex-mikheev,may be rename to open/close_log_file_nolock ?
934,yshestakov,`/etc/logrotate.d/nvmx` is a regular file  not ` dir`
935,hellerguyh,well actually it's a directory in which we put the script nvmx_log (the next line)
936,hellerguyh,do you think it's better to put it not in a folder?
937,yshestakov,I never saw subdirectories under `/etc/logrotate.d/`    Usually every package puts its logrorate script into this directory by name of the package
938,yshestakov,one more issue:  why `0644` mode for directory? I believe it should be `0755`
939,yshestakov,in general it's enough to do:  ```  mkdir -p  {RPM_BUILD_ROOT} {_var}/log/nvmx/  mkdir -p  {RPM_BUILD_ROOT} {_sysconfdir}/logrotate.d/  ```
940,hellerguyh,ok
941,hellerguyh,ok  changed  thanks
942,maxgurtovoy,can you add mask for all ONCS fields while we're here ?  And also please move it to first commit that use this macro.
943,maxgurtovoy,how this can be non zero value ?
944,maxgurtovoy,can you make the code more readable ?  use a local variable for power_state or use macro mask NVME_POWER_STATE().  also please fail in case the value > id->npss  
945,maxgurtovoy,same here like npss
946,maxgurtovoy,maybe do fallthrow on all
947,maxgurtovoy,can you add mask for aggr_time for more readable code
948,maxgurtovoy,can you add mask for aggr count for more readable code?
949,maxgurtovoy,here you can use the same mask as defined earlier
950,maxgurtovoy,here too
951,yshestakov,risky string addition. Somebody cold define `LOG_FILE_FOLDER` value without trailing `/`. So it's safer to add `/` here explicitly.
952,yshestakov,Why `./scripts/nvme_snap` instead of `/etc/logrotate.d/nvme_snap` ?
953,hellerguyh,since I don't want to check using the previous installed version of nvme_snap script but with the newly created script which matches the version i'm working on.     I think this is true for all the program calls in the test  but I didn't want to change it in this unrelated PR.
954,yshestakov,I see. thanks
955,yshestakov,BTW  on the CI machine (docker container) we don't install nvmx (nvme-snap) RPM  so that no `/var/log/nvme_snap` directory exists.    It means `opendir(...)` could return NULL which is not checked below in the code
956,hellerguyh,readdir return NULL in case pointer is NULL so no segfault will not occur. nevertheless i've added a test for more correct code.
957,mike-dubman,one can use ``` if  {?_strict_checks:1} ``` to uncomment ```BuildRequires``` for cases when used from CI
958,yshestakov,I guess the above dependency on `rdma-core-devel` needs to be changed according to the version of this package in MLNX_OFED 4.6
959,maxgurtovoy,can you change the commnet to be:  /* 1 BSF size is 64B (4 octwords) */
960,maxgurtovoy,need to run HA flows here and unload/load in a loop
961,nitzancarmi,Done
962,nitzancarmi,Done.
963,maxgurtovoy,Yurii   I think we should change the approach. I actually think we should update the /etc/default/nvmx (first need to rename it to /etc/default/nvme_snap.conf)    in this conf we should add a map:    mlx5_2:/etc/my_config2.json  mlx5_3:/etc/my_config3.json    according to this map we should  1. check /usr/sbin/mlx_device_emulation  2. check and configure switchdev mode  3. run the  /usr/bin/nvme_controller_emu with the correct params.    this will enable the service to open all configured functions in the map and not only 1.
964,alex-mikheev,can it be avoided ?
965,alex-mikheev,it looks like we don't need nctx here  devx ctx is enough
966,alex-mikheev,can we avoid exposing prm_ctx ?
967,maxgurtovoy,please remove this condition.
968,maxgurtovoy,struct = struct ?  you can do memcpy or use raw uint32_t with union in the structure as spdk.
969,maxgurtovoy,why do you have so complicated logic ?  why not to have 1 flow for identify success and second on for identify failure ?
970,maxgurtovoy,did it really happen and fix something or this is just nice to have line ?
971,maxgurtovoy,see comment above
972,nitzancarmi,done
973,nitzancarmi,done
974,nitzancarmi,You're right  At second look  I can merge both flows to nvmf_ctrl_update_namespace() which is much clearer (I had problems before in doing it  but they're gone as we did some code changes).
975,nitzancarmi,It avoids possible redundant calls fo fail_ctrl  but it's not really a bug. removed it.
976,nitzancarmi,Fixed above (to both use memcpy)
977,maxgurtovoy,I don't think it's the right place to put this code.
978,nitzancarmi,@maxgurtovoy  I put it at the same point as TAILQ_FOREACH_SAFE  which was already there.  Where do you think should I move them both to? src/compiler.h?
979,maxgurtovoy,Yes
980,maxgurtovoy,please mention also the ctrl.  this should be nvmx_debug.
981,maxgurtovoy,see above.
982,maxgurtovoy,please don't use __func_name convention.  maybe nvmf_ctrl_find_namespace_lock..
983,maxgurtovoy,why are you using kernel namings ?  how come it's a void function ?
984,maxgurtovoy,don't use do/while.  where did we use that in this file ?
985,maxgurtovoy,can you explain that ?
986,maxgurtovoy,any good reason for this commit ?  you allocate 4k on the stack here...
987,maxgurtovoy,here you use QTAILQ and in nvmf STAILQ ?
988,maxgurtovoy,redundant assert.
989,maxgurtovoy,unlikely ?
990,maxgurtovoy,these checks are not related to add_ns
991,maxgurtovoy,why did you move this code up ?
992,maxgurtovoy,this commit is doing much more that you described in commit message.
993,maxgurtovoy,why are you removing this code ?
994,maxgurtovoy,what ??
995,maxgurtovoy,please leave pi_type
996,nitzancarmi,Done.
997,nitzancarmi,done
998,nitzancarmi,done
999,nitzancarmi,1. I named back this function name. It just seem confusing to me (since later we would have another  update  of namespaces from get log page).  2. This is void since we actually don't care about which namespaces are discovered or not  as long as nvmf_ctrl_verify_namespaces_configured passes. This actually fixes a bug in today's implementation - where if we fail to identify a namespace that is not really needed in configuration file  we fail the connection (for no good reason ). I added an explanation to commit message on it.  
1000,nitzancarmi,I changed the implementation a bit to avoid using do/while.  I'm just trying to avoid iterating over num_pages in a for loop  as it is dependant in a changeable variable (NN).  For example: if we had 1023 namespaces when we start iterating on that list  and at the middle 2 additional namespaces were added (now we have 1025)  we will miss the last one (since it will be in a new page  not requested by us).
1001,nitzancarmi, Yes.  As I moved to using linked list  I had a problem keeping namespaces in 2 different lists in parallel -   changed  (changed_ns_list) and  current  (ctrl->ns_list)  So I started using  old / new  lists instead. Any new/modified namespace identified  is detached from the old (ctrl->ns_list) list  and be added to the new one (new_ns_list).  That way we don't need to look for namespaces we didn't see in the changed_ns_list - we can just delete all remaining  namespaces in the old list (which contains only namespaces that are no longer identified). From there  we can just use the new_ns_list from now on.  I refactored this a bit (but logic stays the same) - I think now it is clearer. Also explained in commit message.
1002,nitzancarmi,removed this commit completely. 
1003,nitzancarmi,Moved every listv to work with the QTAILQ notation.
1004,nitzancarmi,Removed
1005,nitzancarmi,Moved to seperate function nvme_ctrl_check_namespace_attrs
1006,nitzancarmi,done
1007,nitzancarmi,Done. Splitted to 2 seperate commits: validate Json file extensions  and actual linked list implementation.
1008,nitzancarmi,Done.
1009,nitzancarmi,I removed this since all the assigned parameters inside were unused.  I restored the spec violation check. Also restored the pi_type  although it is not in any use too (and I prefer not to have redundant code until it is actually used somewhere).  
1010,nitzancarmi,Removed this comment
1011,nitzancarmi,Done
1012,maxgurtovoy,We can't move code from samples to src because of licencing.  This is the bug:   define TAILQ_REMOVE(head  elm  field) do {                             \          if (((elm)->field.tqe_next)  = NULL)                            \                  (elm)->field.tqe_next->field.tqe_prev =                 \                      (elm)->field.tqe_prev;                              \          else                                                            \                  (head)->tqh_last = (elm)->field.tqe_prev;               \          *(elm)->field.tqe_prev = (elm)->field.tqe_next;                 \  } while (/*CONSTCOND*/0)    define QTAILQ_REMOVE(head  elm  field) do {                            \          if (((elm)->field.tqe_next)  = NULL)                            \                  (elm)->field.tqe_next->field.tqe_prev =                 \                      (elm)->field.tqe_prev;                              \          else                                                            \                  (head)->tqh_last = (elm)->field.tqe_prev;               \          *(elm)->field.tqe_prev = (elm)->field.tqe_next;                 \          (elm)->field.tqe_prev = NULL;                                   \    <--------  } while (/*CONSTCOND*/0)
1013,maxgurtovoy,so I recommend to take the _SYS_QUEUE_H_ from /usr/include/sys/queue.h to us and fix it there.  QTAILQ_* is QEMU version (first Q)  Also need to change the controller.c accordingly and look for more diffs in the files.
1014,maxgurtovoy,please remove all the licencing stuff from commit message.
1015,maxgurtovoy,alignment
1016,maxgurtovoy,\ is missing
1017,maxgurtovoy,maybe the compiler is not the right place for it. Just include it  queue.h  is the needed places.
1018,maxgurtovoy,it's confusing. don't do this wrapping
1019,maxgurtovoy,please add a comment here. In which case you do ns initialization ?
1020,maxgurtovoy,maybe need to memset the nsdata to 0 in case of failure
1021,maxgurtovoy,no need to set rc here.
1022,maxgurtovoy,use TAILQ notation. and please add it in the queue.h commit.
1023,nitzancarmi,Done
1024,nitzancarmi,Done
1025,nitzancarmi,Done
1026,nitzancarmi,I need to pass linked list pointer as function argument later on  so I need a named struct for it  so I can't  just work with TAILQ_HEAD(nvme_ns_list  nvme_ns)  as it is actually a declaration for  struct nvme_ns_list .  Do you prefer that I'll use  struct nvme_ns_list  instead (the struct implicitly created inside TAILQ_HEAD macro)?
1027,nitzancarmi,Done
1028,hellerguyh,why do you need to add compiler.h? 
1029,hellerguyh,what's the rationale behind cleaning this field?    if it is cleaned shouldn't also ns->id and ns->ctrl should be cleaned? 
1030,hellerguyh,`nvmf_ctrl_update_namespace` - I think this name is a bit misleading and maybe be better called nvmf_ctrl_update_namespace_in_list.   if we were working in a OO environment this was a list method (and not a namespace method)
1031,hellerguyh,redundant tabs 
1032,hellerguyh,should pi_type also be initialized here? (for cases in which nsdata->lbaf[...].ms = 0)
1033,maxgurtovoy,ok please add a comment near the typedef.
1034,maxgurtovoy,so you do use it instead of nvme_ns_list_t eventually ?  I don't understand.
1035,nitzancarmi,Done
1036,nitzancarmi,You are right. this is no longer needed. removed.
1037,nitzancarmi,If we call init_namespace from create_namespace context   than it really doesn't matter (as we will soon free the non-initialized namespace).  But in case we fail to init_namespaces from modify_namespace context  it means a  good  (in use) namespace became bad - and I might still need some namespace fields in order to decide what next to do with it. For nsdata - I didn't have to memset it  but I wanted to be aligned with ctrl->cdata handling (which is zeroed while it is stopped).
1038,nitzancarmi,You are right. updated it to use nvme_ns_list_t
1039,nitzancarmi,done
1040,nitzancarmi,done
1041,nitzancarmi,In those cases pi_type just remains 0 (as it is a sub-type of metadata).  I didn't change it's current behavior in code here.
1042,maxgurtovoy,if you change this logic please add a test with 8K namespaces configured at the target side and make sure you pass it :)
1043,maxgurtovoy,no really need to have local variable here.
1044,maxgurtovoy,I guess ctrl->ns_list should be empty here. can we add assert if it doesn't ?
1045,maxgurtovoy,make sure you run these 2 tests that were changed. And add it to commit message.
1046,maxgurtovoy,please add a comment that we check that frontend_nsid == backend_nsid since we don't support ns mapping.
1047,maxgurtovoy,we call this func for id > 0 so this can't be NULL never. please add assert here.
1048,maxgurtovoy,please use different rc
1049,maxgurtovoy,please print the rc here.  also nsid
1050,maxgurtovoy,don't use ns_iter variable.  return ns if you have a match. and NULL  in the end of the func.  same as before.
1051,maxgurtovoy,what is this override value ? and it's seems to be unrelated to this commit.
1052,maxgurtovoy,i was talking about the overriding.
1053,maxgurtovoy,why don't you use id_ns zero structure ?
1054,maxgurtovoy,need to return nvme spec return value
1055,maxgurtovoy,i don't know why we do identify to non active ns  but at least lets do it right.
1056,nitzancarmi,done
1057,nitzancarmi,done
1058,nitzancarmi,done
1059,nitzancarmi,done
1060,nitzancarmi,done
1061,nitzancarmi,done
1062,nitzancarmi,done
1063,nitzancarmi,changed.
1064,nitzancarmi,I recalled what the problem was originally. When we get the changed namespaces list via Get Log Page command (in Async events future commit)  we have to decide for each namespace - whether its new/removed/modified namespace. We do it by calling `Identify namespace` - if we fail - this namespace was deleted.    Anyway  Added an explanation to commit message too.
1065,nitzancarmi,nvme_identify_cmd() above already checks the nvme cqe status inside  and return `-1` if it is bad. The same thing happens on the similar nvmf_identify_ctrl(). I wanted to be consistent here.  
1066,maxgurtovoy,you need to lock it as you mentioned in the comment.  You do it in nvmf_ctrl_destruct_namespaces
1067,maxgurtovoy,use ENOMEM
1068,maxgurtovoy,ret = -EINVAL and goto clean_nsdata
1069,maxgurtovoy,you need to set the rc in case of default.
1070,maxgurtovoy,why don't you solve it in the nvme_driver_fail_ep function ?  what is the benefit of new sem ?
1071,maxgurtovoy,these can be moved to be after destroy_id + need to add a comment that it's allocated during cm event and destroied after id is destroyed.
1072,nitzancarmi,done
1073,nitzancarmi,I'm afraid of races. nvme_driver_fail_ep() start with taking the lock. What saves us from this scneario:    1. 2 parallel calls arrive together to nvme_driver_fail_ep()  2. First call takes lock  second one waits.  3. first call moves state to DELETING  and after that state quickly to NEW.  4. Second call takes lock  and start DELETING again (for no reason).    Instead  the new semaphore forbids any  duplicate  calls to be generated (so they won't  wait  on the lock)   and this forcidding is removed only after nvme_emu_ep_start_backend() is started successfully (e.g. any error   from now on would be on the  new  controller).
1074,maxgurtovoy,typo
1075,maxgurtovoy,need to remove 1 new line
1076,maxgurtovoy,what is 0xffffff ?
1077,maxgurtovoy,take only 4 lower bits for lsp according to the spec.
1078,maxgurtovoy,no need to   with 0xFFFF. just use uint16_t for log_size
1079,maxgurtovoy,why is this comment important ?
1080,nitzancarmi,done
1081,nitzancarmi,done
1082,nitzancarmi,done
1083,nitzancarmi,done
1084,nitzancarmi,done
1085,maxgurtovoy,why don't you follow SPDK BSD code ? NVME_GLOBAL_NS_TAG definition ?
1086,nitzancarmi,We can pass 0 or 0xFFFFFFFF according to spec. (for ctrl-wide log pages). changed it to 0 now.
1087,maxgurtovoy,maybe use SPDK example for code readability:  /**   * Asynchronous Event Request Completion   */  union spdk_nvme_async_event_completion {          uint32_t raw;          struct {                  uint32_t async_event_type       : 3;                  uint32_t reserved1              : 5;                  uint32_t async_event_info       : 8;                  uint32_t log_page_identifier    : 8;                  uint32_t reserved2              : 8;          } bits;  };    with needed adaptation.
1088,maxgurtovoy,maybe add a new func that does handle of TYPE_NOTICE ?
1089,maxgurtovoy,please comment that you default to static
1090,maxgurtovoy,I don't understand why all the logic in this feature is void and best effort. what will be the behaviour if something will go wrong ?
1091,maxgurtovoy,see the layer progress we did in nvmf_emu_fail_ctrl as an example:  nvmf_ctrl_attach_namespace -> nvme_emu_ep_attach_volume -> nvme_driver_attach_ep_volume (here you need to check that the driver type is not MULTIPLE_EP) -> driver->nvme_ctrl_attach_namespace(driver->dev->ctrl  nsid  ...)
1092,maxgurtovoy,didn't we discuss it ?
1093,maxgurtovoy,you can use dev->ctrl + casting instead of this embedded struct.
1094,maxgurtovoy,no need for typedef.  please move this to src/nvme_emu.h
1095,maxgurtovoy,maybe pass pointers to functions and keep it in driver->attach_ns and driver->detach_ns
1096,maxgurtovoy,i don't understand this new name. see above.
1097,maxgurtovoy,use defines from nvme.h:  enum NvmeLogIdentifier {      NVME_LOG_ERROR_INFO             = 0x01       NVME_LOG_SMART_INFO             = 0x02       NVME_LOG_FW_SLOT_INFO           = 0x03       NVME_LOG_CHANGED_NS_LIST_INFO   = 0x04   };  
1098,maxgurtovoy,please add a task in RM to add a cmd_dump here
1099,maxgurtovoy,let's support 1 outstanding for now. please comment it's a zero based value
1100,maxgurtovoy,it's slow path. no need likely here.
1101,maxgurtovoy,you need to make sure that host didn't set unsupported async events.  fail it otherwise with proper rc.  why you need a lock here ?
1102,maxgurtovoy,unlikely/likely is only for fastpath always
1103,maxgurtovoy,code conventions
1104,maxgurtovoy,if no real need to have many inflight async events it will ease the code.  Also shouldn't we add additional reqs for async ? according to spec and SPDK imp ?
1105,nitzancarmi,done
1106,nitzancarmi,done
1107,nitzancarmi,done
1108,nitzancarmi,I removed NVME_METADATA_SIZE_OVERRIDE.  Now we can just modify NVME_METADATA_SIZE to whatever we like  and it will be the expected value for all namespaces. 
1109,nitzancarmi,removed
1110,nitzancarmi,as we talked  I keep it as  best effort  for now.
1111,nitzancarmi,done
1112,nitzancarmi,done
1113,nitzancarmi,done
1114,nitzancarmi,done
1115,nitzancarmi,done
1116,nitzancarmi,done
1117,nitzancarmi,done
1118,nitzancarmi,done
1119,nitzancarmi,done
1120,nitzancarmi,According to spec:  It is recommended that implementations support a minimum of four  Asynchronous Event Request Limit commands outstanding simultaneously.  So I prefer to support that if I can (I need to keep track of pending events anyway).    I didn't find any  additional  requests for async on spec./spdk  only in kernel code (since they want to avoid the annoying block layer timeouts there).
1121,nitzancarmi,Please don't pass qid only for printing purposes.  Maybe just pass  nvmf_rdma_qpair_t *queue  as an argument   and you should have all parameters you need (qid  cm_sem).  
1122,maxgurtovoy,please add a comment above it. 
1123,maxgurtovoy,yup.
1124,maxgurtovoy,see nvmf_keep_alive_thread for example.
1125,maxgurtovoy,this start to be tricky since we remove the lock in the cm_handler.  we might have a race here.
1126,hellerguyh,why shouldn't I pass a field only for print?
1127,hellerguyh,are you referring to the queue->state_lock? if so  it was removed from the cm thread 
1128,maxgurtovoy,I don't understand why you pass stderr here. don't you pass it in the init function ?
1129,maxgurtovoy,can you create an opposite destruction function ?
1130,vasilyMellanox,Because it's different flow inside the fatal macro (assertion) and init function it's for signals in cases like segmentation fault etc. 
1131,vasilyMellanox,added
1132,maxgurtovoy,and use it properly too ?
1133,maxgurtovoy,but you have 1 global FILE that you set in the beginning. why not calling debug_print_backtrace(); ?  The idea of adding args is to have more flows and options. but you always call it with stderr and 0.  Where do you think using different file in the init function and different file in the backtrace ?
1134,vasilyMellanox,We don't want to call off the backtracing at all...
1135,vasilyMellanox,It's already used in the gtest added in the next commit
1136,vasilyMellanox,it's the initial commit in the serias (according to Sasha) - will be added more features to backtracing
1137,vasilyMellanox,it's just a macro that can be used without main - where I put init (in tests section for example)
1138,maxgurtovoy,why not ?
1139,maxgurtovoy,No you didn't use debug_print_backtrace in the next commit.  And in this commit set debug_print_backtrace exactly for the needs of this patchset  otherwise it will stay like this forever.  The right architecture should be:  call debug_print_backtrace();  if global stream is NULL --> return (since nobody run the initialization)  else use the global stream.
1140,maxgurtovoy,what about fix to devx_init_dev ?
1141,maxgurtovoy,see function definitions convention in the code.  also call it nvmf_rdma_get_cm_event and evt to be expected evt.  you must make the code readable.
1142,maxgurtovoy,if (rc)  maybe we should move the cm_fd to O_NONBLOCK ? consulte with Alex.
1143,maxgurtovoy,what was changed here ?
1144,maxgurtovoy,again
1145,maxgurtovoy,let's change the huge create function to call:  - nvmf_rdma_resolve_addr (this will take care that addr_resolved handler finished successfully)  - nvmf_rdma_resolve_route (this will take care that route_resolved handler finished successfully)  - nvmf_rdma_connect (this will take care that connected handler finished successfully and we establish connection)
1146,maxgurtovoy,can we remove this sem ?
1147,hellerguyh,yes
1148,hellerguyh,you are right  it is moved in the next commit. 
1149,hellerguyh,ok
1150,hellerguyh,it's a good idea  maybe it will be more intuitive if we separate it into:   1. nvmf_rdma_resolve_addr: performs rdma_resolve_addr  and waits for RDMA_CM_EVENT_ADDR_RESOLVED  2. nvmf_rdma_resolve_route: performs rdma_resolve_route  and waits for RDMA_CM_EVENT_ROUTE_RESOLVED  3. nvmf_rdma_connect: performs rdma_connect and waits for RDMA_CM_EVENT_CONNECT_RESPONSE/RDMA_CM_EVENT_ESTABLISHED
1151,hellerguyh,it doesn't show but the if (rc) was fixed 
1152,hellerguyh,changed
1153,maxgurtovoy,why don't you ack inside the nvmf_rdma_get_cm_event func ?
1154,maxgurtovoy,which event is expected ? need to print.
1155,maxgurtovoy,if you always print from the nvmf_rdma_get_cm_event in case of an error  you don't need to print twice.
1156,maxgurtovoy,don't add \n to nvmx_error. it's done already.
1157,maxgurtovoy,usually we don't destroy resources that we didn't created in the function. You already have the infrastructure in the main q_alloc function for that.
1158,maxgurtovoy,please hide all the ack_cm_event to a proper place. In the new arch  the function should get the event and make sure that we ready to get another event in the future.
1159,maxgurtovoy,pfd = {0};
1160,hellerguyh,this error message also includes the queue id. (inside of the nvmf_rdma_get_cm_event we don't have the queue id)
1161,maxgurtovoy,please add a comment above  that negative timeout means infinity that means you will be triggered by the rdma_cm
1162,maxgurtovoy,TIMEOUT.  also above.
1163,hellerguyh,because there are cases where you need the event data afterwards.   for example: in nvmf_rdma_connect you get the RDMA_CM_EVENT_CONNECT_RESPONSE  and then use it in nvmf_rdma_alloc_reqs_rsps. 
1164,hellerguyh,see the above note  we need the data from the event in further logic.   in addition  in this new architecture while application is handling an event it doesn't need to be ready for another event.
1165,hellerguyh,ok
1166,hellerguyh,ok
1167,hellerguyh,ok
1168,hellerguyh,ok
1169,hellerguyh,ok
1170,maxgurtovoy,please remove the above commented out lines.  also in the below
1171,yshestakov,done
1172,yshestakov,`optparse` is an outdated module. It is recomended to use `argparse` instead starting Python 2.7
1173,yshestakov,I suggest to add a few Python3-compat rows:  `from __future__ import print_function`
1174,yshestakov,`parser = argparse.ArgumentParser()`
1175,yshestakov,`parser.add_argumen(...)`
1176,yshestakov,`print(....)` -- as a function call
1177,yshestakov,It make sense to run `mlxconfig q` only once  parse the output into a dictionary  check for needed  components   build `mlxconfig set` command parameters
1178,nitzancarmi,Maybe we can use something less text-dependant  like:  `ls /sys/bus/pci/drivers/mlx5_core/ | grep 0000 | sed 's/^.....//'`
1179,nitzancarmi,Better to use out.splitlines()  
1180,yshestakov,`lspci -d 15b3:a2d2`  :)    ```  [root@r-nvmx02-snic  ]  lspci -d 15b3:a2d2  03:00.0 Ethernet controller: Mellanox Technologies MT416842 BlueField integrated ConnectX-5 network controller  03:00.1 Ethernet controller: Mellanox Technologies MT416842 BlueField integrated ConnectX-5 network controller  03:00.2 Ethernet controller: Mellanox Technologies MT416842 BlueField integrated ConnectX-5 network controller  03:00.3 Ethernet controller: Mellanox Technologies MT416842 BlueField integrated ConnectX-5 network controller  ```
1181,maxgurtovoy,actually it's much faster to check each param separately 
1182,maxgurtovoy,better to comment that:  /* how much time to wait before the next IO RETRY */
1183,maxgurtovoy,is this new definition ?  if so  I don't see it in this commit
1184,maxgurtovoy,please fix that and we'll merge.  Also please mention in the commit message what is the backend used for fio_verify
1185,vasilyMellanox,added  commit amend issue
1186,vasilyMellanox,fixed
1187,maxgurtovoy,please add a condition to nvme_emu_ep_activate_sq  that you will run ep->ep_ops.activate_sq(ep  sqid); only if the cqid that is associated with this sqid is in NVME_EMU_EP_QUEUE_ACTIVE state.    cq =  ep->cqs[ep->sqs[sqid].cqid]  if (cq->state  = NVME_EMU_EP_QUEUE_ACTIVE ) return an error.
1188,nitzancarmi,done
1189,nitzancarmi,maybe it is cleaner to use  paths : [{}]    (I checked it briefly and it doesn't seem to matter)
1190,yshestakov,Thanks
1191,maxgurtovoy,we can leave it as an example for good configuration.
1192,maxgurtovoy,i don't think we need mlx5_1 file. we use mlx5_2/3/4/5 for nvme
1193,yshestakov,We need to have if only 1 nvme pf is enabled.
1194,yshestakov,It's added into `/etc/nvme_snap/` directory. I've changed  configure option `--sysconfdir=/etc/nvme_snap`
1195,nitzancarmi,I prefer that all ctrl->id_ctrl fillings will be under nvme_ctrl_id_init(). It is easier to maintain it that way.  Please move it there (you can reach the bar from there easily through ctrl->dev->bar.curr).
1196,nitzancarmi,please add comment that UUID is not included in id_ns  and therefore we don't do this memcpy.
1197,nitzancarmi,Can't we just use memcpy(id_desc->nid  nid  sizeof(nid)) instead of this complex function?  AFAIK  union  allocates 16 bytes no matter what  so worst case we will compare some trailing zeroes.
1201,nitzancarmi,If I understood correctly  the reason you moved this code out of samples/controller.c is for using it also inside src/nvme_emu_io_driver.c. I don't think driver should be aware of namespace management type value (see below)  so this change is no longer needed IMO.
1202,nitzancarmi,In addition - from nvme spec:  Version (VS): Indicates support for revision 1.2 or higher revisions of the PCI Power Management Specification.  - I think this version number from the bar is not related to the NVMe spec. version. The way I see it - ctrl.ver value needs to be taken from JSON file ( ctrl.ver )  and we also need to verify that any new remote target that we discover matches this value (at least) - similar to how we checked oncs in src/nvmf_emu_ctrl.c (nvmf_ctrl_verify_oncs_configuration). The default value for this can be 1 1 0 I think.
1203,nitzancarmi,Please don't do in-place modifications for  value . Someone might be using this value later (outside of this function) and he won't understand how it got modified here. It is actually better to put  const  before the function parameter.
1204,nitzancarmi,Not sure I understand the calculations in this function. Can you please add comments here of what you did?  Also  why we can't use the built-ins be64toh() etc. here?
1205,nitzancarmi,The warning is too long for users to understand - I think it is enough to say  Namespace  d: Global ID is set to 0 .
1206,nitzancarmi,If we fail it supposed to be only when there is **BAD** global ID configured on JSON (missing values just causes default assignments). So we need to use nvmx_error( bad nid .... ) here and go to error flow
1207,nitzancarmi,please add NVME_NS_IDENTIFIER_NONE = 0x0 for backward compatability.
1208,nitzancarmi,We have to preserve backward compatibility. In case we don't have nidt configured - please put default one (NVME_NS_IDENTIFIER_NONE - see below).
1209,nitzancarmi,It is not an error - just set a warning and put NVME_NS_IDENTIFIER_NONE. You can look at how we parse backned type ( memdisk / posix_io / nvmf_rdma ) in nvme_driver_create() as an example.
1210,nitzancarmi,please use the right types (uint32_t  uint16_t etc.) and not regular ints.
1211,nitzancarmi,Please check that  dummy  controllers JSON files still work. they shouldn't have  global  values included  but they still needs to work as before. Also add this to the  Tests:  section in commit message
1212,nitzancarmi,all functions in this file needs to have nvme_json_... prefix  instead of just json_
1213,nitzancarmi,After we add version validation in nvmf_emu_ctrl.c to previous commit  We don't need this  ver  value anymore.
1214,nitzancarmi,This version validation needs to be in the previous commit  and in a separate function (like nvmf_ctrl_verify_oncs_configuration()). Anyway  Why is this relevant for dynamic namespaces only?
1215,nitzancarmi,We don't need to cache this value here (see below - We don't really have to know if we're in static/dynamic namespace management).
1216,nitzancarmi,After we check that version in previous commit  we can only check one of these values here
1217,vasilyMellanox,fw writes ver on the start stage
1218,vasilyMellanox,sscanf issue
1219,nitzancarmi,small type in commit message ( net  instead of  next )
1220,nitzancarmi,We always return 0 here (we always update ns_mgmt_type)   so maybe just return the chosen ns_mgmt_type value (instead of filling the pointer).
1221,nitzancarmi,Commit message is a little confusing. we return error only if target is less than 1.1 **and dynamic namespace management** (like you explained me :-)). In case of static - we never return error because of this.
1222,nitzancarmi,Please move this default assignment inside nvme_json_get_global_id.  If we are returned with error from nvme_json_get_global_id  this is a REAL error (e.g. something is wrong with JSON) and we should fail here (not warn and continue).
1223,nitzancarmi,still needs that check :-)
1224,nitzancarmi,still needs to shorten print
1225,nitzancarmi,Still needs a fix here
1226,nitzancarmi,Here you need to assign NVME_NS_IDENTIFIER_NONE  set warning and continue (not fail with error)
1227,nitzancarmi,please add the  buf  to the print
1228,nitzancarmi,You need to add case for NVME_NS_IDENTIFIER_NONE. It is not an error if we reach this.
1229,nitzancarmi,Actually  if we are in STATIC ns_mgmt_type  we don't care about target's descriptors list at all  am I right?  So  we can just finish here and not enter the whole if/else below.
1230,nitzancarmi,You cannot move the pointer of  data  here  and then free it afterwards. You need another pointer to iterate through the list.
1231,nitzancarmi,I don't understand the purpose of this  while . If we only need 1 nid (for this stage)  lets just take the first element in the list and use it.
1232,nitzancarmi,Please don't put  return 0  at this stage  use  goto  to the end of function instead. It makes the code harder to understand  causes duplicate code (we call  free(data)  in a lot of different places)  and it will be difficult to add/remove code from this function later. 
1233,nitzancarmi,Please use memcmp  like it is used in the beginning of the function (we need to be consistent in our  comparison handling).
1234,nitzancarmi,Please add some additional tests to the  Tests:  section:   - Run with  dummy  controllers JSON file.   - Run with  static / Dynamic  namespaces management.   - Run with all 4 possible types serial IDs (no serial ID  eui64  nguid  uuid).   - Run with SPDK intermediate target (customer's case).   - (If possible) Run with old ( < 1.1) remote NVMf target.
1235,nitzancarmi,Thinking about this further - **Any entry should be of size nidl+4 ** not just nidl**.  Are you sure this is the right jump? Did you check target with more than 1 uid?
1236,maxgurtovoy,please write the misconfiguration scenarios in detail.
1237,maxgurtovoy,why are you continue with the flow in case you have an error ?
1238,nitzancarmi,I want to print all the misconfigurations in a row (if there are multiple)  instead of just the first one.
1239,maxgurtovoy,I know the result of it  but why doing it and where did you see that we accumulate error ?
1240,maxgurtovoy,can you fix alignment please ?
1241,maxgurtovoy,I guess this will be fixed after you rebase over my code.
1242,maxgurtovoy,let's keep conventions:  if () {  } else if () {  }
1243,maxgurtovoy,this if can be removed (we can set 0 and it's ok).
1244,maxgurtovoy,should we remove the on behalf ? This is also a tunneled command..
1245,maxgurtovoy,same as for the QUERY_ROCE address
1246,maxgurtovoy,on behalf should be changed to tunneled command
1247,maxgurtovoy,on behalf should be changed to tunneled command
1248,maxgurtovoy,on behalf should be changed to tunneled command
1249,maxgurtovoy,this should be changed to tunneled command
1250,maxgurtovoy,I've changed the to MLX5_OBJ_TYPE_DEVICE_EMULATION enum
1251,maxgurtovoy,on_behalf to tunnel cmd...
1252,maxgurtovoy,on_behalf to tunnel cmd...
1253,maxgurtovoy,i mentioned this in the previous review  so you can ignore it
1254,maxgurtovoy,ignore the previous review
1255,maxgurtovoy,this is not tunneled command since mkey/psv is created by FW.
1256,maxgurtovoy,what if cdata.mdts == 0 ?  also the logic is not right.  mdts is the host_mdts.
1257,maxgurtovoy,In case we can't read mdts we use NVME_CTRL_MDTS_DEFAULT. so please add a test that you don't have mdts in the json and see if you pass it.
1258,snimrod,Why do you need 64 bits for a hard coded 9 bits string? This is of course a non-issue but I just wander why 64 and not 32 or 16 for that matter.
1259,snimrod,I guess you meant return rc
1260,nitzancarmi,you're right  I removed param[] completely.
1261,nitzancarmi,Right  I didn't check all error flows again after a minor change I did right before commiting. Fixed and re-checked.
1262,alex-mikheev,you can use if () goto...; if () goto; code here and save on the indentation
1263,hellerguyh,maybe it's better to put the lock under [/var/lock](https://refspecs.linuxfoundation.org/FHS_3.0/fhs/ch05s09.html)?
1264,hellerguyh,maybe print here errno as well so we will be sure it fails because of another process
1265,hellerguyh,can be set in definition
1266,hellerguyh,errno here as well
1267,hellerguyh,not sure about this one  I think our services run in user context (I might be mistaken here)  if it's correct and we lock the file from root without closing it for some reason (let's say power off) then services won't be able to open it.   Maybe it's better to have permission as 0666? I don't see how it can cause harm.
1268,nitzancarmi,done
1269,nitzancarmi, m is a GCC standard to print the errno :-)
1270,nitzancarmi,done
1271,nitzancarmi,already printed  as said before
1272,nitzancarmi,Hmm very interesting scenario.  I checked: start emulation  run FW reset (brutal reboot)  and see if the file still exist.  AND (drums...) it didn't - meaning when the process dies  it unlinks the file  and the file disappears (and there is no problem running new processes).  So I think it is better to keep it 0600 to block malicious users from modify that file.
1273,vasilyMellanox,The  else if  condition could shorter if you sum slba and nlb and after that shift by block_order
1274,vasilyMellanox,same as above
1275,vasilyMellanox,the same
1276,vasilyMellanox,the same
1277,vasilyMellanox,in case of  size_mb - the size_mb zero and not NVME_DISK_DEFAULT_SIZE
1278,nitzancarmi,done
1279,nitzancarmi,done
1280,nitzancarmi,done
1281,vasilyMellanox,le64_to_cpu(rw->slba) + le16_to_cpu(rw->nlb) + 1  
1282,vasilyMellanox,I mean if size_b get zero value from the config => the func will return zero and won't reset size_b to NVME_DISK_DEFAULT_SIZE
1283,nitzancarmi,Ohh now I see it. You're right  fixed 
1284,nitzancarmi,done
1285,snimrod,Looks like duplicate code of the Read above (identical other than read/write print)  please make it a function and reuse the code.
1286,snimrod,Where this 20 value comes from? it is bad practice to use hard coded values (especially when they are repeated as in here)  please use define with a meaningful name.  I see you just moved this function and not wrote it but let's use this opportunity to fix this.
1287,snimrod,Same duplicate code again...
1288,nitzancarmi,done
1289,nitzancarmi,done
1290,nitzancarmi,done
1291,nitzancarmi,Why is this true? We call nvme_emu_dev_open before we reach these lines.
1292,nitzancarmi,It is confusing to use  is_global_id_zero  an int (and use   operand) while it actually that returns 0 only to indicate  True . I think it is better to replace it (maybe return boolean True/False from is_global_id_zero).
1293,nitzancarmi,To avoid multiple prints with the same warning  we can replace the order of checks:  if (num_id_descs)  {...}  if ( num_id_descs || id_zero) {nvmx_warn(...);}
1294,nitzancarmi,Why not passing the nidt enum instead of the type as a string?
1295,nitzancarmi,Please add a comment from NVMe spec. that explains why is this enough to use NVME_NS_ID_TYPES_NUM everywhere:   A controller shall not return multiple descriptors with the same Namespace Identifier Type (NIDT). 
1296,nitzancarmi,We already do memset to 0 inside nvmf_identify_namespace().  I prefer do this only inside function (and not here)  but it's your choice.   Anyway  no need to do this twice
1297,nitzancarmi,Maybe move all the chunk  if {STATIC} else {DYNAMIC}  to a helper function (like nvmf_verify_configured_global_ids).  It will ease the reading of this function.
1298,nitzancarmi,redundant space
1299,nitzancarmi,Please try to split this juge function into helpers  it will be much easier to understand the logic this way.
1300,nitzancarmi,There is no real need in these local pointers...
1301,nitzancarmi,Please fix indentation
1302,nitzancarmi,We can't treat  list  parameter as of type  nvme_id_ns_descriptor_t \*   as it's elements are not of same size. Even though we currently don't use it  curr->nid value isn't correct in this kind of casting. Lets just keep it always as char\* (or void\*). If we must  we can memcpy X bytes from curr into local variable of type nvme_id_ns_descriptor_t.
1303,nitzancarmi,id_desc++;
1304,snimrod,Wrong English  should be  In case of descriptor with unrecognized type... 
1305,snimrod,Why are you using a switch case here? If all three types share the same handling then it is more of a if case I think.    Should be something like:    if (id   id < NVME_NS_IDENTIFIER_LAST) {  } else {  }
1306,snimrod,In general please try to give meaningful names and not using i and j 
1307,snimrod,Please give meaningful name and not use c
1308,maxgurtovoy,can we remove the enum definition ?
1309,nitzancarmi,done
1310,maxgurtovoy,why dont use struct mlx5dv_flow_action_attr flow_attr ? why array ?
1311,maxgurtovoy,please add Tests section in all commit.  Also you can add the exact command line for new tests (like I did with new tests for nvme_namespace)
1312,alex-mikheev,can you change devx_alloc_uar() to int devx_alloc_uar(ctx  *uar_obj) ?
1313,mike-dubman,why not to use autotools template spec.in->.spec logic?    here:   https://github.com/Mellanox/nvmx/blob/master/configure.ac L58  
1314,yshestakov,It's an egg-chicken issue.  `rpmbuild` knows nothing about `autotool`. It works with src.rpm and the spec file.  The version and revision attributes  should be harcoded into the spec file.  ```    rpm -qlp /.autodirect/mswg/release/nvme/nvme-snap-1.0.1-2.mlnx.src.rpm  nvme-snap-1.0.1.tar.gz  nvme-snap-googletest-1.0.1.tar.gz  nvme-snap.spec  ```    rpmbuild couldn't unpack tar.gz before processing SPEC file to regenerate the SPEC file based on configure.ac content.    In fact  `build_rpm.sh` and `build_srpm.sh` scripts extract  version  attribute from the `configure.ac`:    ```    cat build_scripts/get_ver.sh   /bin/bash  wd= (dirname  0)  awk '/AC_INIT/ {v= 2; gsub( [\\[\\] ]*     v); print v}'  wd/../configure.ac  ```    Next   revision  attribute is not part of `configure.ac` and it gets values of `BUILD_NUMBER` from the CI job (as an env variable).    .      
1315,mike-dubman,* rpmbuild nor any build script should not know/work with autotool  * The tarball should contain products of autotool (spec.in -> spec) and rpmbuild should use the product  * The tarball creation process starts with autogen->autotool>generates all files->packs into tarball and creates srpm/rpm.  
1316,yshestakov,I got your idea.    Currently  source tarball(s) are generated by `git-archive` command to get exactly the same state of sources for the given GIT commit/tag. Otherwise  we can produce a bit different content of binary RPM despite having the same RPM version-revision tuple as a unique identifier.     I could rework the build scripts and implement  build_srpm  as a Makefile target. So that nvme-snap.spec will be generated by `autotool`  next - pack ` WORKSPACE` directory into the `nvme-snap- VER.tar.gz` archive  generate SRC.RPM.
1317,mike-dubman,why not make dist?
1318,mike-dubman,git-archive is a mistake to create tarball. ```make dist''' should be used. You can use got-archive to transfer workspace from node to node  but  make dist  to create product tarball.
1319,yshestakov,We have a problems with `make dist` implementation due to long file names in the googletest sources we're trying to pack into the dist tarball:  ```  tardir=nvme-snap-1.0.1    {TAR-tar} chof -  tardir  | GZIP=--best gzip -c >nvme-snap-1.0.1.tar.gz  tar: nvme-snap-1.0.1/googletest/googletest/xcode/Samples/FrameworkSample/WidgetFramework.xcodeproj/project.pbxproj: file name is too long (max 99); not dumped  tar: Exiting with failure status due to previous errors  ```
1320,mike-dubman,Why we pack gtest into rpm?  It should be available for ci and dev not at delivery package.
1321,yshestakov,We don't control build environment  i.e. presence of gtest installed on machines used to build MLNX OFED including our `nvme-snap` from SRC.RPM. So that we pack gtest  we compile it each time (3 times): unittest stage  package state  compile stage of CI
1322,mike-dubman,i mean  no need to package tests (and gtest ext package) into snap.rpm.  gtest is used from ci  when jenkins can extract contrib/gtest-src.tgz and build tests with it.  no need it in snap.rpm    
1323,mike-dubman,below is a common practice:    1. Add gtest.m4 into snap same location with configure  enable-gtest  option  https://github.com/openucx/ucx/blob/master/config/m4/gtest.m4  2. Enable opt in configure.ac in snap  https://github.com/openucx/ucx/blob/master/configure.ac L209  3. Add embedded gtest self-contained code into snap repo  update  include  from proj files to point into it  https://github.com/openucx/ucx/blob/master/test/gtest/common/gtest.h  https://github.com/openucx/ucx/blob/master/test/gtest/common/gtest-all.cc      a. Now  when you build tarball with make dist   gtest code will be part of it.  b. The ./configure call from .spec file will be w/ or w/o building tests (IMO   no need tests in rpm)  c. The CI will configure with  enable-gtest and hence gtest will be available    
1324,yshestakov,I found the root cause of tar using old (v7) format:  `autoreconf` called from `./autogen.sh` produces `aclocal.m4`  which contains  chof  options to create a TAR archive.  ho  means old (v7) format to use.
1325,yshestakov,I can't build nvme-snap w/o  googletest  sources present:  ```  configure: creating ./config.status  config.status: creating Makefile  config.status: creating samples/Makefile  config.status: creating src/Makefile  config.status: creating tests/Makefile  config.status: creating config.h  config.status: executing depfiles commands  config.status: executing libtool commands  === configuring in googletest (/root/rpmbuild/BUILD/nvme-snap-1.0.1/googletest)  configure: WARNING: no configuration information is in googletest  + make -j16  make  all-recursive  make[1]: Entering directory `/root/rpmbuild/BUILD/nvme-snap-1.0.1'  Making all in googletest  make[2]: Entering directory `/root/rpmbuild/BUILD/nvme-snap-1.0.1/googletest'  make[2]: *** No rule to make target `all'.  Stop.  make[2]: Leaving directory `/root/rpmbuild/BUILD/nvme-snap-1.0.1/googletest'  make[1]: *** [all-recursive] Error 1  make[1]: Leaving directory `/root/rpmbuild/BUILD/nvme-snap-1.0.1'  make: *** [all] Error 2  error: Bad exit status from /var/tmp/rpm-tmp.EPmJ3a ( build)  ```
1326,yshestakov,`Makefile.am` wants it to build:  ```  SUBDIRS = googletest src samples tests    ```
1327,mike-dubman,not needed with new procedure
1328,mike-dubman,also  old tar format is not needed
1329,yshestakov,`aclocal.m4` is regenerated each time we run `autoreconf` tool (part of ./autogen.sh). So that I have to patch `aclocal.m4` after `autoreconf` and before `configure`
1330,mike-dubman,Why
1331,yshestakov,> Why?    `aclocal.m4` is not part of `nvmx.git` repo  it's generated by autotool(s).  I can't tell automake (called by autoreconf) to use tar-pax format instead of default one (v7). It ignores `AUTOMAKE_OPTIONS` env for some reason:  ```    AUTOMAKE_OPTIONS=--tar-pax autoreconf --force      grep chof *m4  aclocal.m4:  [am__tar=' {TAR-tar} chof -  tardir ' am__untar=' {TAR-tar} xf -']   ```  
1332,mike-dubman,Aclocal is byproduct and should not be treated bu nvmx scripts.  Why tar pax opt used at all?
1333,yshestakov,> Aclocal is byproduct and should not be treated by nvmx scripts.    `aclocal.m4` is included from `Makefile.in` -- genrated by autotools from `Makefile.am`.  It contains tar options to use for  make dist  target of Makefile.    > Why tar pax opt used at all?    No  v7 format is used by default as specified in `aclocal.m4` -- default behavior of automake.  I'm trying to tell automake to use pax format option (tar cf) instead of v7 (tar chof)  Because  tar chof  command turns old v7 formar and doesn't pack long file names like we have in googletest framework
1334,mike-dubman,U dont need googletest to pack in a 1st place.
1335,mike-dubman,See my comment with recipe 
1336,yshestakov,> U dont need googletest to pack in a 1st place.    Ok  it means I have to add `--skip-tests` option to `configure.ac` and add `ifdef` check into `Makefile.am` to skip build of `googletest`  and `tests` sub-directories.  
1337,yshestakov,> See my comment with the recipe    It's easy to add `nvme-snap.spec.in` into `AC_CONFIG_FILES` section of `configure.ac` to get RPM  Version  attribute substituted from `AC_INIT` macro. However  it doesn't provide RPM  Revision  number  which reflects BUILD_NUMBER (of Jenkins job)  i.e. sequential or unique number to identify changes to the binary/source RPM files. It is needed because of  micro  value of NVME-SNAP version (1.0.1 -- major=1  minor=0  micro=1) isn't changed each time we have nvme-snap code changed  i.e. new PR merged into  master  branch
1338,mike-dubman,micro should be calculated in configure.ac as well. It can read env variable BUILD_NUMBER and use it. -or- calculate it as amount of commits from branch start. see example below:    https://github.com/Mellanox/hcoll/blob/fe2976952ae7d1fcceca999b18f9f934925335d6/configure.ac L15    ```  define([hcoll_ver_micro]  esyscmd([sh -c  git rev-list HEAD | wc -l | sed -e 's/ *//g' | xargs -n1 printf ]))  ```    You can always override var from .spec file with --define
1339,yshestakov,> You can always override var from .spec file with --define    The `--define` option doesn't work for VladS when he takes src.rpm to build a binary RPM for MLNX OFED.  It means I have to hardcode all RPM version-revision values into the `nvme-snap.spec`  which is packed into the SRC.RPM file.    It seems I'm not so good at hacking `autotool`:    ```  ./configure --without-gtest  ...  configure: error: Support for GoogleTest was explicitly disabled. Currently GoogleMock has a hard  dependency upon GoogleTest to build  please provide a version  or allow  GoogleMock to use any installed version and fall back upon its internal  version.  configure: error: ./configure failed for googlemock  configure: error: ./configure failed for googletest  ```
1340,mike-dubman,because you fetch googlemock together with gtest (it is probably packaged togther). They have embed version or take one from ucx. (latest or near latest gtest embedded ver). I dont think we use gmock at all in nvmx.
1341,snimrod,Please calc sleep_interval_usec = 1000 * get_polling_time_interval_msec once prior to the loop and don't convert msec to usec every time within the loop.
1342,alex-mikheev,Let's change it to the debug
1343,alex-mikheev,let's remove flr callback too
1344,alex-mikheev,let's call bar write callback every X msec as it was.  Also may be it is good idea to call it from the mlx_dev_emu like it was. Someday we will have async event on the bar write... 
1345,nitzancarmi,done
1346,nitzancarmi,done
1347,nitzancarmi,you're right. rolled back this change. done.
1348,maxgurtovoy,devx_emu_caps_t
1349,maxgurtovoy,*emu_caps
1350,maxgurtovoy,limits ?  maybe we can call it caps ?
1351,maxgurtovoy,why we need new line ?
1352,maxgurtovoy,can be without {}
1353,nitzancarmi,removed
1354,nitzancarmi,removed
1355,maxgurtovoy,but here we are overriding it.  how does this works ?  The unit test only read from json file.  you should see it in the id-ctrl cmd
1356,maxgurtovoy,this is not related to the commit.  lets push only related stuff now.
1357,yshestakov,Max  I need it to run unit-test in the docker container.
1358,yshestakov,@maxgurtovoy  It's a bug in the script  which blocks my local tests of the nvmx. Please approve the change. It doesn't affect you in any way
1359,maxgurtovoy,I prefer not to use the global static ops (it might limit us in the future).  Let's add it as a ptr to the nvme_emulator_t (will be added by each  open  function  mlx and test.).  All other function that are called by the controller should have the emulator as first argument  so we are good here.    and in this function just use local variable:  nvme_emu_ops_t *nvme_emu_impl = nvme_load_ops(dev_name);    nvme_emu_impl->open()  
1360,alex-mikheev,updated
1361,alex-mikheev,I think we should use 127.x.x.x ip address
1362,alex-mikheev,can we either use dependancies or make all this part of nvme_snap@mlx5_2 service ? It confuses Gil that there are two services now ;)
1363,yshestakov,At this moment `nvme_sf` service creates only 1 NVMe SF. However  it could and will create more. I can't move this code into `nvme_snap@.service`  because of dependency on mlx5_2 (mlx5_3  ..) RDMA interfaces. 
1364,yshestakov,@maxgurtovoy  asked to use 1.1.1.1 explicitly.  I agree that usage of `1.1.1.1` is not safe  we can easily get into a situation when multiple SmartNIC on the same ETH (broadcast) network get the duplicate IP address assigned.
1365,yshestakov,I've changed to 192.168.101.2 assigned to `eth0` and used by default configuration of nvmf_proxy and nvme_snap@mlx5_2
1366,maxgurtovoy,no need to have this conf file. please remove. this will be created by the admin  and if not  we're doing ifup eth0 1.1.1.1 in the nvme_sf service already.
1367,maxgurtovoy,we don't need eth0 conf file.
1368,maxgurtovoy,same here
1369,maxgurtovoy,please remove all the ovs configuration from sf script.  we don't need it OOB.
1370,maxgurtovoy,Malloc0
1371,maxgurtovoy,please remove this commented stuff.  Also the bellow Nvme section is not needed.
1372,yshestakov,fixed already
1373,umanskymax,Max  how we can connect to the remote target without OVS configuration?
1374,yshestakov,fixed
1375,yshestakov,fixed
1376,yshestakov,fixed
1377,maxgurtovoy,lets remove this ovs configuration.  We don't want it OOB.
1378,maxgurtovoy,lets remove this commented lines
1379,maxgurtovoy,lets remove this un-needed code for ovs.
1380,maxgurtovoy,can you explain the logic of the install/uninstall here ?
1381,maxgurtovoy,In the future please create different PR for different tasks.  It's hard to maintain repos this way.  I used to work according to Linux kernel development rules and this will not pass there.
1382,maxgurtovoy,we need to check caps for each emulation application (in our case nvme_emulator)
1383,maxgurtovoy,alignment
1384,maxgurtovoy,we need to refactor devx_obj_create like we did in the devx_cmd. To embed the tunnel inside the command.  Also the state of mind in writing the code was and should be for using the same code for BF-1 and BF-2
1385,alex-mikheev,We still want the query function. It is going to be a part of the common library for nvme/virtio emulation
1386,maxgurtovoy,I don't see a usage for this func.
1387,alex-mikheev,fixed
1388,alex-mikheev,Looks like  a good candidate for a helper function or macro. 
1389,alex-mikheev,pd and channel should be properties of sf
1390,alex-mikheev,may be pass mdev   sf as params ? qid is used only to set up vector in create_cq  it is better to choose vector by cpu mask and not qid.  Perhaps better to poll from sq.
1391,alex-mikheev,there is no need to keep wr/sge for every rx queue entry.  You can use a local variable when doing ibv_post_recv
1392,maxgurtovoy,i wanted to do this  but I don't want to limit the sf to have 1 pd/channel
1393,maxgurtovoy,Yes. I did it to avoid setting wr/sge for each post_recv.
1394,maxgurtovoy,How googletests related to this task and commit ?
1395,maxgurtovoy,why are we configuring network in nvme snap service ? we said we won't do it.
1396,maxgurtovoy,why static ?  let's use dynamic and set  namespaces : []
1397,maxgurtovoy,please uncomment this to be (in case of sf**1**):  11:11:11:11:11:11  and for sf2 (OOB)  22:22:22:22:22:22  
1398,maxgurtovoy,please uncomment this.
1399,maxgurtovoy,lets move the logic of setting MAC env variable to setup_sf function
1400,maxgurtovoy,alignment
1401,maxgurtovoy,why don't we seperate between sf service and spdk-proxy and nvme_snap services ?
1402,yshestakov,It is not. It is cleanup of nvme-snap.spec from old comments
1403,yshestakov,> why are we configuring network in nvme snap service ? we said we won't do it.    How it would work w/o configuring `eth0` interface created by NVMe SF? How nvme-snap-controller will find proper interface to work on?
1404,yshestakov,Do you want to remove all supported parameters from the example configuration file provided by the rpm package?  What about the support of NS with 4K sector size?
1405,yshestakov,We're not going to create 2 SFs out of the box  so that we shouldn't provide `sf2.conf`. Otherwise `nvme_sf.service` will try to bring up the 2nd SF and fail  the resulting status of the service will be  FAILURE  and no other services depending on it would start (nvmf_proxy  nvme_snap@XXXX)
1406,yshestakov,I'm going to remove `NVME_SF_ECPF_PORT` -- it is not used. In fact the value of `NVME_SF_ECPF_DEV` is used to determine which device is ECPF.
1407,yshestakov,It is separated already. However  there is a complex dependency between 3 services:  - one `nvme_sf.service` -- the root  - one `spdk_proxy.service` -- intermediate SPDK proxy  but a customer may configure nvme-snap in the  direct mode   i.e. don't use this service  turn it off  - one or more `nvme_snap@NAME` services depending on `nvme_sf` (always) and `nvmf_proxy` (not always  a customer may change the behavior
1408,snimrod,We should have only one sf file (sf1.conf) and default MAC should be 11:11:11:11:11:11 as we agreed
1409,maxgurtovoy,We agreed already not to blend non-related cleanups in commits.
1410,maxgurtovoy,you bring up the interface in the sf service  right ?  and the snap json file have primary_ip to find the needed sf. You merged my PR last week.
1411,maxgurtovoy,not all supported params. But we want dynamic namespaces OOB.
1412,maxgurtovoy,the nvmf_proxy should be a separate service that will start spdk ONLY is nvmf_proxy.conf exist. If it doesn't exist  we run in full offload mode. OOB we'll run proxy mode.  Snap always run after sf and proxy services. proxy service shouldn't be turned off. It will do nothing in case proxy conf file doesn't exist.
1413,yshestakov,removed network configuration from `nvme_snap_run_ctrl.sh`
1414,yshestakov,done
1415,yshestakov,done
1416,yshestakov,fixed
1417,maxgurtovoy,in case MAC is not given  we agreed it to be:  (p_num+1 p_num+1):(p_num+1 p_num+1):(p_num+1 p_num+1):...  
1418,maxgurtovoy,can you remove the binding between sf service and nvmf_proxy and nvme_snap please ?
1419,maxgurtovoy,see my comment from previous review. This shouldn't be started in sf script. Snap service should run after proxy service finishes successfully.
1420,maxgurtovoy,still fix is missing for the MAC. we don't want to use MAC+2 anymore at all
1421,maxgurtovoy,also need to enable snap service that should run after the proxy.
1422,maxgurtovoy,what will happen to this service in case the proxy.conf file doesn't exist ?  In this case we should return 0 (probably user wants offload mode) and not run the ExecStartPost commands.
1423,yshestakov,Ok  fixed
1424,yshestakov,Well  ok. Let me fix it as well.
1425,yshestakov,Check the latest push with explicit `systemctl enable nvme_snap@nvme0` in the nvme-snap.spec.in
1426,yshestakov,This config file is provided by nvme-snap RPM by default  Also  nvme-snap  post scriptlet  enabled `nvmf_proxy.service` because it provides custom config and custom service file with dependency on `nvme_sf.service`
1427,yshestakov,Your suggestion doesn't work due to the way how some bits of MAC-address are handles by hw/fw  
1428,yshestakov,this change is not related to SystemD fix
1429,yshestakov,nvme-snap-pr-verification pipeline failed on PR 520:  http://dev-r-vrt-079-019.mtr.labs.mlnx/blue/organizations/jenkins/nvme-snap-pr-verification/detail/nvme-snap-pr-verification/76/pipeline    ```  restart nvme_snap@nvme0 service...      rsws10-snic.mtr.labs.mlnx failed | msg: Unable to start service nvme_snap@nvme0: A dependency job for nvme_snap@nvme0.service failed. See 'journalctl -xe' for details.  ```
1430,yshestakov,```  Sep 08 16:08:09 rsws10-snic.mtr.labs.mlnx ansible-systemd[17494]: Invoked with no_block=False force=None name= daemon_reexec=False enabled=None daemon_reload=False state=restarted  Sep 08 16:08:09 rsws10-snic.mtr.labs.mlnx systemd[1]: Starting nvme SF - configure...  -- Subject: Unit nvme_sf.service has begun start-up  -- Defined-By: systemd  -- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel  --  -- Unit nvme_sf.service has begun starting up.  Sep 08 16:08:09 rsws10-snic.mtr.labs.mlnx nvme_snap_create_sf.sh[17499]: /usr/sbin/nvme_snap_create_sf.sh: line 72: echo: write error: No such device  Sep 08 16:08:09 rsws10-snic.mtr.labs.mlnx systemd[1]: nvme_sf.service: main process exited  code=exited  status=1/FAILURE  Sep 08 16:08:09 rsws10-snic.mtr.labs.mlnx systemd[1]: Failed to start nvme SF - configure.  -- Subject: Unit nvme_sf.service has failed  -- Defined-By: systemd  -- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel  --  -- Unit nvme_sf.service has failed.  --  -- The result is failed.  Sep 08 16:08:09 rsws10-snic.mtr.labs.mlnx systemd[1]: Dependency failed for NVMe SNAP controller [nvme0].  -- Subject: Unit nvme_snap@nvme0.service has failed  -- Defined-By: systemd  -- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel  --  -- Unit nvme_snap@nvme0.service has failed.  ```
1431,maxgurtovoy,no nut this is different commit. And we wanted it to merge yesterday.
1432,maxgurtovoy,yoo will close the device twice in error flow.
1433,hellerguyh,break should prevent closing the device a second time
1434,maxgurtovoy,but found is True. so you'll say you're succeeded. Let's set found=true in case of success and continue otherwise.
1435,hellerguyh,done
1436,maxgurtovoy,error flow should be fixed. dereg_mr
1437,yshestakov,It's a bad idea to add  sleep 2  there due to the following points:  - nvmf_proxy should start BEFORE nvme_snap  but  - nvme_snap@.service SHOULD NOT depend strictly on `nvmf_proxy` because we have so-called DIRECT mode  - it's better to add  sleep 2  at the end of `nvme_snap_create_sf.sh` scripts
1438,maxgurtovoy,nvme snap doesn't depend on proxy. this sleep 2 is a WA since interface of sf is not up immediately with bond
1439,yshestakov,Look  you need to add  sleep 2  only once when the SF is created and before nvmf_proxy / nvme-snap  not the each time nvmf_proxy / nvme-snap services are started (restarted)
1440,maxgurtovoy,done
1441,maxgurtovoy,its general async event request here.
1442,maxgurtovoy,maybe better name is nvme_json_get_quirks
1443,maxgurtovoy,maybe we can add a helper function as I did with the registers.  you'll never print the  UNKNOWN 
1444,maxgurtovoy,lets leave the failure print and add the  removing  print to detach_namespace function. Also the  Adding  to create_namespace function bellow.
1445,maxgurtovoy,same to modify.
1446,maxgurtovoy,no need to add new line
1447,maxgurtovoy,we need to unmask all the async event we support. Maybe it's 1 but we need the general logic here.
1448,hellerguyh,remove redundant line
1449,hellerguyh,wrong change. logic should be:  if  cc_remote (or invalid value) then   mdts = 1  else  read value and handle accordingly 
1450,hellerguyh,change back to 300
1451,hellerguyh,fix redundant space 
1452,hellerguyh,no reason for goto
1453,hellerguyh,remove redundant line
1454,snimrod,Why do you need err? just check rc of sem_init
1455,nitzancarmi,done
1456,maxgurtovoy,why was it moved from PR for SGID to this one ?
1457,alex-mikheev,Let's also update the comment before function. 
1458,alex-mikheev,I think we can remove check_iface_valid from nvme_emu_ops_t :)
1459,nitzancarmi,Done
1460,nitzancarmi,Done
1461,maxgurtovoy,why do you need mdev ?
1462,maxgurtovoy,please fix code conventions also during copy   paste.
1463,nitzancarmi,done
1464,nitzancarmi,I'm going to use it in my next commit (for mlx_nvme_copy_roce_addr()).  I just wanted to introduce here the general interface  and not change it right away at my next commit.  
1465,maxgurtovoy,I wrote this in the first review:  why do you change so many places only for adding an helper ? because of naming ?
1466,maxgurtovoy,call it nvmf_queue_to_mlx_nvme_emulator instead of changing so many places..
1467,snimrod,Why aren't you checking sscanf rc? it might fail
1468,mike-dubman,also  code should take defensive programming measures  getenv can return null (no env set) and strndup will fail.    
1469,maxgurtovoy,we check the getenv in the  if  above.
1470,maxgurtovoy,please keep convention of the commit messages.
1471,mike-dubman,i see  thanks.  IMO better practice to cache getenv() into var in line 415 and use it downstream in line 419.  * easier to refactor if needed  * less chances for races (getenv/setenv calls are not thread safe) maybe not a case here but as general practice it is more  defence programming measures .
1472,snimrod,1. No hard coded numbers  please use a define.  2. Are you sure size_b was not overwritten by the failed sscanf? wouldn't it be safer to use NVME_DISK_DEFAULT_SIZE?
1473,snimrod,When you implement both 'if' and 'else' it has more sense to define a positive condition as a negative condition will have a double negative meaning on the 'else' side (your 'else' here means that rc is not not equal 1).
1474,maxgurtovoy,please add to commit message:  1. how to run the full configure command with SPDK.  2. how to run the new application ?
1475,mike-dubman,also plz add to README file  @yshestakov @umanskymax - please note  need to add testcase in CI for this.
1476,yshestakov,@mike-dubman Got it.  Also  I see 2 points of changes:  - add package dependency on spdk (which version? custom build with devel headers/libs?)  - add SPDK framework (headers/libs) into the Docker image used to build snap-rdma (CI job)
1477,mike-dubman,welcome to 2019    @yshestakov - we need copyright checker
1478,maxgurtovoy,I was testing the reviewers :)
1479,mike-dubman,need to call it from CI  @yshestakov 
1480,maxgurtovoy,can you keep the code conventions please ? for ().    btw  where do you use this ?
1481,maxgurtovoy,please keep same code convention in the file.
1482,maxgurtovoy,what is this ?
1483,maxgurtovoy,you don't use it anywhere
1484,maxgurtovoy,Please print the bdev name as a start
1485,nitzancarmi,This is the SPDK conf file  like /etc/spdk/nvmf_proxy.conf. Actually when we don't use SPDK block device we don't necessarily need it  so I should remove this limitation.
1486,maxgurtovoy,md_capable and pi_capable are confusing.  rename it to md_configurable.
1487,maxgurtovoy,need to remove ssvid from all json files.  Why not push to snap-3.0 ?
1488,nitzancarmi,Already inserted to snap-3.0. I'm closing this PR.
1489,maxgurtovoy,please add ability to pass:  void *user_ctx  struct ibv_comp_channel *channel  int vector    to create_cq.    
1490,maxgurtovoy,I wouldn't do this post_recv in allocation function because of the symmetry for the destruction function.  I can be done in  create_sw_qp
1491,maxgurtovoy,lets use calloc without casting
1492,maxgurtovoy,lets have arg for hw_pd and sw_pd
1493,maxgurtovoy,why we have this comment ?
1494,maxgurtovoy,code conventions *
1495,maxgurtovoy,lets zero the send_wr and send_sgl with {};
1496,maxgurtovoy,what is UCS ?
1497,maxgurtovoy,can't we get INPROGRESS ?
1498,maxgurtovoy,maybe we need to put some threshold on the inline size ?
1499,maxgurtovoy,same initialization as above.
1500,maxgurtovoy,lets keep same code convention for  if  and  if/else  clauses..
1501,maxgurtovoy,same here*
1502,alex-mikheev,we will fail in the create_sw_qp if tx_elem_size is too big for the inline send
1503,alex-mikheev,nope  only error or ok
1504,alex-mikheev,fixed
1505,alex-mikheev,fixed
1506,alex-mikheev,added comments  fixed
1507,alex-mikheev,fixed
1508,alex-mikheev,fixed
1509,alex-mikheev,fixed
1510,maxgurtovoy,this should be destroy_qp_helper( q->sw_qp);
1511,maxgurtovoy,you are setting  init_attr.cap.max_inline_data = attr->tx_elem_size;  so I guess this test is redundant.
1512,maxgurtovoy,but the user can give len > inline_len in theory  right ?  And then it won't go inline..
1513,maxgurtovoy,redundant memset here.
1514,alex-mikheev,fixed
1515,alex-mikheev,i don't want to do extra operation in the fast path
1516,alex-mikheev,Then post_send will fail. There is a corner case where inline_len > tx_elem_size and user give len > tx_elem_size   len < inline_len. But i am not sure that extra if() in fast path is worth it. 
1517,alex-mikheev,fixed
1518,alex-mikheev,I removed default initialization instead because there is already memset before attr is reused.  
1519,maxgurtovoy,do we realy need these  if 0 in the code ?
1520,maxgurtovoy,but this is the right way to work. You can forget to set some unneeded values below and the operation will be wrong.  It's not a big overheard.
1521,maxgurtovoy,if it's the right logic  we must check this in the fastpath and add likely/unlikely for optimization.  A user sometimes will do it wrong and we can't afford it.
1522,maxgurtovoy,same here
1523,maxgurtovoy,why not using NULL for pointers for code readability ?
1524,alex-mikheev,better to use  if instead of  ifdef
1525,maxgurtovoy,done
1526,steevenlee,If tx return error(-1) and rx returns 1  the final result still 0.
1527,steevenlee,Is it possible to add a callback context here? used to know which context data to process/clean  otherwise has to define a new data struct that containing this struct  looks no convenient.     void *ctx; /**< callback context */
1528,maxgurtovoy,maybe you can use cc->bits.en = 0; for code readability ?  union nvme_cc_register *cc =  curr_bar->cc;
1529,Tom-Wu-2019,we need to create a bit structure then we can do it like: struct nvme_cc_register *cc =  curr_bar->cc; cc->en = 0;  struct nvme_cc_register {      int en: 1;      int res1: 3;      int css: 3;      int mps: 4;      ....  };    but  we only use once in here  so is it worth to define such a structure only use once?
1530,maxgurtovoy,please fix unvme and tests in different commit.
1531,maxgurtovoy,but we have it already
1532,Tom-Wu-2019,yeah  I saw it in src/nvme_regs.h  will fix it  thanks.
1533,maxgurtovoy,is emu_type exist in other json files ?  if so  please remove.
1534,maxgurtovoy,NVME_EMU_QUEUE_DB is not supported.  Can be removed.
1535,maxgurtovoy,you can return here in case of admin.
1536,Tom-Wu-2019,if we decided to remove NVME_EMU_QUEUE_DB  we can return here in case of admin.
1537,Tom-Wu-2019,I checked the usage of NVME_EMU_QUEUE_DB  it is not used much but used in somewhere  we need to do the change in the place where NVME_EMU_QUEUE_DB used if we remove it at here  otherwise that piece of code will become dead code.  But  I want this pull reqeust focus on do the change for 'SNAP work mode parameter changing'  I can open another pull request to remove NVME_EMU_QUEUE_DB  it will be more traceable  is that ok?
1538,alex-mikheev,can not easilu remove db because it will break tests.  Tests will have to be updated to emulate sqe only mode. It is better done as a separate PR.
1539,maxgurtovoy,if this is for testing only  that Alex said  we need to put here emu_type: db_only.    Actually  I prefer to have a commit prior to this one  that removes the db mode from our library.
1540,Tom-Wu-2019,we can keep this parameter setting in the .json file only used by unittest  but rename 'emu_type' to 'emu_mode'.  this can keep the unittest work.
1541,maxgurtovoy,ok but let's do it and not postpone it to unknown date.  Tom   can you work on it after we merge this one ?
1542,maxgurtovoy,please open new PR. They way we work is not only concentrate on this task you get. If you see some bugs or fixes needed inside the code  we do it also (can be done in different PR but we don't leave bugs laying down in the code).
1543,alex-mikheev,should be set to the ssvid that is going to be fw default. 
1544,maxgurtovoy,no need for local variable to get 1 or 2 params
1545,maxgurtovoy,I call it pci_params in snap-rdma (lets stay with this name)
1546,maxgurtovoy,I prefer not using  NVMe  stuff in virtio ctrls. Please rename the nvme_emu logger to emu_logger in a preparation commit.
1547,maxgurtovoy,here you can also re-define.  Where are you using the re-defined macros ?    Also please fix the  ifndef _NVME_LOG_H to  ifndef EMU_LOG_H
1548,maxgurtovoy,I thought that know you will use snap_logger instead of SPDK...
1549,maxgurtovoy,*block_device_cmd_status
1550,maxgurtovoy,attrs should be defined as input argument for flexibility and sizes defined by the user.
1551,maxgurtovoy,<snap.h> should work.
1552,maxgurtovoy,dont use {} for one liner if/else.
1553,maxgurtovoy,you need pf_id when you open snap_device.
1554,nitzancarmi,I know  controller will take it from virtio_blk_config struct.
1555,maxgurtovoy,alignment
1556,maxgurtovoy,usually we don't allocate objects twice. you can embed bdev inside the null_bdev and use container_of if needed.  It will remove all the un-needed casting and make the code more readable and clean.
1557,maxgurtovoy,this and the 2 below should be inside the block_device_null_open.
1558,maxgurtovoy,type can be moved to attrs.
1559,maxgurtovoy,embed bdev.
1560,maxgurtovoy,add to_null_bdev(bdev) func.
1561,umanskymax,I think better provide just simple description w/o details  like:  `  --mem-size - set memory limit for SPDK application  1.2G minimum size for optimal work.  W/o this option  a single SPDK based app consumes all available huge pages.  `
1562,yshestakov,ok  fixed
1563,maxgurtovoy,the comment is not correct.  The reason to return 0 is the dynamic type.  you can check in the beginning:   if ( config || ctrl->ep->io_driver->ns_mgmt_type == NAMESPACE_MGMT_TYPE_DYNAMIC)          return 0;  
1564,yshestakov,`p_num` variable is used later at line  92 - logger string  line  93 - get `dev` value by rdma port
1565,umanskymax,We have line 76 `p_num= {NVME_SF_ECPF_DEV *.}` in the `function create_sf()`.  But better leave p_num= setting in the `function setup_sf()` too. Will fix now.  
1566,maxgurtovoy,dev = get_unvme_name();    shouldn't we call the function get_upci_name()
1567,umanskymax,I think we need to clearly indicate  if the user change `NVME_SF_GUID` variable  needs to provide changes in `udev` rules too.
1568,yshestakov,done
1569,maxgurtovoy,I think that in the spec it says INVALID_FIELD should be returned.
1570,maxgurtovoy,is this the SPDK way of work ? to stop app within the start_app process ?
1571,nitzancarmi,This is a little tricky step  since our  app_start  callback is called from different workqueue after the main spdk_spp_start() is finished. Our function is eventually called by spdk_subsystem_init_next(). Before it calls our callback  it calls spdk_app_stop(rc) in case of an error  so I followed the same path. so **yes  this is SPDK way of work**.
1572,alex-mikheev,Can you please add here a comment why we  force cc.en = 0. 
1573,maxgurtovoy,done
1574,alex-mikheev,style: align at first param declaration (one more space right)
1575,alex-mikheev,style: align at first param declaration (one more space right)
1576,maxgurtovoy,done
1577,maxgurtovoy,done
1578,alex-mikheev,Please don't use leading _ in the automatic variables names.    All identifiers that begin with an underscore are always reserved for use as identifiers with file scope in both the ordinary and tag name spaces.  from C99  7.1.3 
1579,alex-mikheev,style: pls add empty line after enum end
1580,nitzancarmi,Done
1581,nitzancarmi,Removed. thanks 
1582,maxgurtovoy,this function duplicates the above. You can create static helper.
1583,maxgurtovoy,this looks like a bug. I guess you should add   here.  Also the param should be a quirk mask and the constant should be ALLOWED_QUIRKS.
1584,maxgurtovoy,please fix code conventions.
1585,maxgurtovoy,?
1586,maxgurtovoy,why do you use the local variable  backend   and then copy it ?
1587,maxgurtovoy,same here. why to copy ?
1588,maxgurtovoy,code conventions.
1589,maxgurtovoy,.
1590,nitzancarmi,Done
1591,nitzancarmi,I added detailed explanation and changed the macro to NVME_CONFIG_ALLOWED_QUIRKS. Hope it is clearer now.
1592,maxgurtovoy,you're assuming that other layers are working well. You can check above that we're in dynamic mode and then we're good. If we're in static mode the function should fail if no ctrl.namespaces
1593,maxgurtovoy,why warn ?
1594,maxgurtovoy,this function looks wrong. we check only the first namespace metadata.
1595,maxgurtovoy,no need for the // comment
1596,maxgurtovoy,why do you need extra variable here ?
1597,maxgurtovoy,no need for {} in one liner.
1598,maxgurtovoy,code conventions:  if {  } else if {  } else {  }    and not  if   else if  else if {  }
1599,maxgurtovoy,same here. why extra local variable ?
1600,maxgurtovoy,again
1601,maxgurtovoy,if ( type_str)      config->ns_mgmt_type = NVME_CONFIG_DEFAULT_NS_MGMT_TYPE;  else if ()     config->ns_mgmt_type = ...;    
1602,maxgurtovoy, ...reverting to default type.. 
1603,maxgurtovoy,you always return 0.  make it a void function.
1604,maxgurtovoy,how do you change a const char here ?
1605,nitzancarmi,I don't change the original (const) string (which is empty in this case)  but just move the pointer to another const string.
1606,nitzancarmi,I need to be consistent with the function signatures  as I use them in src/nvme_config_json.c
1607,maxgurtovoy,should status be enum nvme_emu_status and not int ?
1608,maxgurtovoy,NVME_READ == DMA_TO_HOST  NVME_WRITE == DMA_FROM_HOST
1609,maxgurtovoy,why mdts is needed when you allocate resource for req ?
1610,maxgurtovoy,lets use req and not r.
1611,maxgurtovoy,* struct nvme_io_driver_req 
1612,maxgurtovoy,This is still confusion. This is not NVME_REQ for read/write.  maybe something like:  enum async_req_op {          REQ_OP_READ           REQ_OP_WRITE   };    or   enum dma_req_dir {          DMA_FROM_HOST           DMA_TO_HOST   };  
1613,maxgurtovoy,DMA_TO_HOST is write to host memory  right ?  DMA_FROM_HOST is read from host memory ?
1614,alex-mikheev,@umanskymax @yshestakov is it going to work ? Maybe it is better to move to the top level makefile.
1615,alex-mikheev,you need this commit because linux json parser does not support hex numbers ?
1616,nitzancarmi,Yes. But more importantly - this breaks JSON specification (http://json.com/specs/).
1617,alex-mikheev,things like flags and register values may be more readable in hex. But so far we only have quirks that need it. Guess we can deal with it later.
1618,nitzancarmi,@umanskymax Is it approved from your POV?
1619,alex-mikheev,@nitzancarmi  try make dist and check that nvme0.json is included
1620,nitzancarmi,@alex-mikheev  Verified. nvme0.json is included.
1621,alex-mikheev,this may lead to the unpredictable results. It is better either not to include queue.h at all if spdk one is used or rename list macros. 
1622,alex-mikheev,perhaps nvme_emu.h is not needed ? 
1623,alex-mikheev,perhaps add nvme_ctlr_progress_admin(ctrl) and nvme_ctrl_progress_all_io(ctlr) ? You are going to add io polling threads in another pr right ?
1624,nitzancarmi,Done
1625,alex-mikheev,what about adding AC_CHECK_HEADER(sys/queue.h) to the configure.ac and than      if HAVE_QUEUE_H    include <sys/queue.h>    else    old code    endif   
1626,alex-mikheev,can you add a kerneldoc description for both of these functions ? I started moving doxygen like comment to the kerneldoc format
1627,nitzancarmi,Done
1628,maxgurtovoy,reads host memory to data_buf 
1629,maxgurtovoy,if list  read all list entries ...
1630,maxgurtovoy,can you comment out this instead of if 0 ?
1631,maxgurtovoy,can you add a comment that this is the case where prp2 is not a list ?
1632,maxgurtovoy,also please comment here  what we do in case of failure.
1633,maxgurtovoy,why does write became read ?
1634,maxgurtovoy,can you explain why you multiply by 8 ?
1635,maxgurtovoy,} else {     blabla;  }
1636,maxgurtovoy,} else {    blabla;  }
1637,maxgurtovoy,} else {
1638,maxgurtovoy,this line can be outside the if's
1639,alex-mikheev,we have to read prp list entries from the host memory
1640,alex-mikheev,style: align void *cmd  see for example nvme_ctrl_handle_cmd()
1641,alex-mikheev,this should be part of the test emulator
1642,alex-mikheev,also move to the test emulator
1643,alex-mikheev,interrupt can be removed
1644,alex-mikheev,may be you can move this function to the test emulator ?
1645,alex-mikheev,lets keep interface as is and have test_emulator do additional magic instead of the test controller implementation
1646,alex-mikheev,lets not add another special test mode to the controller. It kind of defeats the purpose of removing db_only mode
1647,Tom-Wu-2019,Done
1648,Tom-Wu-2019,done
1649,Tom-Wu-2019,done
1650,Tom-Wu-2019,since I removed 'nvme_emu_interrupt' invoking from nvme_post_cqes and no other place invoked this function  but this PR is for remove 'db_only mode' related code  shall I remove  'nvme_emu_interrupt' in this PR? I suggest we should open another PR to delete it if we do  not use it at this moment.  
1651,Tom-Wu-2019,done
1652,Tom-Wu-2019,done
1653,Tom-Wu-2019,we really need a flag to distinguish the handling is from test controller and other case in 'nvme_cq_full'  and if we did not handle the handling for test controller special  then we cannot handle the CQ full situation and full_cq and full_cq2 these two testcases will failure. and add a 'quirk' flag is the solution I can find to fix this with minimax modifcation to existing code. Or  my understanding is wrong and we do not need handle cq full for test controller special.
1654,alex-mikheev,this is going to be a major lock contention point with multi threading. Here and also in the sample controller code. At least add TODO: notice. 
1655,alex-mikheev,isnt it better to return error here ?
1656,nitzancarmi,Done
1657,nitzancarmi,Done
1658,alex-mikheev,rename to test_emu_squeue
1659,alex-mikheev,test_emu_cqueue
1660,alex-mikheev,rename to test_emu_cqe
1661,alex-mikheev,I think you can calculate address based on the current shadow cq index right ?
1662,alex-mikheev,This can be a separate helper function since you use this logic twice
1663,alex-mikheev,please split this function in two. One for cq and one for sq handling. It is too big
1664,alex-mikheev,rename to test_emu_sq_inc_head  no need for inline
1665,alex-mikheev,rename to test_emu_cq_... no need to inline
1666,alex-mikheev,make it static  rename to test_emu_cq_full
1667,alex-mikheev,make it static  rename to test_emu_sq_empty
1668,alex-mikheev,make it static  rename to test_emu_get_cmd
1669,Tom-Wu-2019,done
1670,Tom-Wu-2019,done
1671,Tom-Wu-2019,done
1672,Tom-Wu-2019,done
1673,Tom-Wu-2019,done
1674,Tom-Wu-2019,done
1675,Tom-Wu-2019,done
1676,Tom-Wu-2019,done
1677,Tom-Wu-2019,done
1678,Tom-Wu-2019,done
1679,Tom-Wu-2019,done
1680,maxgurtovoy,you can ignore it also
1681,maxgurtovoy,you can uncomment it
1682,maxgurtovoy,you can ignore merge commits and use this rule.
1683,maxgurtovoy,when we can remove this ignore ?
1684,maxgurtovoy,no need to ignore this.
1685,maxgurtovoy,i dont see this rules above...
1686,maxgurtovoy,return [] ?
1687,maxgurtovoy,return [] ?
1688,maxgurtovoy,only name and id are different here.
1689,maxgurtovoy,why 80 and 78 ?
1690,maxgurtovoy,please squash commits 3+4 to commit 1.  We don't review the development process.
1691,alex-mikheev,better use strncmp
1692,alex-mikheev,add blanc linke between } and the macro
1693,nitzancarmi,Don't we need to add NVME_EMU_QUEUE_NVME2NVMF_REMOTE as well?
1701,alex-mikheev,not in this pr. for cc remote you also have to use external qp instead of dma_q
1702,nitzancarmi,Please use NvmeDriverBase::NR_IO_QUEUES instead (like in shutdown_stress test).  Also make sure we don't have any other places with hard-coded values like this in the file.
1703,Tom-Wu-2019,Done
1704,mike-dubman,simplify to  ```  nvme_controller_emu_CFLAGS  =  (BASE_CFLAGS) -I (srcdir)/../src  (SNAP_CFLAGS)  if HAVE_SPDK  nvme_controller_emu_CFLAGS +=  (SPDK_CFLAGS)  endif  ```    less code to support and take care
1705,mike-dubman,same
1706,mike-dubman,same
1707,yshestakov,fixed
1708,mike-dubman,why not same if HAVE_SPDK? 
1709,mike-dubman,same cond
1710,yshestakov,I guess it doesn't make sense. In case `--with-spdk` is not specified  ` (SPDK_CFLAGS)` expands to empty string.
1711,mike-dubman,it does:  * some1 defines SPDK_CFLAGA= env variable and compiles snap w/o spdk - it will pick up var and break  while it shouldn`t
1712,yshestakov,fixed now
1713,mike-dubman,we should not add jenkins CI vars into autotools files.  You can add ```get_revid.sh``` script which will have CI specifics and return resolved revision ID  same as you have get_ver.sh script    
1714,mike-dubman,should be removed  and on 1st line add defines as below:  ```   { ?configure_options:  global configure_options  {nil}}   _prefix /opt/mellanix/snap   _sysconfdir /etc/nvme_snap  ```  so one can rellocate  also use var name ``` {_prefix}``` in configure line
1715,yshestakov,ok  going to fix this issue
1716,mike-dubman,need add at the end line 57: ``` {?configure_options}```
1717,yshestakov,Why is it needed?  At this moment `configure` command looks like this (as ran by rpmbuild):    ```  + ./configure --build=aarch64-redhat-linux-gnu --host=aarch64-redhat-linux-gnu --program-prefix= --disable-dependency-tr  acking --prefix=/usr --exec-prefix=/usr --bindir=/usr/bin --sbindir=/usr/sbin --sysconfdir=/etc --datadir=/usr/share --i  ncludedir=/usr/include --libdir=/usr/lib64 --libexecdir=/usr/libexec --localstatedir=/var --sharedstatedir=/var/lib --ma  ndir=/usr/share/man --infodir=/usr/share/info --sysconfdir=/etc/nvme_snap --with-gtest=no --with-snap=/usr --with-spdk=/  opt/mellanix/snap --with-dpdk=/opt/mellanix/snap  ```
1718,mike-dubman,why if block needed? spec file should never run or rely autogen. it should include autogen generated artifacts  not re-create it.  imagine src.rpm that does ```autogen``` in its spec and it fails because packages needed for build are not present on the system it runs.
1719,mike-dubman,typo: installed
1720,mike-dubman,why needed? how it can be not installed?
1721,mike-dubman,why ```--with-snap=/usr``` and not ```--with-snap= {snap_prefix}```  
1722,yshestakov,The workflow of building RPM from source should be following:  1. check out from Git  2. ./autogen.sh -> generates `configure`  3. ./configure (takes BUILD_NUMBER or GH_PR_ID as `scm_revision`):     -- generates `nvme-snap.spec` from `nvme-snap.spec.in`  4. make dist -> generates nvme-snap-3.0.0.tar.gz  which includes `nvme-snap.spec`  5. rpmbuild -ta nvme-snap-3.0.0.tar.gz  builds both SRC and BIN RPM packages having  `configure` and `*.spec` generated at steps 2 and 3    It almost the same for DEB: `configure` generates `./debian/changelog` with proper SCM version and revision values
1723,mike-dubman,why prefix used /usr and not snap_prefix?  also missing configure_opt var (see prev review)
1724,yshestakov,I had a typo in `snap_prefix` value like `/opt/mellanIx/snap`.  The `m4/spdk.m4` script executed by autoconf   doesn't check for directory to exists and silently accepts it. At the end the  configure  doesn't defined `HAVE_SPDK=yes`
1725,mike-dubman,right  but spec file should not own autogen phase  spec should include result of autogen actions  i.e. Makefiles.  rpmbuild src.rpm is not running in the developmen environment where depenandt packages can be found  but in environment where devel packages not present.
1726,yshestakov,Because `libsnap.so` and headers are installed into `/usr` as prefix.  I.e.  snap-rdma  library is compiled and installed (packaged) in such way. Nitzan told me that customers want to have it in default path
1727,yshestakov,Same answer as above
1728,mike-dubman,maybe better use ```snap_usr_prefix=/usr``` so it will be possible to override it during rpmbuil and place all in /opt/mellanox/snap if desired (usually we prefer keep all under same dir and not spread)
1729,yshestakov,I agree. However our existing CI compiles RPM from sources directly  not out-of-tree. Look at `build_scripts/build_rpm.sh`. It uses `git archive` command to generate TAR.GZ with sources.   The CI job need to be rewritten to:    a) use out-of-tree method to build RPM    b) add a step to build DEB package  Until this I would prefer to keep `test -e ./configure` in the SPEC file to keep CI working
1730,yshestakov,Mike  let's not discuss  snap-rdma  library (libsnap) improvements in this PR.  I can't made so many changes in parallel in 2 repos (2 projects) and fix 2 CIs jobs as well.
1731,mike-dubman,i dont see m4/spdk.m4 file in this tree. if it does not check - it should fail if libspdk.so does not present  not here.
1732,mike-dubman,what flag here belongs to project from diff repo?
1733,yshestakov,`m4/spdk.m4` is present in `snap-3.0` branch  not `master`:  https://github.com/Mellanox/nvmx/blob/snap-3.0/m4/snap.m4
1734,mike-dubman,check for libspdk also exists  need to understand why it did not work for you  no need to add hacks    https://github.com/Mellanox/nvmx/blob/snap-3.0/m4/spdk.m4 L47  
1735,mike-dubman,spdk.m4 treats missing libspdk as warning  not error  https://github.com/Mellanox/nvmx/blob/snap-3.0/m4/spdk.m4 L54  should be fixed.  if -with-spdk requested but not found - it should fail.
1736,mike-dubman,i dont see ```git archive``` there  but ```make dist``` which is good  also need to add ```make distcheck``` before  https://github.com/Mellanox/nvmx/blob/master/build_scripts/build_rpm.sh L15    we should fix it in a right way
1737,yshestakov,`--with-snap=/usr` tells configure where to find `libsnap.*` and headers:     https://github.com/Mellanox/nvmx/blob/snap-3.0/m4/snap.m4    libsnap (snap-rdma project) RPM is built by this script (as part of swx_ci.git):     https://github.com/Mellanox/swx_ci/blob/master/snap-rdma-release/build_rpm.sh    ```    autoreconf -ivf    ./configure --prefix=/usr --with-gtest=no    make dist  ```  `--with-spdk=` and `--with-dpdk=` tells configure where to look for SPDK and DPDK headers and libraries.    SPDK is built from branch `v.20.01.y` of Mellanox/spdk/ git repo there:      https://github.com/Mellanox/spdk/commit/5f997aeb806cc0d45cbb548879f4cac98eb13481
1738,yshestakov,Well  OK. Going to remove the  sanity  test from the spec there. Assuming `m4/spdk.m4` will be fixed later by AlexM or Nitzan
1739,mike-dubman,ok
1740,yshestakov,The code in `master` was fixed 3 months ago by me when I reworked it for Feb release of SNAP.  However I didn't touch `snap-3.0` branch because it was under heavy development. Just notified MaxG and Nitzan to take a look at it and fix automake/autoconf in same way as I did for  master  branch.
1741,yshestakov,Going to do clean up in `snap-3.0` now
1742,mike-dubman,never assume  fix or ask them to fix.   plz replace WARN to ERROR in https://github.com/Mellanox/nvmx/blob/snap-3.0/m4/spdk.m4 L54  
1743,yshestakov,OK. Fixed  will be pushed in next commit today
1744,mike-dubman,thx  btw also here: https://github.com/Mellanox/nvmx/blob/snap-3.0/m4/spdk.m4 L46 WARN->ERROR  no .h file - no spdk  @alex-mikheev 
1745,yshestakov,@mike-dubman  Well  I understand now why AlexM used WARN instead of ERROR in the `m4/spdk.m4`:    1. COVERITY step is executed in the special (dedicated) x86 VM where we have no SPDK installed.    2. Some components (tests  examples  samples) of SNAP-3.0 don't depend on SPDK libraries/headers and could be build without it.     last CI job failed:     http://hpc-master.lab.mtl.com:8080/blue/organizations/jenkins/pr-nvme-snap-3.0-pipeline/detail/pr-nvme-snap-3.0-pipeline/167/pipeline/39
1746,mike-dubman,with_spdk=no should be the default  unless explicitly provided in cmd line and then fail is expected if spdk not available.      
1747,mike-dubman,typo: packaing
1748,yshestakov,fixed
1749,mike-dubman,no need  should come from autogen.sh before running this script
1750,mike-dubman,lines: 28-32 should use get_revision.sh script
1751,yshestakov,fixed
1752,yshestakov,fixed
1753,mike-dubman,why  HOME? if it is in mellanox NFS - it will fail (also for developer who runs it locally)  it should be  WORKSPACE/rpmbuild ... and rpmbuild below should be pointed to it
1754,yshestakov,The package is built inside of Docker container  where `swx-jenkins` HOME=`/scrap/workspace/...`
1755,nitzancarmi,No need of brackets in a one liner for loop.
1756,nitzancarmi,again  no brackets needed
1757,nitzancarmi,Why is this code dropped? we need to go into teardown flow here.
1758,nitzancarmi,same here - please keep the ordinary teardown flow at the bottom.
1759,nitzancarmi,AFAIK nvme_ctrl_stop() is still needed after we exit the run_*() function.
1760,nitzancarmi,brackets :-)
1761,nitzancarmi,convention of spaces (after  if   before  { )
1762,nitzancarmi,space before  d
1763,nitzancarmi,I think the indexing here is wrong. If pthread_create fails for thread x  you'll try to destroy thread x (that wasn't ever created). You're also missing destroying thread  0. better to use:  for (i--; i >= 0; i--)    Another thing is that you must always call pthread_join() after pthread_cancel().
1764,nitzancarmi,need to add  return;  here  right?
1765,nitzancarmi,LIKE
1766,nitzancarmi,I prefer not to expose nvme_pg in the header file  as it is for internal use inside samples/controller.c.    Please add only forward declaration here (struct nvme_pg)  and move the actual struct definition to samples/controller.c    IMO it is also better to rename it to nvme_ctrl_pg (as it always belong to a controller)  but it's up to you here.
1767,nitzancarmi,Please merge these 2 comment blocks. It is confusing this way - looks like there is a missing function here.
1768,Tom-Wu-2019,done
1769,Tom-Wu-2019,done
1770,Tom-Wu-2019,done
1771,Tom-Wu-2019,have move the definiton to samples/controller.c and rename it to 'nvme_ctrl_pg'
1772,Tom-Wu-2019,yes  it should be remained. make a mistake by porting the code.
1773,Tom-Wu-2019,done
1774,Tom-Wu-2019,done
1775,Tom-Wu-2019,done
1776,Tom-Wu-2019,done
1777,Tom-Wu-2019,done
1778,Tom-Wu-2019,yes  you are right  the index to destory thread is wrong used.
1779,Tom-Wu-2019,do you meaning to check the return value of 'pthread_join' or add a 'return;' santance for run_multi_threaded()?
1780,Tom-Wu-2019,want me to adjust the comments style?
1781,nitzancarmi,Yes  it'll be better.
1782,nitzancarmi,Never mind  my bad. Code is fine.
1783,alex-mikheev,lets have only nvme_ctrl_init(). Instead of conf_filename it can take a params struct { conf_filename  nthreads }. In the future conf_filename can be replaced with actual parameters.
1784,alex-mikheev,There is a subtle bug here. It is possible that some io requests are still in progress. They can be either in the io_driver (spdk bdev) or in flight.  We must wait on qp until all operations in progress has been completed   and we also should be able to handle io completions on the queue that no longer exists.     But let's fix it in the separate PR. 
1785,alex-mikheev,no need for else here as the poller is started anyway
1786,alex-mikheev,extra newline
1787,Tom-Wu-2019,done
1788,Tom-Wu-2019,ok  let do this in a separate PR.
1789,Tom-Wu-2019,done
1790,Tom-Wu-2019,done
1791,hellerguyh,doesn't this mean we will not include snap lib in case snap-virtio-blk-ctrl lib exists?   (cause AC_SUBST will replace -lsnap with -lsnap-virtio-blk-ctrl in SNAP_LDFLAGS content)
1792,nitzancarmi,You're 100  right. Fixed.
1793,hellerguyh,remove extra line
1794,hellerguyh,why extra line?
1795,alex-mikheev,AC_SUBST is already called in previous check  probably it is best to use it only once for every output variable
1796,alex-mikheev,any chance to preserve type checking here ?
1797,alex-mikheev,extra ;
1798,alex-mikheev,extra ;
1799,alex-mikheev,space before *: struct foo * here and in other places
1800,alex-mikheev,indentation
1801,alex-mikheev,indentation
1802,alex-mikheev,is it possible to include system .h file (in <>) first ?
1803,hellerguyh,for functions other then progress_all_io and progress_admin yes  with no cost.  for progress functions  it will cost us either writing different progress for each of the protocol types or adding an  if  on the data path. What do you think?
1804,hellerguyh,fixed
1805,hellerguyh,fixed
1806,hellerguyh,fixed
1807,hellerguyh,it's used to separate the label and variable declaration (won't compile without it) 
1808,hellerguyh,it's used to separate the label and variable declaration (won't compile without it)
1809,hellerguyh,yes  fixed
1810,hellerguyh,fixed
1811,hellerguyh,added wrapper functions as suggested
1812,alex-mikheev,can you include  config.h  to pick the define ? I run into this issue on my setup
1813,hellerguyh,moved declarations to top
1814,hellerguyh,where is this used?
1815,maxgurtovoy,Not used for now.
1816,nitzancarmi,typo (casa)
1817,nitzancarmi,I wonder if we want to call put_response after write completion.  Is it helpful to read response multiple times for a single request   or we wish to return ERROR for second time?  Personally I prefer the second approach.
1818,nitzancarmi,I would add AC_CHECK_HEADERS(snap_json_rpc_client.h) check.  This will generate  HAVE_SNAP_JSON_RPC_CLIENT_H  automatically.  We can use it later instead of the generic HAVE_SNAP flag.
1819,maxgurtovoy,thanks
1820,maxgurtovoy,I added this above already.
1821,maxgurtovoy,It was like this in my first implementation but it seems weird that one can send 1000 request successfully but sent only 1 response successfully. It will always hold the last good request answer.
1822,nitzancarmi,I meant that when you use AC_CHECK_HEADERS instead of AC_CHECK_HEADER (like you did above) - HAVE_SNAP_JSON_RPC_CLIENT_H flag is automatically generated and can be used.    You currently force nvmx to compile only against the newest libsnap  when we don't really have to do so.
1823,alex-mikheev,add line and make the function static and probably inline
1824,alex-mikheev,comment block alignment is of by one space
1825,nitzancarmi,We can remove brackets for one-liner.
1826,alex-mikheev,add space after struct name and before  *: struct foo *
1827,alex-mikheev,space
1828,yshestakov,Is `sudo` really needed there?
1829,nitzancarmi,No. I just followed the previous bash cmd in code. Removed sudo from both.
1830,nitzancarmi,**This breaks spec** - '\0' is not considered a valid ASCII character. The ASCII string is not null terminated  by spec design (see 1.5 on nvme spec v1.4 - conventions).    If you wish to print dst value afterwards  maybe better try: printf( .*s\n   max_len  dst)
1831,nitzancarmi,spacing conventions
1832,nitzancarmi,db_only and sqe_test are no longer exist in code  we can remove them. There are now only:  1. sqe_only - Get nvme (_not nvmf_) command.  2. sqe_cc - Get nvmf command (command capsule  cc).  3. cc_remote - full offload.
1833,nitzancarmi,alignment (so for nvmx_error just below)
1834,nitzancarmi,need to add `static`  AFAIU it is internal function. Also  maybe better name would be snap_config_get_backend_internal() or something.
1835,nitzancarmi,To be consistent across all file namings  let's call the structs  snap_config_backend  and  snap_config_namespace_path  and their typedefs too of course.  Besides  I'm afraid `struct backend` might cause suplicate declaration linkage problems with other libraries.
1836,nitzancarmi,it is not ns_path  it is be_path. Generally  when we use it  we connect to remote **subsystem**  which may have single or multiple namespaces attached.
1837,nitzancarmi,snap_config_load_path
1838,nitzancarmi,If nvme_json_get_value fails  there is no guarantee path->ka_timeout_ms remains intact (I know  shitty implementation  but that's what it is). So you might end up with default value overridden by garbage.  Same for all other fields in this function.
1839,nitzancarmi,don't use backend->be_type is input buffer. same fo all other inputs in function.
1840,nitzancarmi,it's info  not an error
1841,nitzancarmi,I think rmda_device and pci_func are the only 2 parameters which are mandatory. They are vital for us to know on which function to open our controller. Just using default  mlx5_0  and  0  as default is not a good practice.
1842,nitzancarmi,in case the  if  statement contains {}  the  else  sections need to have it too (code conventions). (same for below cases).
1843,nitzancarmi,this is mandatory feature. Also  I can't find the setter/getter for the new  rdma_device  attribute. What am I missing?
1844,nitzancarmi,just return NULL in this case. Otherwise you will free unallocated memory.
1845,nitzancarmi,why do you need nvme_emu_ep.h for?
1846,snimrod,Defines such as NVME_EMU_ADDR_MAX_LEN  NVME_EMU_MAX_NQN_LEN  etc
1847,snimrod,Replaced to snprintf
1848,snimrod,Done
1849,snimrod,Done
1850,snimrod,It is not internal function.
1851,snimrod,Done
1852,snimrod,Done
1853,snimrod,It is a local function  no need for 'snap_' prefix  Not having the prefix on internal functions also make it easier to detect when reading the code
1854,snimrod,Done
1855,snimrod,Done
1856,snimrod,Done
1857,snimrod,1. It will call the destroy function that will go out if it is null  2. There is no issue with calling to free(null)  it simply does nothing.
1858,snimrod,Changed to fail configuration init in case no pci_func in file
1859,snimrod,Removed default value for pci_func
1860,snimrod,Done
1861,snimrod,1. if be_type fails we fail the parsing and controller disabled anyway  2. Added reset for the rest of the non-mandatory values (to avoid garbage if failed)
1862,nitzancarmi,redundant space
1863,nitzancarmi,Default timeout should be 15000 (like it was on src/nvmf_emu_ctrl.c before the changes)
1864,snimrod,Done
1865,snimrod,Done
1866,nitzancarmi,it is safer to use int *oldstate  and put it instead of NULL.  That way we won't enable the flag be accident  for example in nvme_emu_io_driver.
1867,alex-mikheev,fixed
1868,snimrod,No need to add attribute for offload.  The offload parameter just helps to set the value in the existing emu_mode    No need for offload_emu_mode first because the original emu_mode is used and secondly because you can alway translate mode to string using emu_modes_str array  no need to keep string separately.
1869,snimrod,If offload parameter is not found at this point (when we know that emu_mode is also not defined) need to fail the configuration loading  user must specify if he want to run in offload or non-offload mode.    NOTE: If a developer set the hidden param emu_mode then we use it and don't bother to check if offload is defined or not  this is ok.
1870,snimrod,offload_emu_mode should be a local param of the function  I would also change the name to conf_emu_mode or something like that    Also use OFFLOAD_EMU_MODE and NON_OFFLOAD_EMU_MODE instead of specific modes.
1871,snimrod,This is only theoretical  this function can't fail cause you hard coded sent it a legal value.  The only purpose of this code is to avoid compilation error for not checking return code of a non-void function.    Anyway  there is a bug in the print  no value for the first  d (you should have got a warn for this)  I would change it to be:    nvmx_info( Failed setting emu mode ( s)  using default ( s)                      conf/offload_emu_mode  snap_config_get_emu_mode_str(sconfig));
1872,snimrod,I can't understand if the indentation of the  else  is fine  double check this please
1873,snimrod,Since the modes that are used for offload (cc_remote) and non-offload (sqe_only) might change in the future I suggest to add two defines:     define OFFLOAD_EMU_MODE SNAP_EMU_CC_REMOTE   define NON_OFFLOAD_EMU_MODE SNAP_EMU_SQE_ONLY
1874,snimrod,Here and in all other places you check if this is full offload  compare to OFFLOAD_EMU_MODE instead of specifically to CC_REMOTE
1875,snimrod,I don't see anywhere the handling of cores we discussed (use only the last core if this is full offload).
1876,nitzancarmi,This cannot yet be done. We currently create controller only after SPDK application is created (and cores were allocated)  and  read JSON file as part of ctrl creation.  We should finally read json file much sooner -  in main()  before starting SPDK application  and then we can decide how many cores to allocate based on json configuration.
1877,nitzancarmi,These are local variables inside function.  For offload - I have to provide some boolean param to fill in nvme_json_get_value()  so I must use it.  offload_emu_mode can be removed  but then we will have some kind of:  snap_config_set_emu_mode(sconfig  offload ? emu_modes_str[OFFLOAD_EMU_MODE] :  emu_modes_str[NON_OFFLOAD_EMU_MODE])).    I think it is confusing to put conditional assignment into function  but if you think it's better I can change it.
1878,snimrod,Let's further discuss offline  good enough for now.
1879,nitzancarmi,nvmx_error instead of nvmx_info
1880,snimrod,Done
1881,nitzancarmi,Please add  Tests:  section to commit message with what you did
1882,nitzancarmi,Please also make  AC_MSG_WARN([libsnap not found])  in m4/snap.m4 into AC_MSG_ERROR so in case libsnap is not installed  we will fail at configure stage  not runtime.
1883,hellerguyh,added
1884,hellerguyh,done  Also removed the check for HAVE_SNAP here
1885,snimrod,The commit message should be that this commit initialize nvme controller with an initialized sconfig struct instead of with json file name which the nvme controller should use to init sconfig struct by itself.
1886,snimrod,AT this point the controller have sconfig so it does not need the conf_filename as it is not relevant anymore.  Also it does not need ctrl->config so it should be removed from the struct (might cause more changes but this needs to be done..:-( ) and not initialize it with json_parse_config  Lines 733 to 741 can be removed.  
1887,snimrod,No need for this as ctrl->config ashould also be removed
1888,hellerguyh,why was ctrl->config left in the first place when sconfig was inserted?
1889,snimrod,Because it was used for static namespaces which we knew that will soon be removed and required big effort to move to new config so we left it as is with old config.  Now that static namespaces was removed old config can also be removed.
1890,hellerguyh,done
1891,hellerguyh,done
1892,alex-mikheev,I think you need to initialize semaphores before starting the thread
1893,alex-mikheev,This requires some kind of locking. Potentially it can cause corruption if both vendor command and async event happen at the same time
1894,alex-mikheev,also should be under the lock
1895,alex-mikheev,and here
1896,andrii-holovchenko,Also need to add `uuid-dev` in `Build-Depends` section
1897,alex-mikheev,lets add nvmx_info so that we know what is our hostid. May be useful in case we need to debug. 
1898,hellerguyh,maybe better to set it to false/true in only one place?
1899,nitzancarmi,I worry since we use ns->block_size at nvme_spdk_read_prp_nb() without taking namespaces_lock().  Of course we can't take the lock there  but we need a way to suspend all traffic while we change the values. @alex-mikheev any suggestions here?
1900,alex-mikheev,I don't see that resize is called when block_size or md_size has changed. It is only called when num_blocks is different. We should not change the block_size or md_size here.  If we want to be safe we should check that  they are still the same. Otherwise we are risking data corruption.    Another intresting info is spdk does not let shrink bdev size if bdev is in use.    We do suspend all io processing with nvme_ctrl_pgs_susped() so that we don't use invalid namespace object. 
1901,Tom-Wu-2019,if we only change `num_blocks`  do we still need to suspend all the traffic? I found we do not use `ns->num_blocks` in the IO path  then i understanding we do not need to suspend the io traffic while change the value of `ns->num_blocks`.
1902,nitzancarmi,@Tom-Wu-2019 you're right. ns->num_blocks can be changed without suspending IO.
1903,Tom-Wu-2019,one more thing is  SPDK already handle the case to shrink bdev size: it will return error if try to shrink bdev size  and no resize event will sent. And  since we do not need to suspend the io traffic while change `num_blocks` only  then i think we do not need to check that they are still the same  becasue it will do no harm to re-assign a same value to `ns->num_blocks`  even SPDK do allow to resize a bdev to a same size.
1904,alex-mikheev,Consider changing it to the AC_CHECK_FUNCS because you only want a define:  https://www.gnu.org/software/autoconf/manual/autoconf-2.67/html_node/Generic-Functions.html
1905,Tom-Wu-2019,> Consider changing it to the AC_CHECK_FUNCS because you only want a define:  > https://www.gnu.org/software/autoconf/manual/autoconf-2.67/html_node/Generic-Functions.html    neither  AC_CHECK_FUNC nor  AC_CHECK_FUNCS works in here  HAVE_SPDK_BDEV_OPEN_EXT will not defined.
1906,alex-mikheev,It looks like AC_CHECK_FUNCS takes libraries from  LIBS and not from the LDFLAGS. Try following:  ```   diff --git a/m4/spdk.m4 b/m4/spdk.m4  index da8d366..fe1d059 100644  --- a/m4/spdk.m4  +++ b/m4/spdk.m4  @@ -34 6 +34 7 @@ AS_IF([test  x spdk_app  == xyes]            save_LDFLAGS= LDFLAGS           save_CFLAGS= CFLAGS           save_CPPFLAGS= CPPFLAGS  +        save_LIBS= LIBS           AS_IF([test x/usr ==  x with_spdk ]              []              [spdk_incl= -I with_spdk/include -I with_dpdk/include -I with_dpdk/include/dpdk  @@ -59 6 +60 8 @@ exit(1);               AC_SUBST(SPDK_DIR       [ with_spdk ])               AC_SUBST(SPDK_CPPFLAGS  [ spdk_incl ])               AC_SUBST(SPDK_CFLAGS    [ spdk_incl ])  +            LIBS= -lspdk  spdk_extra_libs  LIBS  +            AC_CHECK_FUNCS(spdk_bdev_open_ext)               ]                [AC_MSG_ERROR([libsdpk_event not found]); spdk_app=no]                [ spdk_extra_libs])  @@ -66 6 +69 7 @@ exit(1);           LDFLAGS= save_LDFLAGS           CFLAGS= save_CFLAGS           CPPFLAGS= save_CPPFLAGS  +        LIBS= save_LIBS           ] [:])  ```
1907,Tom-Wu-2019,I have tried a new solution by check if spdk_bdev_open_ext have declared from spdk/bdev.h  and this solution works. should I replace this by use AC_CHECK_FUNCS?
1908,alex-mikheev,Please replace. AC_CHECK_FUNCS is a stronger check than AC_CHECK_DECLS
1909,Tom-Wu-2019,have done the replacement.
1910,alex-mikheev,now you don't have to do AC_DEFINE() it will happen automatically
1911,nitzancarmi,It is not in use anymore on latest snap-3.0.
1912,nitzancarmi,I assume this is a leftover print
1913,nitzancarmi,another redundant print? maybe put in debug
1914,nitzancarmi,small typo on comment
1915,nitzancarmi,Indentation of nvme_spdk_ctrl.h
1916,nitzancarmi,These can be together with other spdk includes in the group
1917,nitzancarmi,I wonder if we create subsystem per port  can we still use the same namings?  Maybe it is better to use subsys->subnqn
1918,alex-mikheev,removed later
1919,alex-mikheev,yep  removing
1920,alex-mikheev,removed
1921,alex-mikheev,fixed
1922,alex-mikheev,fixed
1923,alex-mikheev,fixed
1924,alex-mikheev,sn and mn are global per subsystem
1925,alex-mikheev,This is problematic  here and everywhere else.    rw points to the receive buffer that is posted and potentially can be used again after progress. Request lifetime potentially can be much longer.    So you have to copy sgl like (base  len  type) just like we do with prp.
1926,Tom-Wu-2019,are you suggesting that since this API is changed to async style then the posted receive buffer may got changed before it handled  and we should keep a copy for it  right?
1927,alex-mikheev,yes. nvme_cmd_rw_t *rw is valid only during the single progress.
1928,nitzancarmi,Where is it decided that SGL element cannot be larger than 8K? Is it something defined by spec? Do we tell that to host somewhere (maybe in Identify)? I'm asking our of curiosity :-).
1929,Tom-Wu-2019,before Alex introduce .request_submit() callback to snap emulater operation callback set   snap use .dma_read() and .dma_write() callback to do the RDMA read/write with host.  And snap emulater(mlx emulater is the same) use a buffer to store/send data for RDMA   and this buffer is 8192 bytes allocated.  This is my understanding to this 8K limitation applied in here. I am not sure I am right.    and SNAP inform this to host by set the `mdts` filed in controller identify register to 1   then host need to max date length transfered between controller is (1 << mdts) * (1 << cap.mpsmin) = (1 << 1) * (1 << 12) = 8192    Actually  I remember I use mdts=4 in mlnx_snap.json file and set snap work under sqe_cc mode   and I met the limitation error when I try to load nvme driver on host. Just now  I removed this  'nvmx_assertv_always(len <= 8192   len  d is too big\n   (int)len);' and did the same test again   I succeed load nvme driver on host and did same dd/fio test.
1930,alex-mikheev,indentation is a bit off here
1931,alex-mikheev,lets also check oaes in the id_ctrl unit test. 
1932,nitzancarmi,Please use strncasecmp() when dealing with user inputs  total size is anyway sizeof(script_whitelist[i]). In addition  why not be case sensitive?
1933,nitzancarmi,Move the prints here (and below in this function) to DEBUGLOG.
1934,nitzancarmi,Not sure 32e would be enough here. For example  setting nqn value. I suggest put it as 256  and maybe put it under a macro like SPDK_MAX_SCRIPT_ARG_SIZE.
1935,nitzancarmi,I still worried of the option that a malicious user will put  ..  in script name  for example  ../../../usr/bin/rm   and this is a security breach. I think a better way is to iterate over all files in the current directory  and match script_name to one of these values (a demo code is here: https://stackoverflow.com/a/4204758)
1936,nitzancarmi,There will be scripts who expect much larger outputs. Let's put it under macro and enlarge it to  let's say  4096.
1937,alex-mikheev,how you know that opcode uses nsid ? there are few admin commands that does not.
1938,alex-mikheev,There is no need to check for passthrough here. Just set req->status and cqe->result in the passthrough handler
1939,umanskymax,Let s set patch to `/opt/mellanox/mlnx_snap/exec_files/`
1940,andrii-holovchenko,done
1941,alex-mikheev,style: no need for braces now
1942,alex-mikheev,style: no need for braces
1,alex-mikheev,daily merge
2,alex-mikheev,@Artemy-Mellanox may be it worth moving qp functions from test to src ? 
3,Artemy-Mellanox,Yes. Just need to decide - make them part of devx or nvme. Also need to introduce structs for QP  CQ  MR etc...  again either - devx_qp  nvmx_qp  or just use mlx5dv_qp.
4,alex-mikheev,@Artemy-Mellanox  do you want me to merge this PR and do the structs/move in another ?
5,mike-dubman,bot:retest
6,mike-dubman,bot:retest
7,mike-dubman,bot:retest
8,alex-mikheev,@Artemy-Mellanox   inline removed    I added objects for cq and qp. IMHO it is better because send/recv and cq polling are not specific to the nvme emulation. 
9,alex-mikheev,@Artemy-Mellanox  pls take a look at the new commits
10,mike-dubman,bot:retest
11,mike-dubman,bot:retest
12,mike-dubman,bot:retest
13,mike-dubman,bot:retest
14,mike-dubman,bot:retest
15,mike-dubman,bot:retest
16,mike-dubman,bot:retest
17,swx-jenkins2,Can one of the admins verify this patch?
18,alex-mikheev,@miked-mellanox @yshestakov first iteration of the packaging scripts. 
19,alex-mikheev,@orendu @yshestakov @vasilyMellanox @miked-mellanox @vladsokolovsky   Please take a look
20,swx-jenkins2,Can one of the admins verify this patch?
21,mike-dubman,@orendu - how one can generate doxygen docs and in which formats?  If it is manually - need to update README file with howto  Best to add it into autotools  you can check integration example here (also with Yossi)  https://github.com/openucx/ucx/pull/2650/files  
22,alex-mikheev,@vasilyMellanox please also add appropriate set/get-feaures command to the basic sanity script.
23,alex-mikheev,@orenu it looks like adding O_SYNC makes controller too slow so that linux complains and resets it. Without O_SYNC i am getting the same iops writing to file on mmc as writing to the ramdisk
24,alex-mikheev,@orendu please take a look. 
25,alex-mikheev,@orendu Hopefully all your comments are addressed.   @maxgurtovoy   I also add a worker context and a methods to obtain worker/queue context.   The worker context is needed so the upper layer can deal with a bar read event.
26,MrBr-github,bot:retest
27,alex-mikheev,@maxgurtovoy @aviadye @miked-mellanox  can you merge this pr please ?
28,alex-mikheev,@maxgurtovoy obsoleted by https://github.com/Mellanox/nvmx/pull/96
29,swx-jenkins2,Can one of the admins verify this patch?
30,mike-dubman,Ok to test
31,swx-jenkins2,Can one of the admins verify this patch?
32,swx-jenkins2,Can one of the admins verify this patch?
33,swx-jenkins2,Can one of the admins verify this patch?
34,alex-mikheev,@yshestakov  @galshachaf @aviadye   We can not merge this until we get new kernel + devx working. 
35,swx-jenkins2,Can one of the admins verify this patch?
36,MrBr-github,bot:retest
37,alex-mikheev,@vasilyMellanox please rebase to the latest master
38,aviadye,Reviewed both commits. All is Ok.  Also tested on IB and ETH and working.
39,aviadye,Approved
40,maxgurtovoy,what about the case that you get a wrong path for the config file ? lets say /tmp/no_file.json ?  will you return NULL ?     line 685 in  jason_parser.c has error flow for that case
41,vasilyMellanox,line 685 in jason_parser.c has error flow for that case    From: Max Gurtovoy <notifications@github.com>  Sent: Tuesday  September 18  2018 00:48  To: Mellanox/nvmx <nvmx@noreply.github.com>  Cc: Vasily Philipov <vasilyf@mellanox.com>; Author <author@noreply.github.com>  Subject: Re: [Mellanox/nvmx] Json default config file path fix ( 124)      what about the case that you get a wrong path for the config file ? lets say /tmp/no_file.json ?  will you return NULL ?      You are receiving this because you authored the thread.  Reply to this email directly  view it on GitHub<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F124 23issuecomment-422183038 data=02 7C01 7Cvasilyf 40mellanox.com 7C2566b2bc267a43c537d308d61ce73d59 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636728176819805733 sdata=CROOr 2FC1elsLkW3uNs80dqPB2Mo4siDiza1X96xF 2FK0 3D reserved=0>  or mute the thread<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2Fnotifications 2Funsubscribe-auth 2FAIVK2hm90wclGXFs0ZRrQCe-H_e5OzXDks5ucBiPgaJpZM4Wru6X data=02 7C01 7Cvasilyf 40mellanox.com 7C2566b2bc267a43c537d308d61ce73d59 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636728176819805733 sdata=HW3KpJi9xXDcZ6rUI 2B 2BU 2BjAcnrbc5WzNxTQCgi5wahM 3D reserved=0>.  
42,swx-jenkins2,Can one of the admins verify this patch?
43,swx-jenkins2,Can one of the admins verify this patch?
44,swx-jenkins2,Can one of the admins verify this patch?
45,vasilyMellanox,No  as you can see it s for test needs only   gtest tests the  ./src/json/sample.json  only and nothing else  the tests doesn t make sense if user reconfigures the json file    From: Yuriy <notifications@github.com>  Sent: Wednesday  October 03  2018 12:10  To: Mellanox/nvmx <nvmx@noreply.github.com>  Cc: Vasily Philipov <vasilyf@mellanox.com>; Author <author@noreply.github.com>  Subject: Re: [Mellanox/nvmx] Json parser code refactoring ( 130)      @yshestakov commented on this pull request.    ________________________________    In tests/test_controller.cc<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F130 23discussion_r222236598 data=02 7C01 7Cvasilyf 40mellanox.com 7Ce2163338add64e882f0a08d6290ff30f 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636741545815764510 sdata=nmXfiY4cmvYWHSWwL0BJJ61KA9YlwWQyJK302nH6nmE 3D reserved=0>:    > @@ -781 10 +781 20 @@ TEST_F(NvmeControllerTest  adm_feature_async_event_conf) {     TEST_F(NvmeControllerTest  json_config) {         int num_values;         struct nvme_json_val *values =    -            nvme_json_parse_config(JSON_CONFIG_FILE   num_values);    +            nvme_json_parse_config( ./src/json/sample.json    num_values);    I don't like hardcoded path to the config like src/json/sample.json  Could we have default one /etc/nvmx/config.json  and be able to redefine it with shell env like NVMX_CONFIG=./src/json/sample.json`      You are receiving this because you authored the thread.  Reply to this email directly  view it on GitHub<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F130 23discussion_r222236598 data=02 7C01 7Cvasilyf 40mellanox.com 7Ce2163338add64e882f0a08d6290ff30f 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636741545815764510 sdata=nmXfiY4cmvYWHSWwL0BJJ61KA9YlwWQyJK302nH6nmE 3D reserved=0>  or mute the thread<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2Fnotifications 2Funsubscribe-auth 2FAIVK2i-TWCszkIxxQb8Cs4oplBJu4qewks5uhH7SgaJpZM4XFjWT data=02 7C01 7Cvasilyf 40mellanox.com 7Ce2163338add64e882f0a08d6290ff30f 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636741545815774515 sdata=mSG4 2FthfmlkwFhdn6SK 2F4WYuylAXtcgGD0G5I1afX3I 3D reserved=0>.  
46,mike-dubman,who installs and registered nvmx.service file? spec should do it.
47,yshestakov,> who installs and registered nvmx.service file? spec should do it.    I've added needed steps into the `nvmx.spec` file
48,swx-jenkins2,Can one of the admins verify this patch?
49,mike-dubman,+1
50,yshestakov,> +1    Thank you :)
51,swx-jenkins2,Can one of the admins verify this patch?
52,yshestakov,> Can one of the admins verify this patch?    I see nothing specific to the build and work environment.
53,Artemy-Mellanox,Can someone  please  merge this?
54,alex-mikheev,@Artemy-Mellanox will this pr work with the current master code ? or do we need to merge code from dev-l-vrt137 first ?
55,yshestakov,@miked-mellanox Could we merge this PR?
56,vasilyMellanox,Updated    From: Alex Mikheev <notifications@github.com>  Sent: Wednesday  October 10  2018 15:27  To: Mellanox/nvmx <nvmx@noreply.github.com>  Cc: Vasily Philipov <vasilyf@mellanox.com>; Author <author@noreply.github.com>  Subject: Re: [Mellanox/nvmx] Assign namespace loop index fix ( 136)      @alex-mikheev requested changes on this pull request.    ________________________________    In src/nvmf_emu_ctrl.c<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F136 23discussion_r224053874 data=02 7C01 7Cvasilyf 40mellanox.com 7C480727f3aa5143f6ff7708d62eabbcb3 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636747712473804844 sdata=9wObBdrOnv1VqbWEEkVVumKupuwsDkYR1gL 2FK2oLF8k 3D reserved=0>:    > @@ -571 7 +571 7 @@ static int nvmf_ctrl_update_namespaces(nvmf_emu_ctrl_t *ctrl)              * Iterate through the pages and fetch each chunk of 1024 namespaces until              * there are no more active namespaces              */    -        for (i = 0; i < num_pages; i++) {    +        for (i = 0; i < num_pages; j = 0  i++) {    imho it will make code mode readable if j is set to zero just before the inner while() loop. Or may be change inner while() loop to the for()      You are receiving this because you authored the thread.  Reply to this email directly  view it on GitHub<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F136 23pullrequestreview-163326934 data=02 7C01 7Cvasilyf 40mellanox.com 7C480727f3aa5143f6ff7708d62eabbcb3 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636747712473814854 sdata=RnVzykks0hFoDF 2BeOhbHyiJkCt5omv 2B7nfbz86ZL5OM 3D reserved=0>  or mute the thread<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2Fnotifications 2Funsubscribe-auth 2FAIVK2jvWnYCmk_rQU0QJyTTJuG6YEgaIks5ujeetgaJpZM4XS-5D data=02 7C01 7Cvasilyf 40mellanox.com 7C480727f3aa5143f6ff7708d62eabbcb3 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636747712473824859 sdata=HSSlDdE9YmbvpC3NL4BM 2F7e42WV2S3raPgh86QDFe7s 3D reserved=0>.  
57,alex-mikheev,Merging as is because the code became our de facto master.    In the future let's try to do MUCH smaller PRs. 
58,alex-mikheev,@nitzancarmi please resolve conflicts and rebase
59,galshachaf,@alex-mikheev @aviadye please review
60,swx-jenkins2,Can one of the admins verify this patch?
61,kaomri,@aviadye please review
62,maxgurtovoy,Omri  please fix coding style. Use 4 spaces instead of tabs. Don't leave un-needed lines commented.
63,alex-mikheev,@omrikmellanox please also take a look
64,alex-mikheev,@Artemy-Mellanox  can you update devx with fixed mkey headers and add devx submodule change to this PR ?
65,galshachaf,> @Artemy-Mellanox can you update devx with fixed mkey headers and add devx submodule change to this PR ?    I already opened a relevant pull request:     https://github.com/Mellanox/devx/pull/18  --   
66,galshachaf,changes work with FW 18.99.5420.  @aviadye please approve
67,kaomri,@alex-mikheev changed made
68,kaomri,@alex-mikheev added flag and updated PRP list for partial last entry in list
69,galshachaf,@maxgurtovoy please review
70,galshachaf,@aviadye this patch works with FW version 18.99.5433
71,galshachaf,@aviadye changes are done
72,galshachaf,@aviadye @alex-mikheev let's merge this one :)
73,swx-jenkins2,Can one of the admins verify this patch?
74,swx-jenkins2,Can one of the admins verify this patch?
75,yshestakov,Going to pull changes into master and re-create a branch because I've poisoned this one
76,vasilyMellanox,Updated according to Oren s remerks    From: orendu <notifications@github.com>  Sent: Thursday  October 25  2018 15:49  To: Mellanox/nvmx <nvmx@noreply.github.com>  Cc: Vasily Philipov <vasilyf@mellanox.com>; Author <author@noreply.github.com>  Subject: Re: [Mellanox/nvmx] Json parser new API ( 159)      @orendu commented on this pull request.    Looks good  See comments    ________________________________    In src/json/mlnx_nvmx_config.json<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F159 23discussion_r228154898 data=02 7C01 7Cvasilyf 40mellanox.com 7C2f1a435475c542866c8708d63a784082 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636760685484054753 sdata=87UZ 2BGmXkXr6rEwq8o 2FhDfJG 2BcPmnfU9vYd6A1jCiDY 3D reserved=0>:    > @@ -0 0 +1 47 @@    +{    +     // :  This file is a sample for the sintax      is an example of the configuration syntax    ________________________________    In src/json/mlnx_nvmx_config.json<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F159 23discussion_r228155211 data=02 7C01 7Cvasilyf 40mellanox.com 7C2f1a435475c542866c8708d63a784082 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636760685484064763 sdata=tWXMzosxq6Pirx2KzzIHla8HIViU 2FtJKOvf30EK64iY 3D reserved=0>:    > @@ -0 0 +1 47 @@    +{    +     // :  This file is a sample for the sintax      +     // :  It is strongly recommended to user write his own one      obvious - remove    ________________________________    In src/json/mlnx_nvmx_config.json<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F159 23discussion_r228155287 data=02 7C01 7Cvasilyf 40mellanox.com 7C2f1a435475c542866c8708d63a784082 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636760685484064763 sdata=X9 2B1JErOp 2F98qZfBbSy5zsCc4oPWz4iSD7qyIqm 2FWNw 3D reserved=0>:    > @@ -0 0 +1 47 @@    +{    +     // :  This file is a sample for the sintax      +     // :  It is strongly recommended to user write his own one      +     // :  and specify it through NVME_CONFIG env var      obvious - remove    ________________________________    In src/json/mlnx_nvmx_config.json<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F159 23discussion_r228155778 data=02 7C01 7Cvasilyf 40mellanox.com 7C2f1a435475c542866c8708d63a784082 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636760685484074772 sdata=mZB 2Ba4QiXK0x 2BvRhMZkvcE8Uo3Y45oz8K7uei6T9G3c 3D reserved=0>:    > +    +     // :  List of backends used by the ctrl  each with parameters global to the backend      +       backends : [ {    +              id :  nvmftgt1      +              type :  remote_nvmf      +              paths : [    +                     {    +                              addr :  10.0.0.1:4420      +                              iqn :  xxxx    +                     }     +                     {    +                              addr :  10.0.0.2:4420      +                              iqn :  yyyy    +                     }    +             ]    +      } ]    In the ctrl object you refer to 'localfstgt'  better have such backend example    ________________________________    In src/json/mlnx_nvmx_config.json<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F159 23discussion_r228155978 data=02 7C01 7Cvasilyf 40mellanox.com 7C2f1a435475c542866c8708d63a784082 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636760685484074772 sdata=d5sR8ISmBTyibQoaRS4 2B2y6c6CGZHKj 2BWDieFEVVGsI 3D reserved=0>:    > +     // :  It is strongly recommended to user write his own one      +     // :  and specify it through NVME_CONFIG env var      +     ctrl : {    +         vid : 5555     +         ssvid : 45778     +         sqes : 102     +         cqes : 68     +    +              // :  List of namespaces to present to host      +              // :  Each namespace refers to a backend and gives it parameters specific to this namespace      +              namespaces : [ {    +                      nsid : 0     +                      size_mb : 512     +                      format : {  data : 4096   metadata : 0 }     +                      backend : {    +                              id :  localfstgtxxx      why did you add 'xxx'? this shows how the same backend can be used twice  as different namespaces  with different local block path      You are receiving this because you authored the thread.  Reply to this email directly  view it on GitHub<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F159 23pullrequestreview-168358035 data=02 7C01 7Cvasilyf 40mellanox.com 7C2f1a435475c542866c8708d63a784082 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636760685484084786 sdata=7vPdLMqGydeqF2sU 2BF8Wh4gw2J 2F9l3BNZKfCtz0EPv8 3D reserved=0>  or mute the thread<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2Fnotifications 2Funsubscribe-auth 2FAIVK2t_qtmtrntARwgKujAZOfYmdqtPwks5uobMqgaJpZM4X6G0Q data=02 7C01 7Cvasilyf 40mellanox.com 7C2f1a435475c542866c8708d63a784082 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636760685484094786 sdata=E4HN7 2FfhyoIdRx95OsUPlA 2Fa5 2FIOoujJhjiyAoDpiSI 3D reserved=0>.  
77,swx-jenkins2,Can one of the admins verify this patch?
78,swx-jenkins2,Can one of the admins verify this patch?
79,swx-jenkins2,Can one of the admins verify this patch?
80,yshestakov,> it will reset 1st dev  whatif >1 ?  > IMO - should reset all found or specified by pci b:d;f    Do you mean a case when we have many SNICs installed into the server?  In case of single SNIC it is enough to send reset to the first PCI function (.0)
81,yshestakov,```    rpm -qlp upload/nvmx-ansible-0.9-6.mlnx.noarch.rpm |grep LICE  /usr/share/doc/nvmx-ansible-0.9/LICENSE    rpm -qlp upload/nvmx-0.9-6.mlnx.x86_64.rpm |grep LICE  /usr/share/doc/nvmx-0.9/LICENSE  ```
82,swx-jenkins2,Can one of the admins verify this patch?
83,swx-jenkins2,Can one of the admins verify this patch?
84,mike-dubman,+1
85,swx-jenkins2,Can one of the admins verify this patch?
86,swx-jenkins2,Can one of the admins verify this patch?
87,swx-jenkins2,Can one of the admins verify this patch?
88,vasilyMellanox,Updated    From: Alex Mikheev <notifications@github.com>  Sent: Monday  October 29  2018 18:13  To: Mellanox/nvmx <nvmx@noreply.github.com>  Cc: Vasily Philipov <vasilyf@mellanox.com>; Author <author@noreply.github.com>  Subject: Re: [Mellanox/nvmx] Json parser new API ( 159)      @alex-mikheev requested changes on this pull request.    see my comments.      You are receiving this because you authored the thread.  Reply to this email directly  view it on GitHub<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F159 23pullrequestreview-169385193 data=02 7C01 7Cvasilyf 40mellanox.com 7C34ef9c41edbc40dad91808d63db95f7c 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636764263721904036 sdata=8USHWI4iZ1A4TxjlssoOoSaLST 2FQZyjNPNPJCfvtT1o 3D reserved=0>  or mute the thread<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2Fnotifications 2Funsubscribe-auth 2FAIVK2lC9jlI_qanwL_b2MXyaMJ1DP1I1ks5upykAgaJpZM4X6G0Q data=02 7C01 7Cvasilyf 40mellanox.com 7C34ef9c41edbc40dad91808d63db95f7c 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636764263721904036 sdata=tZJKXBVPuOGsC 2BigSSw1Z4GtPHtlP0zv 2FsqFUggperY 3D reserved=0>.  
89,swx-jenkins2,Can one of the admins verify this patch?
90,swx-jenkins2,Can one of the admins verify this patch?
91,swx-jenkins2,Can one of the admins verify this patch?
92,swx-jenkins2,Can one of the admins verify this patch?
93,yshestakov,Wow  We have NVMX CI working now -- green light for this PR ;)
94,mike-dubman,Cool.plz send email to all nvmx players that review and merge only after ci got green and how to read ci reason for fail. What about coverity and vg?
95,mike-dubman,+1
96,yshestakov,> Cool. plz send email to all nvmx players that review and merge only after ci got green and how to read ci reason for fail.    Ok  going to do so now :)    > What about coverity and vg?    Let me prepare Docker image with Covery and Valgrind.
97,alex-mikheev,@aviadye please review
98,swx-jenkins2,Can one of the admins verify this patch?
99,yshestakov,bot:retest
100,vasilyMellanox,updated    From: Alex Mikheev <notifications@github.com>  Sent: Tuesday  November 06  2018 11:02  To: Mellanox/nvmx <nvmx@noreply.github.com>  Cc: Vasily Philipov <vasilyf@mellanox.com>; Author <author@noreply.github.com>  Subject: Re: [Mellanox/nvmx] Json parser new API ( 178)      @alex-mikheev requested changes on this pull request.    ________________________________    In samples/controller.c<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F178 23discussion_r231041929 data=02 7C01 7Cvasilyf 40mellanox.com 7Cf03094b54a5e416223c308d643c6911f 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636770917459172394 sdata=cPeBDbYsz5vmJj9kt3zjT4ezs3XmBxIxn84DIFXq2Os 3D reserved=0>:    >      nvme_ctrl_id_t *id =  ctrl->id_ctrl;             memset(id  0  sizeof(*id));    -    //values = nvme_driver_json_config(ctrl->driver   num_values);    +    values = nvme_driver_json_config(ctrl->driver   num_values);    Let's add an nvmx_assert_always just in case.      You are receiving this because you authored the thread.  Reply to this email directly  view it on GitHub<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F178 23pullrequestreview-171915703 data=02 7C01 7Cvasilyf 40mellanox.com 7Cf03094b54a5e416223c308d643c6911f 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636770917459172394 sdata=OjKlszntDryP8JH3s2vjueRuNoGxtGl0vboU8r5zXcU 3D reserved=0>  or mute the thread<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2Fnotifications 2Funsubscribe-auth 2FAIVK2tDqhn1acsR8Qv8rMnNRKdH65rZ3ks5usVAegaJpZM4YOMLF data=02 7C01 7Cvasilyf 40mellanox.com 7Cf03094b54a5e416223c308d643c6911f 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636770917459182403 sdata=TjIdyetwSLlB 2Bebqodd24yN 2Bzx 2BeNwGhaz1lbFG0Ff4 3D reserved=0>.  
101,vasilyMellanox,Updated    From: Max Gurtovoy <notifications@github.com>  Sent: Tuesday  November 06  2018 11:56  To: Mellanox/nvmx <nvmx@noreply.github.com>  Cc: Vasily Philipov <vasilyf@mellanox.com>; Author <author@noreply.github.com>  Subject: Re: [Mellanox/nvmx] Json parser new API ( 178)      @maxgurtovoy commented on this pull request.    ________________________________    In src/json/mlnx_nvmx_config.json<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F178 23discussion_r231058846 data=02 7C01 7Cvasilyf 40mellanox.com 7C97a9520a8e344b79ba3808d643ce0d38 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636770949592824259 sdata=C9GCudbevb0z5vkcbkIbWBSuple 2BqqzfjOoYBpDBdlg 3D reserved=0>:    > @@ -0 0 +1 39 @@    +{    +     // :  This file is an example of the configuration syntax      +     ctrl : {    +         vid : 5555     +         ssvid : 45778     +         sqes : 102     +         cqes : 68     +         mn :  Microsoft Azure NVMe Controller      +    +              // :  List of namespaces to present to host      lets create a unit_tests.json with needed definitions.  In this mlnx_nvmx_config we should have only parsed attributes (this is the example for customers)    ________________________________    In src/json/mlnx_nvmx_config.json<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F178 23discussion_r231058910 data=02 7C01 7Cvasilyf 40mellanox.com 7C97a9520a8e344b79ba3808d643ce0d38 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636770949592824259 sdata=gb2kuZToZ9khz9SM54Ww 2B2tePCocZXbqr 2F3QqG5thPI 3D reserved=0>:    > @@ -0 0 +1 47 @@    +{    +     // :  This file is an example of the configuration syntax      +     ctrl : {    +         vid : 5555     +         ssvid : 45778     please fix.    ________________________________    In src/nvme_emu_io_driver.c<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F178 23discussion_r231059330 data=02 7C01 7Cvasilyf 40mellanox.com 7C97a9520a8e344b79ba3808d643ce0d38 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636770949592834273 sdata=DfypDZYolosIgDfuJQm4AK0xrLoqLnKo 2F4EW1w9 2B2kA 3D reserved=0>:    > @@ -78 6 +78 22 @@ void nvme_driver_destroy(nvme_emu_driver_t *driver)         free(driver);     }        +static int __get_config_file(nvme_emu_driver_t *driver)    please change func name.  I also don't think we use __name convention for static function in this file.      You are receiving this because you authored the thread.  Reply to this email directly  view it on GitHub<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F178 23pullrequestreview-171936417 data=02 7C01 7Cvasilyf 40mellanox.com 7C97a9520a8e344b79ba3808d643ce0d38 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636770949592834273 sdata=p0w3MLzlQhRHtpp7I7qGLVJ2qJp 2B5AJLv5atMfwxZ 2Fo 3D reserved=0>  or mute the thread<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2Fnotifications 2Funsubscribe-auth 2FAIVK2sNDsOoRltyn21QmSGzNJoeMoafWks5usVytgaJpZM4YOMLF data=02 7C01 7Cvasilyf 40mellanox.com 7C97a9520a8e344b79ba3808d643ce0d38 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636770949592844269 sdata=e6h39Gl4Yhbh61cqgPaXvhgOLrV1Q2oki4tue9s7vK0 3D reserved=0>.  
102,kaomri,@maxgurtovoy @aviadye @alex-mikheev   A reminder
103,galshachaf,Yes  two separate PRs that are not dependent on each other
104,yshestakov,looks good to me
105,galshachaf,fixed. @alex-mikheev please approve
106,swx-jenkins2,Can one of the admins verify this patch?
107,galshachaf,and remove the define?
108,swx-jenkins2,Can one of the admins verify this patch?
109,alex-mikheev,Ideally it should be merged beforePR  192 then  192 will have to be rebased
110,galshachaf,@vasilyMellanox will fix
111,vasilyMellanox,rebased    From: Max Gurtovoy <notifications@github.com>  Sent: Sunday  November 11  2018 19:03  To: Mellanox/nvmx <nvmx@noreply.github.com>  Cc: Vasily Philipov <vasilyf@mellanox.com>; Author <author@noreply.github.com>  Subject: Re: [Mellanox/nvmx] Add more initial settings to the config file ( 197)      @maxgurtovoy requested changes on this pull request.    please rebase your code.  I've pushed code that parses ctrl.sn in Thur.      You are receiving this because you authored the thread.  Reply to this email directly  view it on GitHub<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F197 23pullrequestreview-173705992 data=02 7C01 7Cvasilyf 40mellanox.com 7C7839a915c8944e6aafe008d647f77cac 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636775525606728240 sdata=E24ffQalS2A7jTcastktVkIiCzrnDdLmLOm3RhJ 2FEE0 3D reserved=0>  or mute the thread<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2Fnotifications 2Funsubscribe-auth 2FAIVK2ipfstqnEhZlniFYP1xxBynpUH2Wks5uuFgugaJpZM4YUfQp data=02 7C01 7Cvasilyf 40mellanox.com 7C7839a915c8944e6aafe008d647f77cac 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636775525606738244 sdata=FXukKyZ4pf20tUS 2BOn7 2Bwz8R9JD4 2F 2Fz9QOyHpVKh1rA 3D reserved=0>.  
112,vasilyMellanox,Fixed..    From: Alex Mikheev <notifications@github.com>  Sent: Monday  November 12  2018 11:09  To: Mellanox/nvmx <nvmx@noreply.github.com>  Cc: Vasily Philipov <vasilyf@mellanox.com>; Author <author@noreply.github.com>  Subject: Re: [Mellanox/nvmx] GTESTS: id_ctrl test fixing ( 199)      @alex-mikheev requested changes on this pull request.    ________________________________    In tests/test_nvme_ctrl.cc<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F199 23discussion_r232578124 data=02 7C01 7Cvasilyf 40mellanox.com 7C07ad953c95654d20992e08d6487e8d4d 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636776105705623325 sdata=OWVaUZmjFimNwnz 2BkQhK8IzcY3OhDQfc9TdwlZhaXJY 3D reserved=0>:    > @@ -436 8 +436 19 @@ TEST_F(NvmeControllerTest  id_ctrl) {         /* check data */         nvme_ctrl_id_t *id = (nvme_ctrl_id_t *)prp1;        +    int rc;    +    nvme_config_t config;    +    +    rc = nvme_read_std_config_file( config);    can you move config file reading to the test setup ? We will need this pattern in other places too.      You are receiving this because you authored the thread.  Reply to this email directly  view it on GitHub<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F199 23pullrequestreview-173803022 data=02 7C01 7Cvasilyf 40mellanox.com 7C07ad953c95654d20992e08d6487e8d4d 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636776105705633330 sdata=Q 2BUGfPB 2B47IFVHXTqLjjyUTxpswRWYWHhhe90af6t8M 3D reserved=0>  or mute the thread<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2Fnotifications 2Funsubscribe-auth 2FAIVK2v0BRgs0ZYiGBR4SYFKEYnVeuglGks5uuTrIgaJpZM4YZOL- data=02 7C01 7Cvasilyf 40mellanox.com 7C07ad953c95654d20992e08d6487e8d4d 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636776105705643334 sdata=WmQXqedVNZxyJZzWCfxXQkyp107JXyNFWyXA8mg34eU 3D reserved=0>.  
113,vasilyMellanox,Updated    From: Max Gurtovoy <notifications@github.com>  Sent: Monday  November 12  2018 17:17  To: Mellanox/nvmx <nvmx@noreply.github.com>  Cc: Vasily Philipov <vasilyf@mellanox.com>; Author <author@noreply.github.com>  Subject: Re: [Mellanox/nvmx] GTESTS: id_ctrl test fixing ( 199)      @maxgurtovoy requested changes on this pull request.    why only the vid fixing ? what about other values of identify_ctrl ?      You are receiving this because you authored the thread.  Reply to this email directly  view it on GitHub<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F199 23pullrequestreview-173948810 data=02 7C01 7Cvasilyf 40mellanox.com 7C021b1725649a4fbaad6608d648b1e97c 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636776326295447253 sdata=v9hEnw 2F9lJEU5gFkP344PB0wLmoCqDQEQiwhKnYkeXk 3D reserved=0>  or mute the thread<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2Fnotifications 2Funsubscribe-auth 2FAIVK2gzOnewpAGI68EEg7jYnTo2U-nk9ks5uuZDygaJpZM4YZOL- data=02 7C01 7Cvasilyf 40mellanox.com 7C021b1725649a4fbaad6608d648b1e97c 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636776326295447253 sdata=FGH5YdjhibKja3cMgHRhMWa9 2FmP0hkyF 2Bqlu9NLJJAw 3D reserved=0>.  
114,alex-mikheev,please rebase. I refactored nvme tests.
115,vasilyMellanox,rebased          From: Alex Mikheev <notifications@github.com>  Sent: Wednesday  November 14  2018 18:06  To: Mellanox/nvmx <nvmx@noreply.github.com>  Cc: Vasily Philipov <vasilyf@mellanox.com>; Author <author@noreply.github.com>  Subject: Re: [Mellanox/nvmx] Change config file structure ( 201)      please rebase. I refactored nvme tests.      You are receiving this because you authored the thread.  Reply to this email directly  view it on GitHub<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F201 23issuecomment-438717012 data=02 7C01 7Cvasilyf 40mellanox.com 7Ccd2e0ef58e3b49deed5d08d64a4b0ad1 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636778083551125913 sdata=fNe3ZSkEKG4B6SCd46u6O4jtEAYyfq2xBueE4fRpl1Q 3D reserved=0>  or mute the thread<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2Fnotifications 2Funsubscribe-auth 2FAIVK2iUNXhNIFO7oxNw98Baf_AhRu9Ntks5uvD9bgaJpZM4Ydscb data=02 7C01 7Cvasilyf 40mellanox.com 7Ccd2e0ef58e3b49deed5d08d64a4b0ad1 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636778083551135923 sdata=W7MFqVdL01w3h0 2B2Q 2Bk 2BmH8mhmC1 2FvywrJBQkt9zAWE 3D reserved=0>.  
116,vasilyMellanox,Updated    From: Alex Mikheev <notifications@github.com>  Sent: Thursday  November 15  2018 11:46  To: Mellanox/nvmx <nvmx@noreply.github.com>  Cc: Vasily Philipov <vasilyf@mellanox.com>; Author <author@noreply.github.com>  Subject: Re: [Mellanox/nvmx] Change config file structure ( 201)      @alex-mikheev requested changes on this pull request.    generally it looks good    ________________________________    In samples/controller.c<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F201 23discussion_r233771906 data=02 7C01 7Cvasilyf 40mellanox.com 7C7beecb9ffda645e42fe808d64adf2636 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636778719611208844 sdata=LCQAlGhteDhlLMf1sBKlb8YMrNYH2dS25wPVmM 2BEbPk 3D reserved=0>:    > @@ -413 15 +414 55 @@ static void nvme_ctrl_ns_init(nvme_ctrl_t *ctrl)             nvme_namespace_t *ns =  ctrl->namespaces[i];             nvme_id_ns_t *id_ns =  ns->id_ns;        +        uint64_t ns_size;    +    +        int nsid  size_mb;    +        int disk_block_order;    +    +        char param[64];    +        char namespace[64];    +    +        nvme_config_t *config =  ctrl->driver->config;    +    +        sprintf(namespace   ctrl.namespaces[ d]   i);    please use snprintf everywhere instead of sprintf    ________________________________    In src/nvme_emu_io_driver.c<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F201 23discussion_r233772700 data=02 7C01 7Cvasilyf 40mellanox.com 7C7beecb9ffda645e42fe808d64adf2636 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636778719611218845 sdata=oPPyTMT 2BWS 2FVJ 2FFDovFynAFx0GrV 2FGVj0tvDaSiv 2FPE 3D reserved=0>:    >          driver->type = NVME_EMU_MULTIPLE_EP_DRIVER_TYPE;    -        nr_endpoints = 2;    +    } else if (nr_endpoints < 0) {    +        nvmx_error( the patth  s doesn't exist in the config file   param);    typo: ^patth^path      You are receiving this because you authored the thread.  Reply to this email directly  view it on GitHub<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F201 23pullrequestreview-175262151 data=02 7C01 7Cvasilyf 40mellanox.com 7C7beecb9ffda645e42fe808d64adf2636 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636778719611218845 sdata=gKdM1LuNxkTIQxIt8Klm 2BLAoTKhHFPnn68uiQfHtW4k 3D reserved=0>  or mute the thread<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2Fnotifications 2Funsubscribe-auth 2FAIVK2hJLb8bd04zssdBznDcRrt07IMU8ks5uvTfWgaJpZM4Ydscb data=02 7C01 7Cvasilyf 40mellanox.com 7C7beecb9ffda645e42fe808d64adf2636 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636778719611228845 sdata=gBlODjvpSBfLwe8ubBMHwhPrxPnRc8lbpXJSJ5KKkJ0 3D reserved=0>.  
117,vasilyMellanox,Updated    From: Alex Mikheev <notifications@github.com>  Sent: Thursday  November 15  2018 12:32  To: Mellanox/nvmx <nvmx@noreply.github.com>  Cc: Vasily Philipov <vasilyf@mellanox.com>; Author <author@noreply.github.com>  Subject: Re: [Mellanox/nvmx] Change config file structure ( 201)      @alex-mikheev commented on this pull request.    ________________________________    In samples/controller.c<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F201 23discussion_r233788847 data=02 7C01 7Cvasilyf 40mellanox.com 7C81793f3b8d8344feeb8708d64ae5893c 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636778747064218735 sdata=9k3f3ZHE0zJ0s 2F5k1wRk9PXm6m7eYiJu3MEf0Bm3RNc 3D reserved=0>:    > @@ -324 13 +324 14 @@ static int nvme_ctrl_id_init(nvme_ctrl_t *ctrl)             /* TODO: this is ugly  refactor          */    -    rc = nvme_json_get_value(config   ctrl.num_namespaces      -                              ctrl->num_namespaces);    +    rc = nvme_json_get_array_size(config   ctrl.namespaces );    what happens if user does not define one ns ?      You are receiving this because you authored the thread.  Reply to this email directly  view it on GitHub<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F201 23discussion_r233788847 data=02 7C01 7Cvasilyf 40mellanox.com 7C81793f3b8d8344feeb8708d64ae5893c 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636778747064228744 sdata=f7rNQEncIp5b3DacJbSdVDrrdcmig5rU2E0qEe3iDJo 3D reserved=0>  or mute the thread<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2Fnotifications 2Funsubscribe-auth 2FAIVK2hHlN7zVEbYA1tjQXvop4Hnb7tn0ks5uvUKNgaJpZM4Ydscb data=02 7C01 7Cvasilyf 40mellanox.com 7C81793f3b8d8344feeb8708d64ae5893c 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636778747064228744 sdata=nM7LytMC0sWJXkFq2fgEoAKOMzPFLHIFT43CFJ0Kfp8 3D reserved=0>.  
118,swx-jenkins2,Can one of the admins verify this patch?
119,swx-jenkins2,Can one of the admins verify this patch?
120,swx-jenkins2,Can one of the admins verify this patch?
121,nitzancarmi,I will resend this PR after rebasing over ha code
122,nitzancarmi,Please rebase over latest code and add config file entry for that
123,swx-jenkins2,Can one of the admins verify this patch?
124,vasilyMellanox,    From: Max Gurtovoy <notifications@github.com>  Sent: Sunday  November 18  2018 17:41  To: Mellanox/nvmx <nvmx@noreply.github.com>  Cc: Vasily Philipov <vasilyf@mellanox.com>; Author <author@noreply.github.com>  Subject: Re: [Mellanox/nvmx] Change config file structure ( 201)      @maxgurtovoy requested changes on this pull request.    please check comments    ________________________________    In samples/controller.c<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F201 23discussion_r234448570 data=02 7C01 7Cvasilyf 40mellanox.com 7Cf59b0e5d761843642b3108d64d6c4863 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636781524793997427 sdata=RSriICtOXD 2F3wf658GzbKp3rkl6ytdNiVdiXb6xOIJA 3D reserved=0>:    > @@ -324 13 +324 14 @@ static int nvme_ctrl_id_init(nvme_ctrl_t *ctrl)             /* TODO: this is ugly  refactor          */    -    rc = nvme_json_get_value(config   ctrl.num_namespaces      -                              ctrl->num_namespaces);    +    rc = nvme_json_get_array_size(config   ctrl.namespaces );    please explain what will happen ?    ________________________________    In samples/controller.c<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F201 23discussion_r234448609 data=02 7C01 7Cvasilyf 40mellanox.com 7Cf59b0e5d761843642b3108d64d6c4863 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636781524793997427 sdata=WyjLADa22ViBx 2B0ch1CfHT5V6qvBo74Sm5Je55duIS0 3D reserved=0>:    >      if (rc < 0) {    -        nvmx_error( failed to get max number of namespaces );    +        nvmx_error( failed to get max number of namespaces array );    please write a correct error log    ________________________________    In samples/controller.c<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F201 23discussion_r234448650 data=02 7C01 7Cvasilyf 40mellanox.com 7Cf59b0e5d761843642b3108d64d6c4863 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636781524794007436 sdata=KKVpfScpSAESWxTCGl0eFQf3hOBTPLsz7fX9jsumVRw 3D reserved=0>:    >      if (rc < 0) {    -        nvmx_error( failed to get max number of namespaces );    +        nvmx_error( failed to get max number of namespaces array );    +        return -1;    +    } else if ( rc) {    +        nvmx_error( ctrl.namespaces array should have at least one element );    don't write code in the error log    ________________________________    In src/json/mlnx_nvmx_config.json<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F201 23discussion_r234449324 data=02 7C01 7Cvasilyf 40mellanox.com 7Cf59b0e5d761843642b3108d64d6c4863 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636781524794007436 sdata=h 2FL7uVJ8KD7sDJTkN 2F183QJzK0jVq4NLNn 2FEcY83HJI 3D reserved=0>:    >    -             eps : [ {    +         nr_io_queues : 32     +         name : null     it;s worth adding a comment about the name        Could you provide what do I have  to put there ?    ________________________________    In src/json/mlnx_nvmx_config.json<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F201 23discussion_r234449373 data=02 7C01 7Cvasilyf 40mellanox.com 7Cf59b0e5d761843642b3108d64d6c4863 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636781524794017441 sdata=Xcpv80d9pFaoNR2o9s 2FDLrLMLLrpQxaUGLXebqchegE 3D reserved=0>:    > @@ -72 8 +84 12 @@                     /*                      * integer - the port number in case of `cc_remote` emu type.                      */    -                 port : null    -            } ]    -        }    -    }    +                 port : null     your example now is cc_remote with nvmf_rdma. let's add a port and addr example instead of NULL    ________________________________    In src/nvme_emu_io_driver.c<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F201 23discussion_r234449436 data=02 7C01 7Cvasilyf 40mellanox.com 7Cf59b0e5d761843642b3108d64d6c4863 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636781524794017441 sdata=caBNGwJirPaJkyCqzra28Vo9 2Bx7 2F4EWUwiwJCTLyn 2BM 3D reserved=0>:    >      int disk_block_order;    +    size_t disk_size;    +    +    rc = nvme_json_get_array_size(config   ctrl.namespaces );    +    if (rc < 0) {    +        nvmx_error( failed to get max number of namespaces array );    wrong error log    ________________________________    In src/nvme_emu_io_driver.c<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F201 23discussion_r234449442 data=02 7C01 7Cvasilyf 40mellanox.com 7Cf59b0e5d761843642b3108d64d6c4863 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636781524794027454 sdata=x4gCEzuGamI7ygfwQD5oGpEsdjT5MPj4NlBEMf7bfCU 3D reserved=0>:    >      int disk_block_order;    +    size_t disk_size;    +    +    rc = nvme_json_get_array_size(config   ctrl.namespaces );    +    if (rc < 0) {    +        nvmx_error( failed to get max number of namespaces array );    +        return -1;    +    } else if ( rc) {    +        nvmx_error( ctrl.namespaces array should have at least one element );    see above comment    ________________________________    In src/nvme_emu_io_driver.c<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F201 23discussion_r234449475 data=02 7C01 7Cvasilyf 40mellanox.com 7Cf59b0e5d761843642b3108d64d6c4863 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636781524794027454 sdata=8W 2FzrIA9Z5wPPyvyoqkaPmNildOVQyNPcGa2UQI4zzY 3D reserved=0>:    >      int disk_block_order;    +    size_t disk_size;    +    +    rc = nvme_json_get_array_size(config   ctrl.namespaces );    +    if (rc < 0) {    +        nvmx_error( failed to get max number of namespaces array );    +        return -1;    +    } else if ( rc) {    +        nvmx_error( ctrl.namespaces array should have at least one element );    +        return -1;    +    }    +    +    rc = nvme_json_get_value(config   ctrl.namespaces[0].size_mb    size_mb);    +    if (rc < 0) {    If I set size == 0 ? please fail it. Users/QA will do it for sure.    ________________________________    In src/nvme_emu_io_driver.c<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F201 23discussion_r234449516 data=02 7C01 7Cvasilyf 40mellanox.com 7Cf59b0e5d761843642b3108d64d6c4863 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636781524794037455 sdata=KzW7EG0i6VrEvyE 2FQbdmrlrY1NQ9KC4B 2BQMtP8U 2FTHU 3D reserved=0>:    > +    } else if ( rc) {    +        nvmx_error( ctrl.namespaces array should have at least one element );    +        return -1;    +    }    +    +    rc = nvme_json_get_value(config   ctrl.namespaces[0].size_mb    size_mb);    +    if (rc < 0) {    +        nvmx_error( Each namespace should define size_mb field into the config file );    +        return rc;    +    }    +    +    disk_size = 1024ULL * 1024ULL * size_mb;    +    +    rc = nvme_json_get_value(config   ctrl.namespaces[0].format.block_order      +                              disk_block_order);    +    if (rc < 0) {    we support only block order 12 and 9.  please make sure to fail otherwise.  Error flows     ________________________________    In src/nvme_emu_io_driver.c<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F201 23discussion_r234449562 data=02 7C01 7Cvasilyf 40mellanox.com 7Cf59b0e5d761843642b3108d64d6c4863 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636781524794037455 sdata=H 2FQvTrYmFpkHMTwT34pCyhcCYRjzp2ELJre4 2BJajMf4 3D reserved=0>:    >         driver = calloc(1  sizeof(*driver));         if ( driver)             goto out;             driver->dev = dev;    -    driver->config = *config;    you can save a pointer to config in the driver. We'll need it in the future. And we can use ctrl code in our BSD code    Let s add it when we really will need it  it s enough to have it on ctrl for now..        ________________________________    In tests/test_nvme_ctrl.cc<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F201 23discussion_r234449853 data=02 7C01 7Cvasilyf 40mellanox.com 7Cf59b0e5d761843642b3108d64d6c4863 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636781524794047464 sdata=JJK 2F5UUmNFPdi7nNnRb6EjhsMWO 2FB 2BOo8qG6lLEmT78 3D reserved=0>:    > @@ -1207 6 +1191 30 @@ TEST_P(NvmeControllerTestP  id_ctrl) {         EXPECT_EQ((1<<1)  id->cmic);     }        +TEST_P(NvmeControllerTestP  id_nslist) {    +    drv->ctrl_start();    +    +    nvme_cmd_identify_t *c = (nvme_cmd_identify_t *)    +                                drv->admin_command_prep(1  NVME_ADM_CMD_IDENTIFY);    +    +    c->cns  = cpu_to_le32(0x02);    please use this enum:    enum NvmeIdentifyCns {  NVME_IDENTIFY_CNS_NAMESPACE = 0x00   NVME_IDENTIFY_CNS_CTRL = 0x01   NVME_IDENTIFY_CNS_ACTIVE_NS_LIST = 0x02   NVME_IDENTIFY_CND_NS_ID_DESCRIPTOR_LIST = 0x03   };    from src/nvme.h.  also you can fix other place in different commit as well.      You are receiving this because you authored the thread.  Reply to this email directly  view it on GitHub<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F201 23pullrequestreview-176093172 data=02 7C01 7Cvasilyf 40mellanox.com 7Cf59b0e5d761843642b3108d64d6c4863 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636781524794047464 sdata=XhaXklzIPPfwwZZcPRGXJ7 2FkfMrk8gXbngMxnVcw6uQ 3D reserved=0>  or mute the thread<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2Fnotifications 2Funsubscribe-auth 2FAIVK2m5RdbAtn1vCv1Qn4TOYy3-XxtqUks5uwX-dgaJpZM4Ydscb data=02 7C01 7Cvasilyf 40mellanox.com 7Cf59b0e5d761843642b3108d64d6c4863 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636781524794057473 sdata=IADrH3ksgPgvLGkgnCHuijX 2B8epAvN7 2F4RWs2ZCERFM 3D reserved=0>.  
125,alex-mikheev,@vasilyMellanox please resolve conflicts   rebase.
126,swx-jenkins2,Can one of the admins verify this patch?
127,swx-jenkins2,Can one of the admins verify this patch?
128,alex-mikheev,@galshachaf  please review
129,swx-jenkins2,Can one of the admins verify this patch?
130,galshachaf,@alex-mikheev please see fixed version
131,vasilyMellanox,Updated..    From: Max Gurtovoy <notifications@github.com>  Sent: Thursday  November 29  2018 16:31  To: Mellanox/nvmx <nvmx@noreply.github.com>  Cc: Vasily Philipov <vasilyf@mellanox.com>; Mention <mention@noreply.github.com>  Subject: Re: [Mellanox/nvmx] Change config file structure ( 201)      @maxgurtovoy requested changes on this pull request.    please fix and update conversations    ________________________________    In samples/controller.c<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F201 23discussion_r237414466 data=02 7C01 7Cvasilyf 40mellanox.com 7C28740b2d576746a4557708d656073e0f 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636790986441104689 sdata=MvVY4h0IBb 2BkMtr1mwkmgXCmjncy6G2gSSS0y3pKWQM 3D reserved=0>:    > @@ -324 13 +324 14 @@ static int nvme_ctrl_id_init(nvme_ctrl_t *ctrl)             /* TODO: this is ugly  refactor          */    -    rc = nvme_json_get_value(config   ctrl.num_namespaces      -                              ctrl->num_namespaces);    +    rc = nvme_json_get_array_size(config   ctrl.namespaces );    ???    ________________________________    In samples/controller.c<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F201 23discussion_r237500309 data=02 7C01 7Cvasilyf 40mellanox.com 7C28740b2d576746a4557708d656073e0f 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636790986441104689 sdata=6tPl98Rsq8HTnJ 2BeznvZ 2BbpjMaiRhOksifNdMCUR 2BNQ 3D reserved=0>:    >      if (rc < 0) {    -        nvmx_error( failed to get max number of namespaces );    +        nvmx_error( failed to get namespaces array size );    +        return -1;    +    } else if ( rc) {    +        nvmx_error( at least one namespace should be defined );    can you use our standard NVMX_ERR ?    ________________________________    In src/json/mlnx_nvmx_config.json<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F201 23discussion_r237503481 data=02 7C01 7Cvasilyf 40mellanox.com 7C28740b2d576746a4557708d656073e0f 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636790986441104689 sdata=d2oFuDZ2SrXE1iqj28oKwU2R5Lmllu2QLtF4nRnFXuE 3D reserved=0>:    >    -             eps : [ {    +         nr_io_queues : 32     +         name : null     ??    ________________________________    In src/json/mlnx_nvmx_config.json<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F201 23discussion_r237504103 data=02 7C01 7Cvasilyf 40mellanox.com 7C28740b2d576746a4557708d656073e0f 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636790986441104689 sdata=3nHGnWcYfKV4BnMO8bGw3zbyi4Ks 2FKELyCnz17uusNg 3D reserved=0>:    >               */    -             type :  posix_io      +             backend : {    +                 id :  nvmftgt1      +                 nsid : 1  // we do not support nsid mapping yet    +                 format : {  block_order : 9   metadata : 0   integrity :  crc16  }    if metadata is 0  the integrity should be None    ________________________________    In src/json/msft_nvmx_config.json<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F201 23discussion_r237504198 data=02 7C01 7Cvasilyf 40mellanox.com 7C28740b2d576746a4557708d656073e0f 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636790986441104689 sdata=0RXunKbdVpXKFlpoEJv2ULK443QA2zS1ncI9wAYbnnM 3D reserved=0>:    > @@ -7 21 +7 27 @@              cqes : 68               mn :  Microsoft Azure NVMe Controller                sn :  MAZN12      -         num_namespaces  : 1  // Number of the namespaces    -         emu_type :  cc_remote    +         emu_type :  cc_remote      +         namespaces : [ {    +             nsid : 1     +             size_mb : 512     +             format : {  block_order : 9   metadata : 0 }     +             backend : {    +                 id :  nvmftgt1      +                 nsid : 1  // we do not support nsid mapping yet    +                 format : {  block_order : 9   metadata : 0   integrity :  crc16  }    here too    ________________________________    In src/nvme_emu_io_driver.c<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F201 23discussion_r237504903 data=02 7C01 7Cvasilyf 40mellanox.com 7C28740b2d576746a4557708d656073e0f 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636790986441104689 sdata=W6bitJ 2FFF6zycsS2nrGEnWJaDIqWgrJN4IFg3z0NNPo 3D reserved=0>:    > +        nvmx_error( at least one namespace should be defined );    +        return -1;    +    }    +    +    rc = nvme_json_get_value(config   ctrl.namespaces[0].size_mb    size_mb);    +    if (rc < 0 ||  size_mb) {    +        nvmx_error( Each namespace should define non-zero size_mb     +                    field into the config file );    +        return -1;    +    }    +    +    disk_size = 1024ULL * 1024ULL * size_mb;    +    +    rc = nvme_json_get_value(config   ctrl.namespaces[0].format.block_order      +                              disk_block_order);    +    if (rc < 0 || (9  = disk_block_order   12  = disk_block_order)) {    please change the order of comparison:  disk_block_order  = 9   disk_block_order  = 12    ________________________________    In src/nvme_emu_io_driver.c<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F201 23discussion_r237505032 data=02 7C01 7Cvasilyf 40mellanox.com 7C28740b2d576746a4557708d656073e0f 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636790986441104689 sdata=c1SH6dG01tiPS 2FBPPR14NcZ7KZBKGxgqFvmsiWDm 2BJc 3D reserved=0>:    > +    +    rc = nvme_json_get_value(config   ctrl.namespaces[0].size_mb    size_mb);    +    if (rc < 0 ||  size_mb) {    +        nvmx_error( Each namespace should define non-zero size_mb     +                    field into the config file );    +        return -1;    +    }    +    +    disk_size = 1024ULL * 1024ULL * size_mb;    +    +    rc = nvme_json_get_value(config   ctrl.namespaces[0].format.block_order      +                              disk_block_order);    +    if (rc < 0 || (9  = disk_block_order   12  = disk_block_order)) {    +        nvmx_error( Each namespace should define block_order field     +                    (values: 9 or 12) into the config file );    +        return rc;    NVMX_ERR    ________________________________    In src/nvme_emu_io_driver.c<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F201 23discussion_r237505323 data=02 7C01 7Cvasilyf 40mellanox.com 7C28740b2d576746a4557708d656073e0f 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636790986441104689 sdata=jlXOduvcP6R7R9fuhz5Wjnru7YduH9pxNVC653ghxYk 3D reserved=0>:    >         driver = calloc(1  sizeof(*driver));         if ( driver)             goto out;             driver->dev = dev;    -    driver->config = *config;    do you save the pointer ?    ________________________________    In src/nvme_emu_io_driver.c<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F201 23discussion_r237507610 data=02 7C01 7Cvasilyf 40mellanox.com 7C28740b2d576746a4557708d656073e0f 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636790986441104689 sdata=xE8tbD 2FgT21VBzzbgN2m0oXwjJiQAw6y7Z6Q4M2NMCw 3D reserved=0>:    >         if (strncmp(ep_type_str   posix_io   8) == 0) {             ep_fam = NVME_EMU_ADRFAM_NONE;             ep_type = NVME_EMU_EP_TYPE_POSIX;        -        rc = nvme_json_get_value( driver->config     -                                  backend_driver.endpoints.name   name);    -        if (rc < 0)    +        snprintf(param  sizeof(param)   s.name   backend);    +        rc = nvme_json_get_value(config  param  name);    +        if (rc < 0 ||  NVME_EMU_QUEUE_DB == q_type) {    I don't understand this strange logic. The code should not be adjusted to tests.    ________________________________    In src/nvme_emu_io_driver.h<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F201 23discussion_r237507905 data=02 7C01 7Cvasilyf 40mellanox.com 7C28740b2d576746a4557708d656073e0f 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636790986441104689 sdata=L5wfHO0GosskfpdaHCgBWS 2Fo885wM6x6V08EkicHYps 3D reserved=0>:    >  };         void nvme_driver_destroy(nvme_emu_driver_t *driver);    -nvme_emu_driver_t *nvme_driver_create(char *type     +nvme_emu_driver_t *nvme_driver_create(int q_type     need to check if we really need q_type      You are receiving this because you were mentioned.  Reply to this email directly  view it on GitHub<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F201 23pullrequestreview-179704198 data=02 7C01 7Cvasilyf 40mellanox.com 7C28740b2d576746a4557708d656073e0f 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636790986441104689 sdata=3GzseGKIlQQ0v7Mv0ltDR9whZkEFgHCW3AUwfZAzO1M 3D reserved=0>  or mute the thread<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2Fnotifications 2Funsubscribe-auth 2FAIVK2oTBg1yJpdvo1N8mRv1NkNxhy8Tbks5uz--QgaJpZM4Ydscb data=02 7C01 7Cvasilyf 40mellanox.com 7C28740b2d576746a4557708d656073e0f 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636790986441104689 sdata=kgkMs 2BMh7bghRYG6YcOHYK4fKzIByGABS2XtrSniBgk 3D reserved=0>.  
132,swx-jenkins2,Can one of the admins verify this patch?
133,alex-mikheev,@vasilyMellanox can you please take a look
134,nitzancarmi,I will resend new rebased one instead
135,swx-jenkins2,Can one of the admins verify this patch?
136,swx-jenkins2,Can one of the admins verify this patch?
137,swx-jenkins2,Can one of the admins verify this patch?
138,swx-jenkins2,Can one of the admins verify this patch?
139,vasilyMellanox,update    From: Alex Mikheev <notifications@github.com>  Sent: Tuesday  December 04  2018 13:34  To: Mellanox/nvmx <nvmx@noreply.github.com>  Cc: Vasily Philipov <vasilyf@mellanox.com>; Author <author@noreply.github.com>  Subject: Re: [Mellanox/nvmx] Json hex ( 236)      @alex-mikheev requested changes on this pull request.    please add a couple of hex number testcases  and modify unit_tests.json      You are receiving this because you authored the thread.  Reply to this email directly  view it on GitHub<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F236 23pullrequestreview-181210717 data=02 7C01 7Cvasilyf 40mellanox.com 7Cbbb22cb570e34fc86d3708d659dc52ba 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636795200140592083 sdata=2qrPi8eLJuQfI66IwMvyDa8gVRkSlh6 2BsywSzrydQgw 3D reserved=0>  or mute the thread<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2Fnotifications 2Funsubscribe-auth 2FAIVK2jQlLr50G0Fpoc0XaGu4v1LjSEkiks5u1l2MgaJpZM4ZAdSh data=02 7C01 7Cvasilyf 40mellanox.com 7Cbbb22cb570e34fc86d3708d659dc52ba 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636795200140602088 sdata=6dl8nNTBYAlCM5DA8wnCsAsYBVTNBhCDALsin6313xw 3D reserved=0>.  
140,swx-jenkins2,Can one of the admins verify this patch?
141,nitzancarmi,Not really necessary anymore
142,nitzancarmi,Will be pushed later in config file format
143,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/485/artifact/cov_build/html/index.html
144,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/487/artifact/cov_build/html/index.html
145,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/486/artifact/cov_build/html/index.html
146,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/488/artifact/cov_build/html/index.html
147,vasilyMellanox,updated    From: Max Gurtovoy <notifications@github.com>  Sent: Wednesday  December 05  2018 16:04  To: Mellanox/nvmx <nvmx@noreply.github.com>  Cc: Vasily Philipov <vasilyf@mellanox.com>; Mention <mention@noreply.github.com>  Subject: Re: [Mellanox/nvmx] Change config file structure ( 201)      @maxgurtovoy requested changes on this pull request.    Please fix error flows to all nvme_json_get_value stuff.  I can break the code and make nvmx to stuck with 1 wrong line in the json file.  Also I can t find error flow in nvme_ctrl_init function for free(ctrl->config.values);      You are receiving this because you were mentioned.  Reply to this email directly  view it on GitHub<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F201 23pullrequestreview-181769648 data=02 7C01 7Cvasilyf 40mellanox.com 7Cfa9180cc81d54d415f8f08d65aba7cd7 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636796154334121238 sdata=sn9MKBjfAtJ667epZElwKYBlBke1xhzlXNNcQKRK58U 3D reserved=0>  or mute the thread<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2Fnotifications 2Funsubscribe-auth 2FAIVK2oUR4d_SigFCKtiZGMDfK6pw9w2rks5u19JHgaJpZM4Ydscb data=02 7C01 7Cvasilyf 40mellanox.com 7Cfa9180cc81d54d415f8f08d65aba7cd7 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636796154334121238 sdata=WlADNcHcTLWpKqQuEX9T1M9ijCkNJyyUiP4e1AM6lOE 3D reserved=0>.  
148,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/489/artifact/cov_build/html/index.html
149,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/490/artifact/cov_build/html/index.html
150,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/491/artifact/cov_build/html/index.html
151,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/492/artifact/cov_build/html/index.html
152,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/493/artifact/cov_build/html/index.html
153,nitzancarmi,Will push this alogside with HA patches  in the new config file structure
154,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/494/artifact/cov_build/html/index.html
155,nitzancarmi,@galshachaf Could you please verify that this solved the issue?
156,swx-jenkins2,Can one of the admins verify this patch?
157,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/495/artifact/cov_build/html/index.html
158,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/496/artifact/cov_build/html/index.html
159,nitzancarmi,@alex-mikheev  @maxgurtovoy  could you please approve and merge? 10x 
160,galshachaf,@alex-mikheev @maxgurtovoy can we merge this?
161,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/497/artifact/cov_build/html/index.html
162,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/498/artifact/cov_build/html/index.html
163,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/499/artifact/cov_build/html/index.html
164,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/500/artifact/cov_build/html/index.html
165,swx-jenkins2,Can one of the admins verify this patch?
166,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/501/artifact/cov_build/html/index.html
167,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/502/artifact/cov_build/html/index.html
168,mlx3im,bot:retest
169,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/503/artifact/cov_build/html/index.html
170,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/504/artifact/cov_build/html/index.html
171,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/505/artifact/cov_build/html/index.html
172,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/506/artifact/cov_build/html/index.html
173,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/507/artifact/cov_build/html/index.html
174,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/508/artifact/cov_build/html/index.html
175,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/509/artifact/cov_build/html/index.html
176,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/510/artifact/cov_build/html/index.html
177,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/511/artifact/cov_build/html/index.html
178,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/512/artifact/cov_build/html/index.html
179,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/513/artifact/cov_build/html/index.html
180,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/514/artifact/cov_build/html/index.html
181,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/515/artifact/cov_build/html/index.html
182,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/516/artifact/cov_build/html/index.html
183,swx-jenkins2,Can one of the admins verify this patch?
184,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/517/artifact/cov_build/html/index.html
185,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/518/artifact/cov_build/html/index.html
186,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/519/artifact/cov_build/html/index.html
187,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/520/artifact/cov_build/html/index.html
188,swx-jenkins2,Can one of the admins verify this patch?
189,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/521/artifact/cov_build/html/index.html
190,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/522/artifact/cov_build/html/index.html
191,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/523/artifact/cov_build/html/index.html
192,swx-jenkins2,Can one of the admins verify this patch?
193,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/524/artifact/cov_build/html/index.html
194,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/525/artifact/cov_build/html/index.html
195,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/526/artifact/cov_build/html/index.html
196,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/527/artifact/cov_build/html/index.html
197,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/528/artifact/cov_build/html/index.html
198,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/529/artifact/cov_build/html/index.html
199,swx-jenkins2,Can one of the admins verify this patch?
200,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/530/artifact/cov_build/html/index.html
201,hellerguyh,fixed the issue
202,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/531/artifact/cov_build/html/index.html
203,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/532/artifact/cov_build/html/index.html
204,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/533/artifact/cov_build/html/index.html
205,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/534/artifact/cov_build/html/index.html
206,galshachaf,redundant with https://github.com/Mellanox/nvmx/pull/261
207,alex-mikheev,@aviadye please also take a look
208,galshachaf,tests are not relevant  DB mode not implemented in FW and not planned.
209,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/535/artifact/cov_build/html/index.html
210,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/536/artifact/cov_build/html/index.html
211,swx-jenkins2,Can one of the admins verify this patch?
212,hellerguyh,not sure regarding the   ``rdma_req->nvmf_req.timed_out = true;``  please advise
213,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/537/artifact/cov_build/html/index.html
214,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/538/artifact/cov_build/html/index.html
215,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/539/artifact/cov_build/html/index.html
216,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/540/artifact/cov_build/html/index.html
217,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/541/artifact/cov_build/html/index.html
218,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/542/artifact/cov_build/html/index.html
219,swx-jenkins2,Can one of the admins verify this patch?
220,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/543/artifact/cov_build/html/index.html
221,swx-jenkins2,Can one of the admins verify this patch?
222,swx-jenkins2,Can one of the admins verify this patch?
223,swx-jenkins2,Can one of the admins verify this patch?
224,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/544/artifact/cov_build/html/index.html
225,nitzancarmi,Pushed after max's changes.  @alex-mikheev @aviadye I think this commit is really important to be merged and enter next release.  Today we just don't catch that kind of configuration problems.
226,nitzancarmi,Pushed with Max's changes.  Pay attention that I added additional commit this time  to fix another place when we relied on hard coded nsid=1.
227,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/545/artifact/cov_build/html/index.html
228,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/546/artifact/cov_build/html/index.html
229,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/548/artifact/cov_build/html/index.html
230,swx-jenkins2,Can one of the admins verify this patch?
231,swx-jenkins2,Can one of the admins verify this patch?
232,nitzancarmi,Pushed as part of another PR
233,swx-jenkins2,Can one of the admins verify this patch?
234,mlx3im,bot:retest
235,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-test/137/artifact/cov_build/html/index.html
236,mlx3im,please  ignore this message  bot:retest
237,mlx3im,please  ignore this message  bot:retest
238,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/559/artifact/cov_build/html/index.html
239,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/560/artifact/cov_build/html/index.html
240,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/561/artifact/cov_build/html/index.html
241,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/562/artifact/cov_build/html/index.html
242,vasilyMellanox,fixed    From: Alex Mikheev <notifications@github.com>  Sent: Wednesday  December 26  2018 13:25  To: Mellanox/nvmx <nvmx@noreply.github.com>  Cc: Vasily Philipov <vasilyf@mellanox.com>; Author <author@noreply.github.com>  Subject: Re: [Mellanox/nvmx] More coverity fixes ( 280)      @alex-mikheev requested changes on this pull request.    ________________________________    In src/nvmf_rdma.c<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F280 23discussion_r243976938 data=02 7C01 7Cvasilyf 40mellanox.com 7C2a25c0246c9943bfd2d708d66b24c42e 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636814202982360733 sdata=zA 2FywhNfzhNWueIk6pmrYAdzAjHtKakspBfbDpPU 2F2E 3D reserved=0>:    > @@ -356 6 +356 7 @@ static int nvmf_rdma_alloc_rsps(nvmf_rdma_qpair_t *queue)             if ( rdma_rsp->cqe_mr) {                 nvmx_error( Failed to reg cqe_mr for cqe  d queue 0x p   i                             queue);    +            ret = EINVAL;    should be -EINVAL. We try to use -errno convention. Please grep   fix    ________________________________    In src/nvme_mlx_emu_glue.c<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F280 23discussion_r243977348 data=02 7C01 7Cvasilyf 40mellanox.com 7C2a25c0246c9943bfd2d708d66b24c42e 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636814202982360733 sdata=BasbaAg5onHdOBD5siw 2FMOOSc9T1q3E9opLimczlL 2BA 3D reserved=0>:    > @@ -339 7 +340 7 @@ int nvme_emu_create_sq(nvme_emu_driver_t *driver              attr.log_page_size = 0;             attr.max_transcation_size = 1;             attr.num_of_namespaces = 1; // use 1 ns for now    -        attr.nvme_namespace_l = calloc(1  sizeof(attr.nvme_namespace_l));    +        attr.nvme_namespace_l = calloc(1  sizeof(*attr.nvme_namespace_l));    nice catch      You are receiving this because you authored the thread.  Reply to this email directly  view it on GitHub<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F280 23pullrequestreview-187872036 data=02 7C01 7Cvasilyf 40mellanox.com 7C2a25c0246c9943bfd2d708d66b24c42e 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636814202982360733 sdata=vq7DUFUYhz0Uq8Tc54rDzRHT7aVLBk29ZPTCJCRt1wo 3D reserved=0>  or mute the thread<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2Fnotifications 2Funsubscribe-auth 2FAIVK2uTcmfq1js5ETZBR4X0yL0NZ8y_uks5u81yIgaJpZM4Zhefz data=02 7C01 7Cvasilyf 40mellanox.com 7C2a25c0246c9943bfd2d708d66b24c42e 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636814202982360733 sdata=WfV7Uz7OuCbarJMYecUf2DFqrIxyfRIrWDkJE9eQstU 3D reserved=0>.  
243,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/563/artifact/cov_build/html/index.html
244,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/564/artifact/cov_build/html/index.html
245,swx-jenkins2,Can one of the admins verify this patch?
246,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/565/artifact/cov_build/html/index.html
247,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/566/artifact/cov_build/html/index.html
248,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/567/artifact/cov_build/html/index.html
249,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/568/artifact/cov_build/html/index.html
250,vasilyMellanox,    From: Max Gurtovoy <notifications@github.com>  Sent: Sunday  December 30  2018 11:00  To: Mellanox/nvmx <nvmx@noreply.github.com>  Cc: Vasily Philipov <vasilyf@mellanox.com>; Author <author@noreply.github.com>  Subject: Re: [Mellanox/nvmx] Ns support ( 255)      @maxgurtovoy requested changes on this pull request.    We have a new policy now for each commit that we push  we add a Tests section of running tests before we start review.  E.g    nvmf: added nsid support    bla bla bla...    Tests:      *   snic test suite.    *   fio verify vs. remote target (bs: 4k  8k  16k  32k  devices: nvme0n1 nvme0n2)    *   new gtest: read/write prp    *   fio stress (bs=4k  q_depth=128  runtime=120)    ________________________________    In tests/test_nvmf_ctrl.cc<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F255 23discussion_r244527619 data=02 7C01 7Cvasilyf 40mellanox.com 7C61a43bb2fa4b4eeb844608d66e353eb2 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636817572298308566 sdata=yb2kIxQV6y53YgcPlIHeQKhR8TsHNBKOxxfNTyMoSag 3D reserved=0>:    > +    +            for (int i = 0; i < MAX_PRP_LIST_ENTRIES; i++) {    +                rc = posix_memalign((void **) prp_list[i]  PRP_PAGE_SIZE  PRP_PAGE_SIZE);    +                EXPECT_EQ(0  rc);    +    +                prp_zero(prp_list[i]);    +                prp_zero(data_buf + i * PRP_PAGE_SIZE);    +            }    +        }    +    +        static void prp_zero(void *prp) {    +            memset(prp  0  PRP_PAGE_SIZE);    +        }    +    +        void free_prp() {    +            free(prp1);    For good practice  please destroy in the opposite order of allocation.    ________________________________    In tests/test_nvmf_ctrl.cc<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F255 23discussion_r244527683 data=02 7C01 7Cvasilyf 40mellanox.com 7C61a43bb2fa4b4eeb844608d66e353eb2 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636817572298318575 sdata=4M5I6EtzX6aiO3pgAcgpNs2yo 2Fk6ncF7pvNaZ8YhTMk 3D reserved=0>:    > +                EXPECT_EQ(0  rc);    +    +                prp_zero(prp_list[i]);    +                prp_zero(data_buf + i * PRP_PAGE_SIZE);    +            }    +        }    +    +        static void prp_zero(void *prp) {    +            memset(prp  0  PRP_PAGE_SIZE);    +        }    +    +        void free_prp() {    +            free(prp1);    +            free(prp2);    +    +            for (int i = 0; i < MAX_PRP_LIST_ENTRIES; i++) {    for one liner loop no need to use {}    common in gtests   pls look other tests of other peoples    ________________________________    In tests/test_nvmf_ctrl.cc<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F255 23discussion_r244527710 data=02 7C01 7Cvasilyf 40mellanox.com 7C61a43bb2fa4b4eeb844608d66e353eb2 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636817572298318575 sdata=6YAM 2BlfIwAm2OsyekEjIbWQKsX7TN 2BKuvTvOf8s6uhY 3D reserved=0>:    >         nvme_driver_destroy(driver);     }        +TEST_F(NvmfControllerTest  ns_read_write) {    +    if (skip) {    one line condition.  also please don't put logic before local params - code conventions.    It s like another test defines it    ________________________________    In tests/test_nvmf_ctrl.cc<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F255 23discussion_r244527722 data=02 7C01 7Cvasilyf 40mellanox.com 7C61a43bb2fa4b4eeb844608d66e353eb2 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636817572298328580 sdata=hTkJjgsjQMMmbRUma16SwfPCFgE 2FFoR5fgWMe 2FHjk4w 3D reserved=0>:    >         nvme_driver_destroy(driver);     }        +TEST_F(NvmfControllerTest  ns_read_write) {    +    if (skip) {    +        SKIP_TEST_R(skip_reason.c_str());    +    }    +    +    int rc  num_ns;    +    +    /* Get the number of namespaces */    +    num_ns = nvme_json_get_array_size( config   ctrl.namespaces );    +    EXPECT_TRUE(num_ns > 0);    +    +    rc = create_standalone_io_driver( ctrl   ep   driver);    +    EXPECT_EQ(0  rc);    +    +    nvme_cmd_rw_t rw;    same here. please put above.    If you mean  nvme_cmd_rw_t rw;    it s C++  better to define where u actually use it    ________________________________    In tests/test_nvmf_ctrl.cc<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F255 23discussion_r244527806 data=02 7C01 7Cvasilyf 40mellanox.com 7C61a43bb2fa4b4eeb844608d66e353eb2 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636817572298338585 sdata= 2Bo02MVOn1y6f 2FDjzWkBH2nc8vkDD 2Fi 2FI8Qu8ffsL1rU 3D reserved=0>:    > +    EXPECT_TRUE(num_ns > 0);    +    +    rc = create_standalone_io_driver( ctrl   ep   driver);    +    EXPECT_EQ(0  rc);    +    +    nvme_cmd_rw_t rw;    +    +    rw.dptr.prp1 = (uintptr_t) cpu_to_le64(prp1);    +    rw.dptr.prp2 = (uintptr_t) cpu_to_le64(prp2);    +    +    rw.slba = cpu_to_le64(0xff);    +    rw.nlb = (8 - 1);    +    +    nvme_queue_ops_t ops;    +    +    ops.qid = 1;    can you combine a test to check the qid feature (instead of using 1 qid ?  for (ns) {  for (qid) {  read_write()  }  }    ________________________________    In tests/test_nvmf_ctrl.cc<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F255 23discussion_r244527834 data=02 7C01 7Cvasilyf 40mellanox.com 7C61a43bb2fa4b4eeb844608d66e353eb2 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636817572298338585 sdata=lPI2JRxYMIH1Ad3Qw9uRSfVnIPdYp9p5hos0Ztvg 2BLM 3D reserved=0>:    >          }                 virtual void TearDown() {                 if ( skip) {                     sprintf(bash_cmd   sudo  s/config_nvmf_target.py -c -r   NVMX_SCRIPTS_DIR);                     cmd_run(bash_cmd);                     nvme_json_config_close( config);    +                free_prp();    +            }    +        }    +    +        void dma_init() {    +            memset( bar  0  sizeof(bar));    +    +            bar.vs  = 0x10200;    what are these hard coded values mean ?  I guess define is needed.    There is no define   I ve just copied it from NVMe tests - like AlexM uses it there          You are receiving this because you authored the thread.  Reply to this email directly  view it on GitHub<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F255 23pullrequestreview-188432197 data=02 7C01 7Cvasilyf 40mellanox.com 7C61a43bb2fa4b4eeb844608d66e353eb2 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636817572298348603 sdata= 2F8VG37oZWMTE7u5yEmbk9euIqS07Mit0OwM5iXM3COM 3D reserved=0>  or mute the thread<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2Fnotifications 2Funsubscribe-auth 2FAIVK2iXFUwPiK_JPR2h1i2XJhjSorcQ5ks5u-ICrgaJpZM4ZPAHk data=02 7C01 7Cvasilyf 40mellanox.com 7C61a43bb2fa4b4eeb844608d66e353eb2 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636817572298348603 sdata=j7I8ZDI3Npm9FYlBUWGwz4agHqqBCkkIxqXSguwiZvE 3D reserved=0>.  
251,swx-jenkins2,Can one of the admins verify this patch?
252,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/569/artifact/cov_build/html/index.html
253,vasilyMellanox,updated    From: Max Gurtovoy <notifications@github.com>  Sent: Sunday  December 30  2018 11:00  To: Mellanox/nvmx <nvmx@noreply.github.com>  Cc: Vasily Philipov <vasilyf@mellanox.com>; Author <author@noreply.github.com>  Subject: Re: [Mellanox/nvmx] Ns support ( 255)      @maxgurtovoy requested changes on this pull request.    We have a new policy now for each commit that we push  we add a Tests section of running tests before we start review.  E.g    nvmf: added nsid support    bla bla bla...    Tests:      *   snic test suite.    *   fio verify vs. remote target (bs: 4k  8k  16k  32k  devices: nvme0n1 nvme0n2)    *   new gtest: read/write prp    *   fio stress (bs=4k  q_depth=128  runtime=120)    ________________________________    In tests/test_nvmf_ctrl.cc<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F255 23discussion_r244527619 data=02 7C01 7Cvasilyf 40mellanox.com 7C61a43bb2fa4b4eeb844608d66e353eb2 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636817572298308566 sdata=yb2kIxQV6y53YgcPlIHeQKhR8TsHNBKOxxfNTyMoSag 3D reserved=0>:    > +    +            for (int i = 0; i < MAX_PRP_LIST_ENTRIES; i++) {    +                rc = posix_memalign((void **) prp_list[i]  PRP_PAGE_SIZE  PRP_PAGE_SIZE);    +                EXPECT_EQ(0  rc);    +    +                prp_zero(prp_list[i]);    +                prp_zero(data_buf + i * PRP_PAGE_SIZE);    +            }    +        }    +    +        static void prp_zero(void *prp) {    +            memset(prp  0  PRP_PAGE_SIZE);    +        }    +    +        void free_prp() {    +            free(prp1);    For good practice  please destroy in the opposite order of allocation.    ________________________________    In tests/test_nvmf_ctrl.cc<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F255 23discussion_r244527683 data=02 7C01 7Cvasilyf 40mellanox.com 7C61a43bb2fa4b4eeb844608d66e353eb2 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636817572298318575 sdata=4M5I6EtzX6aiO3pgAcgpNs2yo 2Fk6ncF7pvNaZ8YhTMk 3D reserved=0>:    > +                EXPECT_EQ(0  rc);    +    +                prp_zero(prp_list[i]);    +                prp_zero(data_buf + i * PRP_PAGE_SIZE);    +            }    +        }    +    +        static void prp_zero(void *prp) {    +            memset(prp  0  PRP_PAGE_SIZE);    +        }    +    +        void free_prp() {    +            free(prp1);    +            free(prp2);    +    +            for (int i = 0; i < MAX_PRP_LIST_ENTRIES; i++) {    for one liner loop no need to use {}    ________________________________    In tests/test_nvmf_ctrl.cc<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F255 23discussion_r244527710 data=02 7C01 7Cvasilyf 40mellanox.com 7C61a43bb2fa4b4eeb844608d66e353eb2 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636817572298318575 sdata=6YAM 2BlfIwAm2OsyekEjIbWQKsX7TN 2BKuvTvOf8s6uhY 3D reserved=0>:    >         nvme_driver_destroy(driver);     }        +TEST_F(NvmfControllerTest  ns_read_write) {    +    if (skip) {    one line condition.  also please don't put logic before local params - code conventions.    ________________________________    In tests/test_nvmf_ctrl.cc<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F255 23discussion_r244527722 data=02 7C01 7Cvasilyf 40mellanox.com 7C61a43bb2fa4b4eeb844608d66e353eb2 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636817572298328580 sdata=hTkJjgsjQMMmbRUma16SwfPCFgE 2FFoR5fgWMe 2FHjk4w 3D reserved=0>:    >         nvme_driver_destroy(driver);     }        +TEST_F(NvmfControllerTest  ns_read_write) {    +    if (skip) {    +        SKIP_TEST_R(skip_reason.c_str());    +    }    +    +    int rc  num_ns;    +    +    /* Get the number of namespaces */    +    num_ns = nvme_json_get_array_size( config   ctrl.namespaces );    +    EXPECT_TRUE(num_ns > 0);    +    +    rc = create_standalone_io_driver( ctrl   ep   driver);    +    EXPECT_EQ(0  rc);    +    +    nvme_cmd_rw_t rw;    same here. please put above.    ________________________________    In tests/test_nvmf_ctrl.cc<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F255 23discussion_r244527806 data=02 7C01 7Cvasilyf 40mellanox.com 7C61a43bb2fa4b4eeb844608d66e353eb2 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636817572298338585 sdata= 2Bo02MVOn1y6f 2FDjzWkBH2nc8vkDD 2Fi 2FI8Qu8ffsL1rU 3D reserved=0>:    > +    EXPECT_TRUE(num_ns > 0);    +    +    rc = create_standalone_io_driver( ctrl   ep   driver);    +    EXPECT_EQ(0  rc);    +    +    nvme_cmd_rw_t rw;    +    +    rw.dptr.prp1 = (uintptr_t) cpu_to_le64(prp1);    +    rw.dptr.prp2 = (uintptr_t) cpu_to_le64(prp2);    +    +    rw.slba = cpu_to_le64(0xff);    +    rw.nlb = (8 - 1);    +    +    nvme_queue_ops_t ops;    +    +    ops.qid = 1;    can you combine a test to check the qid feature (instead of using 1 qid ?  for (ns) {  for (qid) {  read_write()  }  }    ________________________________    In tests/test_nvmf_ctrl.cc<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F255 23discussion_r244527834 data=02 7C01 7Cvasilyf 40mellanox.com 7C61a43bb2fa4b4eeb844608d66e353eb2 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636817572298338585 sdata=lPI2JRxYMIH1Ad3Qw9uRSfVnIPdYp9p5hos0Ztvg 2BLM 3D reserved=0>:    >          }                 virtual void TearDown() {                 if ( skip) {                     sprintf(bash_cmd   sudo  s/config_nvmf_target.py -c -r   NVMX_SCRIPTS_DIR);                     cmd_run(bash_cmd);                     nvme_json_config_close( config);    +                free_prp();    +            }    +        }    +    +        void dma_init() {    +            memset( bar  0  sizeof(bar));    +    +            bar.vs  = 0x10200;    what are these hard coded values mean ?  I guess define is needed.      You are receiving this because you authored the thread.  Reply to this email directly  view it on GitHub<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F255 23pullrequestreview-188432197 data=02 7C01 7Cvasilyf 40mellanox.com 7C61a43bb2fa4b4eeb844608d66e353eb2 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636817572298348603 sdata= 2F8VG37oZWMTE7u5yEmbk9euIqS07Mit0OwM5iXM3COM 3D reserved=0>  or mute the thread<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2Fnotifications 2Funsubscribe-auth 2FAIVK2iXFUwPiK_JPR2h1i2XJhjSorcQ5ks5u-ICrgaJpZM4ZPAHk data=02 7C01 7Cvasilyf 40mellanox.com 7C61a43bb2fa4b4eeb844608d66e353eb2 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636817572298348603 sdata=j7I8ZDI3Npm9FYlBUWGwz4agHqqBCkkIxqXSguwiZvE 3D reserved=0>.  
254,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/570/artifact/cov_build/html/index.html
255,nitzancarmi,Will be re-pushed as part of the  nvme-Werror  flag support
256,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/571/artifact/cov_build/html/index.html
257,vasilyMellanox,fixed    From: Max Gurtovoy <notifications@github.com>  Sent: Sunday  December 30  2018 17:40  To: Mellanox/nvmx <nvmx@noreply.github.com>  Cc: Vasily Philipov <vasilyf@mellanox.com>; Author <author@noreply.github.com>  Subject: Re: [Mellanox/nvmx] Ns support ( 255)      @maxgurtovoy requested changes on this pull request.    please resolve all the previous comments or at least comment on them.    ________________________________    In tests/test_nvmf_ctrl.cc<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F255 23discussion_r244538112 data=02 7C01 7Cvasilyf 40mellanox.com 7C411efa26558644dbbeb208d66e6d01c6 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636817811790739484 sdata=h7XJit8QePuTdL 2BmpJRjB6qEmjJiH1sdRfhT39ePKJQ 3D reserved=0>:    > +    EXPECT_EQ(0  rc);    +    +    nvme_cmd_rw_t rw;    +    +    rw.dptr.prp1 = (uintptr_t) cpu_to_le64(prp1);    +    rw.dptr.prp2 = (uintptr_t) cpu_to_le64(prp2);    +    +    rw.slba = cpu_to_le64(0xff);    +    rw.nlb = (8 - 1);    +    +    nvme_queue_ops_t ops;    +    int num_queues = driver->num_queues;    +    +    for (int i = 0; i < num_ns; ++i) {    +        char c = (i + 1)   9;    +        rw.nsid = cpu_to_le32(i + 1);    you shouldn't use sequential nsid anymore. We support now also non-seq namespaces. So need to take the nsid in ns_arr[i]      You are receiving this because you authored the thread.  Reply to this email directly  view it on GitHub<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F255 23pullrequestreview-188443358 data=02 7C01 7Cvasilyf 40mellanox.com 7C411efa26558644dbbeb208d66e6d01c6 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636817811790749489 sdata=tRBBiTwjLucpeXs 2Ff6s0L5fqEHsOaSF5CrD4iHClnhE 3D reserved=0>  or mute the thread<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2Fnotifications 2Funsubscribe-auth 2FAIVK2nNhIjRiqm1pN4WMIZzaXfxAb1Ixks5u-N43gaJpZM4ZPAHk data=02 7C01 7Cvasilyf 40mellanox.com 7C411efa26558644dbbeb208d66e6d01c6 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636817811790749489 sdata=W 2FZCdo7o6 2Ff 2F7QJmsyVI3zqqu4tahWggLEp74gaI2LQ 3D reserved=0>.  
258,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/572/artifact/cov_build/html/index.html
259,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/573/artifact/cov_build/html/index.html
260,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/574/artifact/cov_build/html/index.html
261,nitzancarmi,@alex-mikheev  fixed to return error
262,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/575/artifact/cov_build/html/index.html
263,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/576/artifact/cov_build/html/index.html
264,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/577/artifact/cov_build/html/index.html
265,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/578/artifact/cov_build/html/index.html
266,nitzancarmi,Will be attached to another patch series
267,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/579/artifact/cov_build/html/index.html
268,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/580/artifact/cov_build/html/index.html
269,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/581/artifact/cov_build/html/index.html
270,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/582/artifact/cov_build/html/index.html
271,alex-mikheev,Tests passing  - VM: linux pxe install on nvme disk  - VM: uefi linux boot  - nvme_reset.sh  nvme_basic_sanity.sh  - run_tests_on_snic.sh  - gtest_nvme_ctlr instance 0 (emulation)  - gtest_nvme_ctrl instance 1 (unvme  including flr tests)  
272,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/583/artifact/cov_build/html/index.html
273,nitzancarmi,@maxgurtovoy @alex-mikheev @aviadye   I still need to check this against various targets (especially E8 and Accelero) but it worked well for me against linux kernel target (with multiple namespaces configured).  This can be pretty useful when using non-sequential nsids (so you won't need to be E8 expert to figure out how to write config file)    Meanwhile please go over the script  and see if you have any comments.  I hope this is would be the foundation for auto-generating config files. 
274,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/584/artifact/cov_build/html/index.html
275,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/585/artifact/cov_build/html/index.html
276,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/586/artifact/cov_build/html/index.html
277,alex-mikheev,Obsoleted by https://github.com/Mellanox/nvmx/pull/282
278,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/588/artifact/cov_build/html/index.html
279,nitzancarmi,There is a new PR rebased over latest code (https://github.com/Mellanox/nvmx/pull/291)
280,nitzancarmi,Older PR version is here: https://github.com/Mellanox/nvmx/pull/245    Note that most comments are no longer relevant after rebase
281,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/589/artifact/cov_build/html/index.html
282,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/590/artifact/cov_build/html/index.html
283,vasilyMellanox,Update  pls look at AlexM comments for you    From: Max Gurtovoy <notifications@github.com>  Sent: Thursday  January 03  2019 00:50  To: Mellanox/nvmx <nvmx@noreply.github.com>  Cc: Vasily Philipov <vasilyf@mellanox.com>; Author <author@noreply.github.com>  Subject: Re: [Mellanox/nvmx] Ns support ( 255)      @maxgurtovoy requested changes on this pull request.    Please make sure to read and response to all comments. Make sure you're compliant with our  Tests  policy for each commit. You must mention which tests were run before last submission. Otherwise  the reviewer will not approve the commit.    ________________________________    In tests/test_nvmf_ctrl.cc<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F255 23discussion_r244871013 data=02 7C01 7Cvasilyf 40mellanox.com 7Caa484548e6ef4daecc0808d67104a430 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636820662086242057 sdata=39dS6CRkg2hEvlrGSaiUpnezpS4rgobFM 2FyPv6N 2B 2BrM 3D reserved=0>:    > @@ -32 6 +34 15 @@ class NvmfControllerTest : public ::testing::Test {             nvme_emu_ep_t *ep = NULL;             nvmf_emu_ctrl_t *ctrl = NULL;        +        nvme_bar_t bar;    +        char *prp1  *prp2;    +    +        static const int PRP_OFFSET_BITS = 12;    +        static const int PRP_PAGE_SIZE = 1 << PRP_OFFSET_BITS;    +    +        char *prp_list[MAX_PRP_LIST_ENTRIES];    +        char data_buf[(MAX_PRP_LIST_ENTRIES + 1) * PRP_PAGE_SIZE];    where do you actually use data_buf ?    ________________________________    In tests/test_nvmf_ctrl.cc<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F255 23discussion_r244871273 data=02 7C01 7Cvasilyf 40mellanox.com 7Caa484548e6ef4daecc0808d67104a430 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636820662086252058 sdata=Kqh1koGWKtxotH2pd2J242qmmtDQaGbA4ZHhHtaBaAg 3D reserved=0>:    > @@ -32 6 +34 15 @@ class NvmfControllerTest : public ::testing::Test {             nvme_emu_ep_t *ep = NULL;             nvmf_emu_ctrl_t *ctrl = NULL;        +        nvme_bar_t bar;    +        char *prp1  *prp2;    +    +        static const int PRP_OFFSET_BITS = 12;    +        static const int PRP_PAGE_SIZE = 1 << PRP_OFFSET_BITS;    +    +        char *prp_list[MAX_PRP_LIST_ENTRIES];    where do you use prp_list ?    ________________________________    In tests/test_nvmf_ctrl.cc<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F255 23discussion_r244871953 data=02 7C01 7Cvasilyf 40mellanox.com 7Caa484548e6ef4daecc0808d67104a430 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636820662086252058 sdata=WUmJ3G9M 2FsKHUmUjQAkr8s 2FXQed1hntpokDa7dtkviY 3D reserved=0>:    > +        char c = (i + 1)   9;    +        rw.nsid = cpu_to_le32(ctrl->ns[i].id);    +    +        for (int j = 1; j < num_queues; ++j) {    +            ops.qid = j;    +            constant_fill((void *) prp1  c  NvmfControllerTest::PRP_PAGE_SIZE);    +    +            rc = ep->ep_ops.write_prp(ep   ops   rw  PRP_OFFSET_BITS);    +            EXPECT_EQ(NVMX_OK  rc);    +    +            prp_zero(prp1);    +            rc = ep->ep_ops.read_prp(ep   ops   rw  PRP_OFFSET_BITS);    +            EXPECT_EQ(NVMX_OK  rc);    +    +            EXPECT_EQ(c  prp1[0]);    +            EXPECT_EQ(c  prp1[255]);    don't you want to check the whole prp1 you wrote ?    ________________________________    In tests/test_nvmf_ctrl.cc<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F255 23discussion_r244874893 data=02 7C01 7Cvasilyf 40mellanox.com 7Caa484548e6ef4daecc0808d67104a430 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636820662086262067 sdata=MQ 2FTXGQc9Fa 2FxcXjuO1lg0vLooVhlpBcxImMRHrXCB4 3D reserved=0>:    > +    int rc  num_ns;    +    +    /* Get the number of namespaces */    +    num_ns = nvme_json_get_array_size( config   ctrl.namespaces );    +    EXPECT_TRUE(num_ns > 0);    +    +    rc = create_standalone_io_driver( ctrl   ep   driver);    +    EXPECT_EQ(0  rc);    +    +    nvme_cmd_rw_t rw;    +    +    rw.dptr.prp1 = (uintptr_t) cpu_to_le64(prp1);    +    rw.dptr.prp2 = (uintptr_t) cpu_to_le64(prp2);    +    +    rw.slba = cpu_to_le64(0xff);    +    rw.nlb = (8 - 1);    I'm not sure I understand the logic here. data transfer length is (nlb+1) << block_order.  Are you counting on block_order == 9 ? What will happen in case I'll use json file with block_order = 12 ?    ________________________________    In tests/test_nvmf_ctrl.cc<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F255 23discussion_r244877303 data=02 7C01 7Cvasilyf 40mellanox.com 7Caa484548e6ef4daecc0808d67104a430 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636820662086262067 sdata=0UDmaJH2u7IITlrXLCxWPEZVQX 2FWfoSrMxv7GYc5IIk 3D reserved=0>:    > +    rc = create_standalone_io_driver( ctrl   ep   driver);    +    EXPECT_EQ(0  rc);    +    +    nvme_cmd_rw_t rw;    +    +    rw.dptr.prp1 = (uintptr_t) cpu_to_le64(prp1);    +    rw.dptr.prp2 = (uintptr_t) cpu_to_le64(prp2);    +    +    rw.slba = cpu_to_le64(0xff);    +    rw.nlb = (8 - 1);    +    +    nvme_queue_ops_t ops;    +    int num_queues = driver->num_queues;    +    +    for (int i = 0; i < num_ns; ++i) {    +        char c = (i + 1)   9;    why 9 ? can we use some random generator for traffic ?      You are receiving this because you authored the thread.  Reply to this email directly  view it on GitHub<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F255 23pullrequestreview-188849344 data=02 7C01 7Cvasilyf 40mellanox.com 7Caa484548e6ef4daecc0808d67104a430 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636820662086272072 sdata=onGVwG 2BShq3ckzZXoA1zs8t 2FyTr5XfyhdsAuPrfRZPA 3D reserved=0>  or mute the thread<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2Fnotifications 2Funsubscribe-auth 2FAIVK2qFj5uH-tcc_HpsqNKREvLCAW6BHks5u_TedgaJpZM4ZPAHk data=02 7C01 7Cvasilyf 40mellanox.com 7Caa484548e6ef4daecc0808d67104a430 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636820662086282086 sdata=XySwsSf1kgtRafo9kuc1Mj9pS8MXDC6 2FuRS6W80YUzo 3D reserved=0>.  
284,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/591/artifact/cov_build/html/index.html
285,vasilyMellanox,I mean   the PR is updated + some AlexM/mine comments added    From: Vasily Philipov  Sent: Thursday  January 03  2019 11:41  To: 'Mellanox/nvmx' <reply@reply.github.com>; Mellanox/nvmx <nvmx@noreply.github.com>  Cc: Author <author@noreply.github.com>; Max Gurtovoy <maxg@mellanox.com>; Alexander Mikheev <alexm@mellanox.com>  Subject: RE: [Mellanox/nvmx] Ns support ( 255)    Update  pls look at AlexM comments for you    From: Max Gurtovoy <notifications@github.com<mailto:notifications@github.com>>  Sent: Thursday  January 03  2019 00:50  To: Mellanox/nvmx <nvmx@noreply.github.com<mailto:nvmx@noreply.github.com>>  Cc: Vasily Philipov <vasilyf@mellanox.com<mailto:vasilyf@mellanox.com>>; Author <author@noreply.github.com<mailto:author@noreply.github.com>>  Subject: Re: [Mellanox/nvmx] Ns support ( 255)      @maxgurtovoy requested changes on this pull request.    Please make sure to read and response to all comments. Make sure you're compliant with our  Tests  policy for each commit. You must mention which tests were run before last submission. Otherwise  the reviewer will not approve the commit.    ________________________________    In tests/test_nvmf_ctrl.cc<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F255 23discussion_r244871013 data=02 7C01 7Cvasilyf 40mellanox.com 7Caa484548e6ef4daecc0808d67104a430 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636820662086242057 sdata=39dS6CRkg2hEvlrGSaiUpnezpS4rgobFM 2FyPv6N 2B 2BrM 3D reserved=0>:    > @@ -32 6 +34 15 @@ class NvmfControllerTest : public ::testing::Test {             nvme_emu_ep_t *ep = NULL;             nvmf_emu_ctrl_t *ctrl = NULL;        +        nvme_bar_t bar;    +        char *prp1  *prp2;    +    +        static const int PRP_OFFSET_BITS = 12;    +        static const int PRP_PAGE_SIZE = 1 << PRP_OFFSET_BITS;    +    +        char *prp_list[MAX_PRP_LIST_ENTRIES];    +        char data_buf[(MAX_PRP_LIST_ENTRIES + 1) * PRP_PAGE_SIZE];    where do you actually use data_buf ?    ________________________________    In tests/test_nvmf_ctrl.cc<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F255 23discussion_r244871273 data=02 7C01 7Cvasilyf 40mellanox.com 7Caa484548e6ef4daecc0808d67104a430 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636820662086252058 sdata=Kqh1koGWKtxotH2pd2J242qmmtDQaGbA4ZHhHtaBaAg 3D reserved=0>:    > @@ -32 6 +34 15 @@ class NvmfControllerTest : public ::testing::Test {             nvme_emu_ep_t *ep = NULL;             nvmf_emu_ctrl_t *ctrl = NULL;        +        nvme_bar_t bar;    +        char *prp1  *prp2;    +    +        static const int PRP_OFFSET_BITS = 12;    +        static const int PRP_PAGE_SIZE = 1 << PRP_OFFSET_BITS;    +    +        char *prp_list[MAX_PRP_LIST_ENTRIES];    where do you use prp_list ?    ________________________________    In tests/test_nvmf_ctrl.cc<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F255 23discussion_r244871953 data=02 7C01 7Cvasilyf 40mellanox.com 7Caa484548e6ef4daecc0808d67104a430 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636820662086252058 sdata=WUmJ3G9M 2FsKHUmUjQAkr8s 2FXQed1hntpokDa7dtkviY 3D reserved=0>:    > +        char c = (i + 1)   9;    +        rw.nsid = cpu_to_le32(ctrl->ns[i].id);    +    +        for (int j = 1; j < num_queues; ++j) {    +            ops.qid = j;    +            constant_fill((void *) prp1  c  NvmfControllerTest::PRP_PAGE_SIZE);    +    +            rc = ep->ep_ops.write_prp(ep   ops   rw  PRP_OFFSET_BITS);    +            EXPECT_EQ(NVMX_OK  rc);    +    +            prp_zero(prp1);    +            rc = ep->ep_ops.read_prp(ep   ops   rw  PRP_OFFSET_BITS);    +            EXPECT_EQ(NVMX_OK  rc);    +    +            EXPECT_EQ(c  prp1[0]);    +            EXPECT_EQ(c  prp1[255]);    don't you want to check the whole prp1 you wrote ?    ________________________________    In tests/test_nvmf_ctrl.cc<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F255 23discussion_r244874893 data=02 7C01 7Cvasilyf 40mellanox.com 7Caa484548e6ef4daecc0808d67104a430 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636820662086262067 sdata=MQ 2FTXGQc9Fa 2FxcXjuO1lg0vLooVhlpBcxImMRHrXCB4 3D reserved=0>:    > +    int rc  num_ns;    +    +    /* Get the number of namespaces */    +    num_ns = nvme_json_get_array_size( config   ctrl.namespaces );    +    EXPECT_TRUE(num_ns > 0);    +    +    rc = create_standalone_io_driver( ctrl   ep   driver);    +    EXPECT_EQ(0  rc);    +    +    nvme_cmd_rw_t rw;    +    +    rw.dptr.prp1 = (uintptr_t) cpu_to_le64(prp1);    +    rw.dptr.prp2 = (uintptr_t) cpu_to_le64(prp2);    +    +    rw.slba = cpu_to_le64(0xff);    +    rw.nlb = (8 - 1);    I'm not sure I understand the logic here. data transfer length is (nlb+1) << block_order.  Are you counting on block_order == 9 ? What will happen in case I'll use json file with block_order = 12 ?    ________________________________    In tests/test_nvmf_ctrl.cc<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F255 23discussion_r244877303 data=02 7C01 7Cvasilyf 40mellanox.com 7Caa484548e6ef4daecc0808d67104a430 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636820662086262067 sdata=0UDmaJH2u7IITlrXLCxWPEZVQX 2FWfoSrMxv7GYc5IIk 3D reserved=0>:    > +    rc = create_standalone_io_driver( ctrl   ep   driver);    +    EXPECT_EQ(0  rc);    +    +    nvme_cmd_rw_t rw;    +    +    rw.dptr.prp1 = (uintptr_t) cpu_to_le64(prp1);    +    rw.dptr.prp2 = (uintptr_t) cpu_to_le64(prp2);    +    +    rw.slba = cpu_to_le64(0xff);    +    rw.nlb = (8 - 1);    +    +    nvme_queue_ops_t ops;    +    int num_queues = driver->num_queues;    +    +    for (int i = 0; i < num_ns; ++i) {    +        char c = (i + 1)   9;    why 9 ? can we use some random generator for traffic ?      You are receiving this because you authored the thread.  Reply to this email directly  view it on GitHub<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F255 23pullrequestreview-188849344 data=02 7C01 7Cvasilyf 40mellanox.com 7Caa484548e6ef4daecc0808d67104a430 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636820662086272072 sdata=onGVwG 2BShq3ckzZXoA1zs8t 2FyTr5XfyhdsAuPrfRZPA 3D reserved=0>  or mute the thread<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2Fnotifications 2Funsubscribe-auth 2FAIVK2qFj5uH-tcc_HpsqNKREvLCAW6BHks5u_TedgaJpZM4ZPAHk data=02 7C01 7Cvasilyf 40mellanox.com 7Caa484548e6ef4daecc0808d67104a430 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636820662086282086 sdata=XySwsSf1kgtRafo9kuc1Mj9pS8MXDC6 2FuRS6W80YUzo 3D reserved=0>.  
286,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/592/artifact/cov_build/html/index.html
287,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/593/artifact/cov_build/html/index.html
288,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/594/artifact/cov_build/html/index.html
289,vasilyMellanox,R u kidding ?    From: Max Gurtovoy <notifications@github.com>  Sent: Thursday  January 03  2019 17:39  To: Mellanox/nvmx <nvmx@noreply.github.com>  Cc: Vasily Philipov <vasilyf@mellanox.com>; Author <author@noreply.github.com>  Subject: Re: [Mellanox/nvmx] Ns support ( 255)      @maxgurtovoy requested changes on this pull request.    you didn't run fio verify:  for f in /dev/nvme0n1 ; do for bs in 4k 8k 16k 32k 64k 128k ; do echo  f   echo  bs   fio --bs= bs --numjobs=1 --iodepth=1 --size=100mb --loops=10 --ioengine=psync --direct=1 --invalidate=1 --fsync_on_close=1 --randrepeat=1 --norandommap --filename= f --do_verify=1 --verify_fatal=1 --verify=md5 --output=/tmp/fio_verify.sh-3420.log --name=write-phase --rw=write --name=read-phase --stonewall --rw=read ; done ; done      You are receiving this because you authored the thread.  Reply to this email directly  view it on GitHub<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2FMellanox 2Fnvmx 2Fpull 2F255 23pullrequestreview-189061248 data=02 7C01 7Cvasilyf 40mellanox.com 7Cc6693ca650c34234f2db08d6719187dd 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636821267198673009 sdata=Qoo4LvaXejspjD6lfVjd6Jfl6SrreyQAGPs2 2BsOsnIg 3D reserved=0>  or mute the thread<https://emea01.safelinks.protection.outlook.com/?url=https 3A 2F 2Fgithub.com 2Fnotifications 2Funsubscribe-auth 2FAIVK2mlELKWYZyhY2hxOCb4LaFLQHYknks5u_iP9gaJpZM4ZPAHk data=02 7C01 7Cvasilyf 40mellanox.com 7Cc6693ca650c34234f2db08d6719187dd 7Ca652971c7d2e4d9ba6a4d149256f461b 7C0 7C0 7C636821267198673009 sdata=UpNUXrAyv3eh89FW7Wp0vqLrQ3pK2 2BdAPgMy1T2kjlk 3D reserved=0>.  
290,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/595/artifact/cov_build/html/index.html
291,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/596/artifact/cov_build/html/index.html
292,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/597/artifact/cov_build/html/index.html
293,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/598/artifact/cov_build/html/index.html
294,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/599/artifact/cov_build/html/index.html
295,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/600/artifact/cov_build/html/index.html
296,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/601/artifact/cov_build/html/index.html
297,nitzancarmi,@maxgurtovoy @alex-mikheev @vasilyMellanox @mlx3im   This is the second time we push to master a commit with compilation errors.  We need to verify:  1. always compile with ---enable-debug before every push.  2. We really need to verify this in our automatic checks before Pr approval.
298,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/602/artifact/cov_build/html/index.html
299,mike-dubman,@mlx3im -     we need to enhance CI and build nvmx in two modes: debug and release. currently it is only testing compilation in release mode.    Need to enhance https://github.com/Mellanox/swx_ci/blob/master/nvmx/ci_env.yaml L25 build_mode and add support for multiple build configurations. user can define multiple configure flags to be tested as separate stage  example:    ```  build_mode:      mode release:           configure_args: --enable-release      mode debug:           configure_args: --enable-debug  ```
300,maxgurtovoy,I've opened a task for it to Igor https://redmine.lab.mtl.com/issues/1617681  It will be done soon.
301,maxgurtovoy,old one. Abending.  
302,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/603/artifact/cov_build/html/index.html
303,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/604/artifact/cov_build/html/index.html
304,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/605/artifact/cov_build/html/index.html
305,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/606/artifact/cov_build/html/index.html
306,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/607/artifact/cov_build/html/index.html
307,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/608/artifact/cov_build/html/index.html
308,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/609/artifact/cov_build/html/index.html
309,nitzancarmi,@maxgurtovoy Comments were applied.  The patches were check against all valid configurations of backend_type + emu_type  (Updated on commit message).  
310,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/610/artifact/cov_build/html/index.html
311,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/611/artifact/cov_build/html/index.html
312,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/612/artifact/cov_build/html/index.html
313,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/613/artifact/cov_build/html/index.html
314,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/614/artifact/cov_build/html/index.html
315,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/615/artifact/cov_build/html/index.html
316,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/616/artifact/cov_build/html/index.html
317,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/617/artifact/cov_build/html/index.html
318,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/618/artifact/cov_build/html/index.html
319,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/619/artifact/cov_build/html/index.html
320,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/620/artifact/cov_build/html/index.html
321,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/621/artifact/cov_build/html/index.html
322,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/622/artifact/cov_build/html/index.html
323,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/623/artifact/cov_build/html/index.html
324,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/624/artifact/cov_build/html/index.html
325,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/625/artifact/cov_build/html/index.html
326,nitzancarmi,pushed full auto-create config file
327,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/626/artifact/cov_build/html/index.html
328,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/627/artifact/cov_build/html/index.html
329,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/628/artifact/cov_build/html/index.html
330,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/629/artifact/cov_build/html/index.html
331,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/630/artifact/cov_build/html/index.html
332,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/631/artifact/cov_build/html/index.html
333,nitzancarmi,I don't have any comments on this  but pay attention that this PR refactor all this code area completely.  https://github.com/Mellanox/nvmx/pull/285.    Wiaiting for your review on it as well :-)
334,alex-mikheev,@nitzancarmi are you sure that this one conflicts with  285 ? 
335,nitzancarmi,@alex-mikheev At second look  No. You're right. Anyway  this fix look good to me.
336,alex-mikheev,@vasilyMellanox can you please rebase/recheck and i will merge ? 10x
337,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/632/artifact/cov_build/html/index.html
338,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/633/artifact/cov_build/html/index.html
339,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/634/artifact/cov_build/html/index.html
340,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/635/artifact/cov_build/html/index.html
341,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/636/artifact/cov_build/html/index.html
342,mlx3im,Sorry  I was updating CI.  Will rerun checks.  bot:retest
343,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/637/artifact/cov_build/html/index.html
344,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/638/artifact/cov_build/html/index.html
345,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/639/artifact/cov_build/html/index.html
346,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/644/artifact/cov_build/html/index.html
347,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/645/artifact/cov_build/html/index.html
348,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/646/artifact/cov_build/html/index.html
349,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/647/artifact/cov_build/html/index.html
350,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/649/artifact/cov_build/html/index.html
351,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/650/artifact/cov_build/html/index.html
352,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/651/artifact/cov_build/html/index.html
353,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/652/artifact/cov_build/html/index.html
354,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/653/artifact/cov_build/html/index.html
355,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/654/artifact/cov_build/html/index.html
356,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/655/artifact/cov_build/html/index.html
357,galshachaf,you sure? this commit doesn't deal with tests at all. maybe you meant the previous one?  @alex-mikheev 
358,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/656/artifact/cov_build/html/index.html
359,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/657/artifact/cov_build/html/index.html
360,swx-jenkins2,Can one of the admins verify this patch?
361,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/658/artifact/cov_build/html/index.html
362,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/659/artifact/cov_build/html/index.html
363,nitzancarmi,This is great. But can you please add msec/usec scope?  This can be crucial for performance issues.
364,galshachaf,you mean disabled?  On Wed  16 Jan 2019 at 11:47 Alex Mikheev <notifications@github.com> wrote:  > *@alex-mikheev* commented on this pull request. > ------------------------------ > > In src/nvme_prm.c > <https://github.com/Mellanox/nvmx/pull/308 discussion_r248211126>: > > > @@ -243 6 +244 29 @@ int nvme_init_emu_dev(nvme_ctx_t *nctx) >      return 0; >  } > > +int nvme_destroy_emu_bar(nvme_ctx_t *nctx) > +{ > +    return devx_obj_destroy(nctx->barh); > +    nctx->emu_enabled = 0; > > why is emu_eanbled is here > >   > You are receiving this because you authored the thread. > Reply to this email directly  view it on GitHub > <https://github.com/Mellanox/nvmx/pull/308 pullrequestreview-193046618>  > or mute the thread > <https://github.com/notifications/unsubscribe-auth/AM6GiHg0QPNWbiP2O5FzX-29Xo1APqa-ks5vDvUqgaJpZM4aBE7s> > . > 
365,galshachaf,mm ok  I can move it to the seconds function  On Wed  16 Jan 2019 at 11:48 Gal Shachaf <galshac@gmail.com> wrote:  > you mean disabled? > > On Wed  16 Jan 2019 at 11:47 Alex Mikheev <notifications@github.com> > wrote: > >> *@alex-mikheev* commented on this pull request. >> ------------------------------ >> >> In src/nvme_prm.c >> <https://github.com/Mellanox/nvmx/pull/308 discussion_r248211126>: >> >> > @@ -243 6 +244 29 @@ int nvme_init_emu_dev(nvme_ctx_t *nctx) >>      return 0; >>  } >> >> +int nvme_destroy_emu_bar(nvme_ctx_t *nctx) >> +{ >> +    return devx_obj_destroy(nctx->barh); >> +    nctx->emu_enabled = 0; >> >> why is emu_eanbled is here >> >>   >> You are receiving this because you authored the thread. >> Reply to this email directly  view it on GitHub >> <https://github.com/Mellanox/nvmx/pull/308 pullrequestreview-193046618>  >> or mute the thread >> <https://github.com/notifications/unsubscribe-auth/AM6GiHg0QPNWbiP2O5FzX-29Xo1APqa-ks5vDvUqgaJpZM4aBE7s> >> . >> > 
366,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/664/artifact/cov_build/html/index.html
367,mlx3im,bot:retest
368,mlx3im,bot:retest
369,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/665/artifact/cov_build/html/index.html
370,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/666/artifact/cov_build/html/index.html
371,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/667/artifact/cov_build/html/index.html
372,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/668/artifact/cov_build/html/index.html
373,alex-mikheev,:bot:retest
374,alex-mikheev,bot:retest
375,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/669/artifact/cov_build/html/index.html
376,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/670/artifact/cov_build/html/index.html
377,alex-mikheev,bot:retest  
378,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/671/artifact/cov_build/html/index.html
379,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/673/artifact/cov_build/html/index.html
380,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/672/artifact/cov_build/html/index.html
381,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/674/artifact/cov_build/html/index.html
382,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/675/artifact/cov_build/html/index.html
383,mlx3im,bot:retest
384,mlx3im,bot:retest
385,alex-mikheev,bot:retest
386,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/686/artifact/cov_build/html/index.html
387,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/687/artifact/cov_build/html/index.html
388,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/688/artifact/cov_build/html/index.html
389,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/689/artifact/cov_build/html/index.html
390,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/690/artifact/cov_build/html/index.html
391,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/691/artifact/cov_build/html/index.html
392,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/692/artifact/cov_build/html/index.html
393,nitzancarmi,@alex-mikheev @maxgurtovoy Appreciate your review on this one
394,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/693/artifact/cov_build/html/index.html
395,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/694/artifact/cov_build/html/index.html
396,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/695/artifact/cov_build/html/index.html
397,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/696/artifact/cov_build/html/index.html
398,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/697/artifact/cov_build/html/index.html
399,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/698/artifact/cov_build/html/index.html
400,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/699/artifact/cov_build/html/index.html
401,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/700/artifact/cov_build/html/index.html
402,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/701/artifact/cov_build/html/index.html
403,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/702/artifact/cov_build/html/index.html
404,mlx3im,WIP. Let me verify changes
405,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/704/artifact/cov_build/html/index.html
406,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/705/artifact/cov_build/html/index.html
407,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/706/artifact/cov_build/html/index.html
408,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/707/artifact/cov_build/html/index.html
409,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/708/artifact/cov_build/html/index.html
410,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/709/artifact/cov_build/html/index.html
411,mlx3im,@miked-mellanox @yshestakov   Fixed your requests  please  review the changes.  ty
412,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/710/artifact/cov_build/html/index.html
413,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/711/artifact/cov_build/html/index.html
414,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/712/artifact/cov_build/html/index.html
415,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/713/artifact/cov_build/html/index.html
416,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/714/artifact/cov_build/html/index.html
417,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/715/artifact/cov_build/html/index.html
418,yshestakov,bot:retest
419,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/716/artifact/cov_build/html/index.html
420,yshestakov,bot:retest
421,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/717/artifact/cov_build/html/index.html
422,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/718/artifact/cov_build/html/index.html
423,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/719/artifact/cov_build/html/index.html
424,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/720/artifact/cov_build/html/index.html
425,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/721/artifact/cov_build/html/index.html
426,galshachaf,@alex-mikheev @nitzancarmi   for some reason I can't assign reviewers...
427,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/723/artifact/cov_build/html/index.html
428,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/724/artifact/cov_build/html/index.html
429,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/725/artifact/cov_build/html/index.html
430,yshestakov,@maxgurtovoy Does this commit message look good?
431,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/726/artifact/cov_build/html/index.html
432,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/727/artifact/cov_build/html/index.html
433,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/728/artifact/cov_build/html/index.html
434,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/729/artifact/cov_build/html/index.html
435,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/730/artifact/cov_build/html/index.html
436,alex-mikheev,bot:retest
437,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/731/artifact/cov_build/html/index.html
438,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/732/artifact/cov_build/html/index.html
439,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/733/artifact/cov_build/html/index.html
440,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/734/artifact/cov_build/html/index.html
441,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/735/artifact/cov_build/html/index.html
442,mlx3im,> your commit message doesn't match our styling standards.  > See examples in git log    Updated
443,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/736/artifact/cov_build/html/index.html
444,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/737/artifact/cov_build/html/index.html
445,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/738/artifact/cov_build/html/index.html
446,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/739/artifact/cov_build/html/index.html
447,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/740/artifact/cov_build/html/index.html
448,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/741/artifact/cov_build/html/index.html
449,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/742/artifact/cov_build/html/index.html
450,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/743/artifact/cov_build/html/index.html
451,alex-mikheev,@galshachaf you have to fix nvmx/NvmeControllerTestP.set_n_queues/0 
452,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/744/artifact/cov_build/html/index.html
453,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/745/artifact/cov_build/html/index.html
454,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/746/artifact/cov_build/html/index.html
455,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/747/artifact/cov_build/html/index.html
456,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/748/artifact/cov_build/html/index.html
457,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/749/artifact/cov_build/html/index.html
458,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/750/artifact/cov_build/html/index.html
459,mlx3im,Looks good  example:  http://r-fserv-105/pulp/repos/nvmx-pr/Packages/n/nvmx-0.9-pr330.mlnx.aarch64.rpm
460,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/751/artifact/cov_build/html/index.html
461,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/752/artifact/cov_build/html/index.html
462,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/753/artifact/cov_build/html/index.html
463,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/754/artifact/cov_build/html/index.html
464,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/755/artifact/cov_build/html/index.html
465,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/756/artifact/cov_build/html/index.html
466,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/757/artifact/cov_build/html/index.html
467,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/758/artifact/cov_build/html/index.html
468,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/759/artifact/cov_build/html/index.html
469,vasilyMellanox,fixed
470,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/760/artifact/cov_build/html/index.html
471,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/761/artifact/cov_build/html/index.html
472,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/762/artifact/cov_build/html/index.html
473,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/763/artifact/cov_build/html/index.html
474,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/765/artifact/cov_build/html/index.html
475,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/764/artifact/cov_build/html/index.html
476,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/766/artifact/cov_build/html/index.html
477,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/767/artifact/cov_build/html/index.html
478,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/768/artifact/cov_build/html/index.html
479,yshestakov,It looks good to me
480,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/769/artifact/cov_build/html/index.html
481,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/770/artifact/cov_build/html/index.html
482,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/771/artifact/cov_build/html/index.html
483,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/772/artifact/cov_build/html/index.html
484,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/773/artifact/cov_build/html/index.html
485,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/774/artifact/cov_build/html/index.html
486,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/775/artifact/cov_build/html/index.html
487,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/776/artifact/cov_build/html/index.html
488,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/777/artifact/cov_build/html/index.html
489,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/778/artifact/cov_build/html/index.html
490,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/779/artifact/cov_build/html/index.html
491,vasilyMellanox,fixed
492,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/780/artifact/cov_build/html/index.html
493,vasilyMellanox,fixed
494,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/781/artifact/cov_build/html/index.html
495,vasilyMellanox,fixed
496,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/782/artifact/cov_build/html/index.html
497,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/783/artifact/cov_build/html/index.html
498,galshachaf,compiles on my machine... anyhow  I need to rebase anyhow  I'll recheck each commit as requested (and as done previously of course)
499,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/784/artifact/cov_build/html/index.html
500,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/785/artifact/cov_build/html/index.html
501,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/786/artifact/cov_build/html/index.html
502,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/787/artifact/cov_build/html/index.html
503,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/788/artifact/cov_build/html/index.html
504,vasilyMellanox,fixed
505,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/790/artifact/cov_build/html/index.html
506,vasilyMellanox,fixed
507,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/792/artifact/cov_build/html/index.html
508,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/793/artifact/cov_build/html/index.html
509,vasilyMellanox,@maxgurtovoy @alex-mikheev   fixed + checked with fw-41682-rel-18_99_5476
510,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/797/artifact/cov_build/html/index.html
511,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/799/artifact/cov_build/html/index.html
512,yshestakov,bot:retest
513,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/802/artifact/cov_build/html/index.html
514,yshestakov,bot:retest
515,galshachaf,@maxgurtovoy 
516,nitzancarmi,nvmx_log is used in most cases to print lines incrementaly (without  \n  on every line).  This timestamps will only mess it up.  We need to refactor all nvmx_log usages to match  single printf per line  standard.  I already have partial implementation for this in a local branch.
517,galshachaf,OK  i'll change all incremental printing.
518,yshestakov,bot:retest
519,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/806/artifact/cov_build/html/index.html
520,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/809/artifact/cov_build/html/index.html
521,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/810/artifact/cov_build/html/index.html
522,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/811/artifact/cov_build/html/index.html
523,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/812/artifact/cov_build/html/index.html
524,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/813/artifact/cov_build/html/index.html
525,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/814/artifact/cov_build/html/index.html
526,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/815/artifact/cov_build/html/index.html
527,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/816/artifact/cov_build/html/index.html
528,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/817/artifact/cov_build/html/index.html
529,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/818/artifact/cov_build/html/index.html
530,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/819/artifact/cov_build/html/index.html
531,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/820/artifact/cov_build/html/index.html
532,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/821/artifact/cov_build/html/index.html
533,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/822/artifact/cov_build/html/index.html
534,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/823/artifact/cov_build/html/index.html
535,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/825/artifact/cov_build/html/index.html
536,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/826/artifact/cov_build/html/index.html
537,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/827/artifact/cov_build/html/index.html
538,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/828/artifact/cov_build/html/index.html
539,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/829/artifact/cov_build/html/index.html
540,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/830/artifact/cov_build/html/index.html
541,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/831/artifact/cov_build/html/index.html
542,vasilyMellanox,fixed
543,vasilyMellanox,>   >   > looks good. added minor fixes.  > have you tested the old functionality of memdisk and posix_io backends ?    Yes  just added it to the commit message
544,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/832/artifact/cov_build/html/index.html
545,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/833/artifact/cov_build/html/index.html
546,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/834/artifact/cov_build/html/index.html
547,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/835/artifact/cov_build/html/index.html
548,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/836/artifact/cov_build/html/index.html
549,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/837/artifact/cov_build/html/index.html
550,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/838/artifact/cov_build/html/index.html
551,vasilyMellanox,fixed
552,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/839/artifact/cov_build/html/index.html
553,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/840/artifact/cov_build/html/index.html
554,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/841/artifact/cov_build/html/index.html
555,galshachaf,@maxgurtovoy all issues resolved
556,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/842/artifact/cov_build/html/index.html
557,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/843/artifact/cov_build/html/index.html
558,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/844/artifact/cov_build/html/index.html
559,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/845/artifact/cov_build/html/index.html
560,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/846/artifact/cov_build/html/index.html
561,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/847/artifact/cov_build/html/index.html
562,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/848/artifact/cov_build/html/index.html
563,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/849/artifact/cov_build/html/index.html
564,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/850/artifact/cov_build/html/index.html
565,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/851/artifact/cov_build/html/index.html
566,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/852/artifact/cov_build/html/index.html
567,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/853/artifact/cov_build/html/index.html
568,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/854/artifact/cov_build/html/index.html
569,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/855/artifact/cov_build/html/index.html
570,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/856/artifact/cov_build/html/index.html
571,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/857/artifact/cov_build/html/index.html
572,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/858/artifact/cov_build/html/index.html
573,alex-mikheev,@maxgurtovoy can you take a look at 7d746a5   @vasilyMellanox there was a major refactoring in the test framework. Now it is much easier to do complex traffic tests. Can you please go over the changes.
574,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/859/artifact/cov_build/html/index.html
575,galshachaf,@alex-mikheev please review
576,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/860/artifact/cov_build/html/index.html
577,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/861/artifact/cov_build/html/index.html
578,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/862/artifact/cov_build/html/index.html
579,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/863/artifact/cov_build/html/index.html
580,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/864/artifact/cov_build/html/index.html
581,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/865/artifact/cov_build/html/index.html
582,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/866/artifact/cov_build/html/index.html
583,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/867/artifact/cov_build/html/index.html
584,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/868/artifact/cov_build/html/index.html
585,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/869/artifact/cov_build/html/index.html
586,alex-mikheev,@yshestakov can you also review please. 
587,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/870/artifact/cov_build/html/index.html
588,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/871/artifact/cov_build/html/index.html
589,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/872/artifact/cov_build/html/index.html
590,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/873/artifact/cov_build/html/index.html
591,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/874/artifact/cov_build/html/index.html
592,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/875/artifact/cov_build/html/index.html
593,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/876/artifact/cov_build/html/index.html
594,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/877/artifact/cov_build/html/index.html
595,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/878/artifact/cov_build/html/index.html
596,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/879/artifact/cov_build/html/index.html
597,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/880/artifact/cov_build/html/index.html
598,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/881/artifact/cov_build/html/index.html
599,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/883/artifact/cov_build/html/index.html
600,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/882/artifact/cov_build/html/index.html
601,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/884/artifact/cov_build/html/index.html
602,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/885/artifact/cov_build/html/index.html
603,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/886/artifact/cov_build/html/index.html
604,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/887/artifact/cov_build/html/index.html
605,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/888/artifact/cov_build/html/index.html
606,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/889/artifact/cov_build/html/index.html
607,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/890/artifact/cov_build/html/index.html
608,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/891/artifact/cov_build/html/index.html
609,vasilyMellanox,fixed
610,vasilyMellanox,fixed
611,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/892/artifact/cov_build/html/index.html
612,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/893/artifact/cov_build/html/index.html
613,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/894/artifact/cov_build/html/index.html
614,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/895/artifact/cov_build/html/index.html
615,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/896/artifact/cov_build/html/index.html
616,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/897/artifact/cov_build/html/index.html
617,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/898/artifact/cov_build/html/index.html
618,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/899/artifact/cov_build/html/index.html
619,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/900/artifact/cov_build/html/index.html
620,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/901/artifact/cov_build/html/index.html
621,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/902/artifact/cov_build/html/index.html
622,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/903/artifact/cov_build/html/index.html
623,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/904/artifact/cov_build/html/index.html
624,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/905/artifact/cov_build/html/index.html
625,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/906/artifact/cov_build/html/index.html
626,hellerguyh,the code in lines 1153:1164 and in nvmf_ctrl_verify_mdts_configuration does pretty much the same except that one considers metadata and one doesn't. can it be reduced into a function which receives the metadata size as input  and it will get 0 for the nvmf_ctrl_verify_mdts_configuration?
627,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/907/artifact/cov_build/html/index.html
628,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/908/artifact/cov_build/html/index.html
629,vasilyMellanox,fixed
630,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/909/artifact/cov_build/html/index.html
631,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/910/artifact/cov_build/html/index.html
632,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/911/artifact/cov_build/html/index.html
633,nitzancarmi,@maxgurtovoy @hellerguyh This is 2 preperations commits as part of dynamic namespaces feature.
634,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/912/artifact/cov_build/html/index.html
635,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/913/artifact/cov_build/html/index.html
636,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/916/artifact/cov_build/html/index.html
637,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/917/artifact/cov_build/html/index.html
638,swx-jenkins2,Can one of the admins verify this patch?
639,alex-mikheev,:bot:retest
640,yshestakov,bot:retest
641,yshestakov,bot:retest
642,yshestakov,bot:retest
643,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/927/artifact/cov_build/html/index.html
644,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/928/artifact/cov_build/html/index.html
645,yshestakov,bot:retest
646,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/929/artifact/cov_build/html/index.html
647,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/930/artifact/cov_build/html/index.html
648,yshestakov,bot:retest
649,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/931/artifact/cov_build/html/index.html
650,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/932/artifact/cov_build/html/index.html
651,nitzancarmi,1. Changed the titles a little bit for easier differentiation.  2. Pulled out small chunks of code that was not directly related to the linked-list move.  
652,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/935/artifact/cov_build/html/index.html
653,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/936/artifact/cov_build/html/index.html
654,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/937/artifact/cov_build/html/index.html
655,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/939/artifact/cov_build/html/index.html
656,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/940/artifact/cov_build/html/index.html
657,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/941/artifact/cov_build/html/index.html
658,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/942/artifact/cov_build/html/index.html
659,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/943/artifact/cov_build/html/index.html
660,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/946/artifact/cov_build/html/index.html
661,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/947/artifact/cov_build/html/index.html
662,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/948/artifact/cov_build/html/index.html
663,vasilyMellanox,fixed
664,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/949/artifact/cov_build/html/index.html
665,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/950/artifact/cov_build/html/index.html
666,vasilyMellanox,fixed
667,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/951/artifact/cov_build/html/index.html
668,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/952/artifact/cov_build/html/index.html
669,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/953/artifact/cov_build/html/index.html
670,nitzancarmi,@maxgurtovoy @alex-mikheev please review.  After new debug counters PRM is introduced  I'll push the rest of the smart-log support.
671,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/955/artifact/cov_build/html/index.html
672,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/956/artifact/cov_build/html/index.html
673,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/957/artifact/cov_build/html/index.html
674,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/958/artifact/cov_build/html/index.html
675,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/959/artifact/cov_build/html/index.html
676,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/960/artifact/cov_build/html/index.html
677,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/961/artifact/cov_build/html/index.html
678,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/962/artifact/cov_build/html/index.html
679,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/963/artifact/cov_build/html/index.html
680,alex-mikheev,@vasilyMellanox you need to pass vhca_id to devx_qp_init and use it when flags is QP_ON_BEHALF  @ebarilanM 
681,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/965/artifact/cov_build/html/index.html
682,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/966/artifact/cov_build/html/index.html
683,nitzancarmi,> fix the typo in the commit message and we'll merge.    done.
684,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/967/artifact/cov_build/html/index.html
685,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/968/artifact/cov_build/html/index.html
686,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/969/artifact/cov_build/html/index.html
687,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/970/artifact/cov_build/html/index.html
688,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/972/artifact/cov_build/html/index.html
689,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/971/artifact/cov_build/html/index.html
690,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/973/artifact/cov_build/html/index.html
691,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/974/artifact/cov_build/html/index.html
692,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/975/artifact/cov_build/html/index.html
693,yshestakov,bot:retest
694,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/976/artifact/cov_build/html/index.html
695,nitzancarmi,@alex-mikheev   this is outdated. You can close it now.  A new version for this patch would be: https://github.com/Mellanox/nvmx/pull/384.  
696,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/977/artifact/cov_build/html/index.html
697,yshestakov,bot:retest
698,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/978/artifact/cov_build/html/index.html
699,yshestakov,bot:retest
700,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/979/artifact/cov_build/html/index.html
701,nitzancarmi,@hellerguyh  Can you please go over this patch? it is part of the preperations from dynamic namespaces feature
702,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/980/artifact/cov_build/html/index.html
703,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/983/artifact/cov_build/html/index.html
704,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/984/artifact/cov_build/html/index.html
705,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/985/artifact/cov_build/html/index.html
706,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/987/artifact/cov_build/html/index.html
707,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/988/artifact/cov_build/html/index.html
708,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/990/artifact/cov_build/html/index.html
709,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/991/artifact/cov_build/html/index.html
710,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/992/artifact/cov_build/html/index.html
711,vasilyMellanox,fixed
712,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/993/artifact/cov_build/html/index.html
713,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/995/artifact/cov_build/html/index.html
714,nitzancarmi,@maxgurtovoy Added more detailed explanation about the locks on the commit message   and added some testing (variations of load/unload and HA tests).  Also (manually) extended the start_top_stress unit test period (as it was the test that originally failed).
715,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/996/artifact/cov_build/html/index.html
716,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/997/artifact/cov_build/html/index.html
717,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/999/artifact/cov_build/html/index.html
718,yshestakov,> @yshestakov we need to fix jenkins scripts    It's not clear for me where to clear the reference on (removed) git submodule `src/devx`.  Have you modified and pushed `.git/modules` file to the PR?
719,yshestakov,bot:retest
720,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1003/artifact/cov_build/html/index.html
721,nitzancarmi,Please add default log path and default verbosity level to commit message
722,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1004/artifact/cov_build/html/index.html
723,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1005/artifact/cov_build/html/index.html
724,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1006/artifact/cov_build/html/index.html
725,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1007/artifact/cov_build/html/index.html
726,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1008/artifact/cov_build/html/index.html
727,yshestakov,bot:retest
728,yshestakov,bot:retest
729,yshestakov,bot:retest
730,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1013/artifact/cov_build/html/index.html
731,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1014/artifact/cov_build/html/index.html
732,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1015/artifact/cov_build/html/index.html
733,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1016/artifact/cov_build/html/index.html
734,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1017/artifact/cov_build/html/index.html
735,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1018/artifact/cov_build/html/index.html
736,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1019/artifact/cov_build/html/index.html
737,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1020/artifact/cov_build/html/index.html
738,maxgurtovoy,I merged this in my mlnx-ofed-4.6 alignment.  please abandon this.
739,maxgurtovoy,was merge during mlnx-ofed-4.6 alignment.  please abandon this.
740,maxgurtovoy,was merge during mlnx-ofed-4.6 alignment.  please abandon this.
741,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1021/artifact/cov_build/html/index.html
742,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1022/artifact/cov_build/html/index.html
743,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1023/artifact/cov_build/html/index.html
744,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1024/artifact/cov_build/html/index.html
745,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1025/artifact/cov_build/html/index.html
746,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1026/artifact/cov_build/html/index.html
747,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1027/artifact/cov_build/html/index.html
748,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1028/artifact/cov_build/html/index.html
749,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1029/artifact/cov_build/html/index.html
750,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1030/artifact/cov_build/html/index.html
751,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1031/artifact/cov_build/html/index.html
752,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1032/artifact/cov_build/html/index.html
753,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1033/artifact/cov_build/html/index.html
754,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1034/artifact/cov_build/html/index.html
755,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1035/artifact/cov_build/html/index.html
756,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1036/artifact/cov_build/html/index.html
757,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1037/artifact/cov_build/html/index.html
758,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1038/artifact/cov_build/html/index.html
759,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1039/artifact/cov_build/html/index.html
760,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1040/artifact/cov_build/html/index.html
761,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1041/artifact/cov_build/html/index.html
762,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1042/artifact/cov_build/html/index.html
763,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1043/artifact/cov_build/html/index.html
764,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1044/artifact/cov_build/html/index.html
765,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1045/artifact/cov_build/html/index.html
766,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1046/artifact/cov_build/html/index.html
767,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1047/artifact/cov_build/html/index.html
768,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1048/artifact/cov_build/html/index.html
769,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1049/artifact/cov_build/html/index.html
770,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1050/artifact/cov_build/html/index.html
771,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1051/artifact/cov_build/html/index.html
772,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1052/artifact/cov_build/html/index.html
773,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1053/artifact/cov_build/html/index.html
774,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1054/artifact/cov_build/html/index.html
775,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1055/artifact/cov_build/html/index.html
776,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1056/artifact/cov_build/html/index.html
777,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1057/artifact/cov_build/html/index.html
778,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1058/artifact/cov_build/html/index.html
779,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1059/artifact/cov_build/html/index.html
780,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1060/artifact/cov_build/html/index.html
781,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1061/artifact/cov_build/html/index.html
782,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1062/artifact/cov_build/html/index.html
783,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1063/artifact/cov_build/html/index.html
784,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1064/artifact/cov_build/html/index.html
785,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1065/artifact/cov_build/html/index.html
786,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1066/artifact/cov_build/html/index.html
787,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1067/artifact/cov_build/html/index.html
788,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1068/artifact/cov_build/html/index.html
789,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1069/artifact/cov_build/html/index.html
790,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1070/artifact/cov_build/html/index.html
791,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1071/artifact/cov_build/html/index.html
792,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1072/artifact/cov_build/html/index.html
793,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1073/artifact/cov_build/html/index.html
794,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1074/artifact/cov_build/html/index.html
795,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1075/artifact/cov_build/html/index.html
796,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1076/artifact/cov_build/html/index.html
797,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1077/artifact/cov_build/html/index.html
798,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1078/artifact/cov_build/html/index.html
799,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1079/artifact/cov_build/html/index.html
800,nitzancarmi,Merged in another PR
801,nitzancarmi,Outdated and rebased into another PR
802,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1080/artifact/cov_build/html/index.html
803,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1081/artifact/cov_build/html/index.html
804,vasilyMellanox,fixed
805,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1082/artifact/cov_build/html/index.html
806,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1083/artifact/cov_build/html/index.html
807,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1084/artifact/cov_build/html/index.html
808,maxgurtovoy,Is the scenario you wrote happened to you ?  maybe in case you fail to move state during fail_ep and ret  = 0 do nothing (you already in deleting or new state).  I don't want to solve scenarios that never happen and add semaphores for that.
809,vasilyMellanox,Now the code will print (by default) the backtracing into the stream defined by the logger
810,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1085/artifact/cov_build/html/index.html
811,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1086/artifact/cov_build/html/index.html
812,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1087/artifact/cov_build/html/index.html
813,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1088/artifact/cov_build/html/index.html
814,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1089/artifact/cov_build/html/index.html
815,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1090/artifact/cov_build/html/index.html
816,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1091/artifact/cov_build/html/index.html
817,vasilyMellanox,fixed
818,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1092/artifact/cov_build/html/index.html
819,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1093/artifact/cov_build/html/index.html
820,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1094/artifact/cov_build/html/index.html
821,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1095/artifact/cov_build/html/index.html
822,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1096/artifact/cov_build/html/index.html
823,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1097/artifact/cov_build/html/index.html
824,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1098/artifact/cov_build/html/index.html
825,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1099/artifact/cov_build/html/index.html
826,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1100/artifact/cov_build/html/index.html
827,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1101/artifact/cov_build/html/index.html
828,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1102/artifact/cov_build/html/index.html
829,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1103/artifact/cov_build/html/index.html
830,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1104/artifact/cov_build/html/index.html
831,hellerguyh,moved to a new solution: patch  404 
832,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1105/artifact/cov_build/html/index.html
833,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1106/artifact/cov_build/html/index.html
834,swx-jenkins2,Can one of the admins verify this patch?
835,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1107/artifact/cov_build/html/index.html
836,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1108/artifact/cov_build/html/index.html
837,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1109/artifact/cov_build/html/index.html
838,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1110/artifact/cov_build/html/index.html
839,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1111/artifact/cov_build/html/index.html
840,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1112/artifact/cov_build/html/index.html
841,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1113/artifact/cov_build/html/index.html
842,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1114/artifact/cov_build/html/index.html
843,yshestakov,bot:retest
844,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1115/artifact/cov_build/html/index.html
845,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1116/artifact/cov_build/html/index.html
846,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1117/artifact/cov_build/html/index.html
847,yshestakov,bot:retest
848,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1118/artifact/cov_build/html/index.html
849,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1119/artifact/cov_build/html/index.html
850,swx-jenkins2,Can one of the admins verify this patch?
851,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1120/artifact/cov_build/html/index.html
852,MrBr-github,bot:retest
853,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1121/artifact/cov_build/html/index.html
854,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1122/artifact/cov_build/html/index.html
855,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1123/artifact/cov_build/html/index.html
856,yshestakov,bot:retest
857,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1124/artifact/cov_build/html/index.html
858,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1125/artifact/cov_build/html/index.html
859,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1126/artifact/cov_build/html/index.html
860,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1127/artifact/cov_build/html/index.html
861,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1128/artifact/cov_build/html/index.html
862,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1129/artifact/cov_build/html/index.html
863,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1130/artifact/cov_build/html/index.html
864,maxgurtovoy,Yurii  please don't approve commit that don't stand in the standards of the commit message. don't we have automation that checks this ? (as we discussed in the past regarding the  Tests  clause check  subject length check  inner line length check. maybe add also spelling checks. etc...)
865,yshestakov,> Yurii  please don't approve commit that don't stand in the standards of the commit message. don't we have automation that checks this? (as we discussed in the past regarding the  Tests  clause check  subject length check  inner line length check. maybe add also spelling checks. etc...)    OK. Got it. BTW commit msg of PR 405 was fixed by my before the merge  Regarding automated test of commit messages. Sorry  it is a low priority task for me. Need to spend a few days on reworking  pr-nvmx-pipeline :  - move googletest code out of project  i.e. don't check out it and don't compile it  - remove extra compilation of the code (we compile it 3-5 times now)  - add Valgrind check  - add commit message check
866,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1131/artifact/cov_build/html/index.html
867,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1132/artifact/cov_build/html/index.html
868,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1133/artifact/cov_build/html/index.html
869,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1134/artifact/cov_build/html/index.html
870,nitzancarmi,Max/Alex  how do you think we should behave - connect to same disk through all interfaces  or use a  dummy  ones as we spoke before?
871,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1135/artifact/cov_build/html/index.html
872,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1136/artifact/cov_build/html/index.html
873,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1137/artifact/cov_build/html/index.html
874,yshestakov,> Max/Alex  how do you think we should behave - connect to same disk through all interfaces  or use a  dummy  ones as we spoke before?    I would run `nvme_controller` with dummy disk if there is no config file - `/etc/nvme_snap/mlx5_3.json` for example
875,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1138/artifact/cov_build/html/index.html
876,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1139/artifact/cov_build/html/index.html
877,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1140/artifact/cov_build/html/index.html
878,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1141/artifact/cov_build/html/index.html
879,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1142/artifact/cov_build/html/index.html
880,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1143/artifact/cov_build/html/index.html
881,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1144/artifact/cov_build/html/index.html
882,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1145/artifact/cov_build/html/index.html
883,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1146/artifact/cov_build/html/index.html
884,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1147/artifact/cov_build/html/index.html
885,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1148/artifact/cov_build/html/index.html
886,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1149/artifact/cov_build/html/index.html
887,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1150/artifact/cov_build/html/index.html
888,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1151/artifact/cov_build/html/index.html
889,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1152/artifact/cov_build/html/index.html
890,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1153/artifact/cov_build/html/index.html
891,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1154/artifact/cov_build/html/index.html
892,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1155/artifact/cov_build/html/index.html
893,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1156/artifact/cov_build/html/index.html
894,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1157/artifact/cov_build/html/index.html
895,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1158/artifact/cov_build/html/index.html
896,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1159/artifact/cov_build/html/index.html
897,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1160/artifact/cov_build/html/index.html
898,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1161/artifact/cov_build/html/index.html
899,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1162/artifact/cov_build/html/index.html
900,nitzancarmi,I don't like adding namespaces by default.  This adds additional  tiny  disk in host side  which he can read/write from. it is confusing for users   and we should avoid it as much as we can (even now we are having very  wierd  workarounds).  If there is a bug on HP boot  I think it is better to special case it and modify it manually.    Maybe check something like: `dmidecode | grep  Vendor `
901,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1163/artifact/cov_build/html/index.html
902,yshestakov,> I don't like adding namespaces by default.  > This adds additional  tiny  disk in host side  which he can read/write from. it is confusing for users   > and we should avoid it as much as we can (even now we are having very  wierd  workarounds).  > If there is a bug on HP boot  I think it is better to special case it and modify it manually.  >   > Maybe check something like: `dmidecode | grep  Vendor `    We don't have access to the SMBIOS tables of x86 host from the ARM side.
903,hellerguyh,> > I don't like adding namespaces by default.  > > This adds additional  tiny  disk in host side  which he can read/write from. it is confusing for users   > > and we should avoid it as much as we can (even now we are having very  wierd  workarounds).  > > If there is a bug on HP boot  I think it is better to special case it and modify it manually.  > > Maybe check something like: `dmidecode | grep  Vendor `  >   > We don't have access to the SMBIOS tables of x86 host from the ARM side.    Can we add a parameter while installing the nvme-snap? Yurii what will be the best option for that? 
904,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1165/artifact/cov_build/html/index.html
905,hellerguyh,This patch is not needed - the problem this patch intended to solve was solved by changing the ECPF parameters for the next boot. 
906,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1167/artifact/cov_build/html/index.html
907,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1169/artifact/cov_build/html/index.html
908,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1170/artifact/cov_build/html/index.html
909,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1171/artifact/cov_build/html/index.html
910,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1172/artifact/cov_build/html/index.html
911,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1173/artifact/cov_build/html/index.html
912,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1174/artifact/cov_build/html/index.html
913,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1175/artifact/cov_build/html/index.html
914,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1176/artifact/cov_build/html/index.html
915,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1177/artifact/cov_build/html/index.html
916,vasilyMellanox,fixed
917,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1178/artifact/cov_build/html/index.html
918,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1179/artifact/cov_build/html/index.html
919,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1180/artifact/cov_build/html/index.html
920,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1181/artifact/cov_build/html/index.html
921,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1182/artifact/cov_build/html/index.html
922,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1183/artifact/cov_build/html/index.html
923,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1184/artifact/cov_build/html/index.html
924,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1185/artifact/cov_build/html/index.html
925,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1186/artifact/cov_build/html/index.html
926,yshestakov,bot:retest
927,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1187/artifact/cov_build/html/index.html
928,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1188/artifact/cov_build/html/index.html
929,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1189/artifact/cov_build/html/index.html
930,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1190/artifact/cov_build/html/index.html
931,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1193/artifact/cov_build/html/index.html
932,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1194/artifact/cov_build/html/index.html
933,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1195/artifact/cov_build/html/index.html
934,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1196/artifact/cov_build/html/index.html
935,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1197/artifact/cov_build/html/index.html
936,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1198/artifact/cov_build/html/index.html
937,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1199/artifact/cov_build/html/index.html
938,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1200/artifact/cov_build/html/index.html
939,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1201/artifact/cov_build/html/index.html
940,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1202/artifact/cov_build/html/index.html
941,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1203/artifact/cov_build/html/index.html
942,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1204/artifact/cov_build/html/index.html
943,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1205/artifact/cov_build/html/index.html
944,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1206/artifact/cov_build/html/index.html
945,nitzancarmi,bot:retest
946,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1208/artifact/cov_build/html/index.html
947,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1209/artifact/cov_build/html/index.html
948,vasilyMellanox,fixed
949,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1210/artifact/cov_build/html/index.html
950,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1211/artifact/cov_build/html/index.html
951,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1212/artifact/cov_build/html/index.html
952,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1213/artifact/cov_build/html/index.html
953,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1215/artifact/cov_build/html/index.html
954,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1216/artifact/cov_build/html/index.html
955,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1218/artifact/cov_build/html/index.html
956,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1217/artifact/cov_build/html/index.html
957,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1219/artifact/cov_build/html/index.html
958,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1220/artifact/cov_build/html/index.html
959,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1221/artifact/cov_build/html/index.html
960,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1222/artifact/cov_build/html/index.html
961,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1223/artifact/cov_build/html/index.html
962,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1224/artifact/cov_build/html/index.html
963,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1225/artifact/cov_build/html/index.html
964,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1226/artifact/cov_build/html/index.html
965,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1227/artifact/cov_build/html/index.html
966,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1228/artifact/cov_build/html/index.html
967,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1230/artifact/cov_build/html/index.html
968,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1229/artifact/cov_build/html/index.html
969,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1231/artifact/cov_build/html/index.html
970,vasilyMellanox,bot:retest
971,vasilyMellanox,bot:retest
972,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1232/artifact/cov_build/html/index.html
973,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1233/artifact/cov_build/html/index.html
974,vasilyMellanox,bot:retest
975,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1234/artifact/cov_build/html/index.html
976,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1235/artifact/cov_build/html/index.html
977,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1236/artifact/cov_build/html/index.html
978,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1237/artifact/cov_build/html/index.html
979,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1238/artifact/cov_build/html/index.html
980,vasilyMellanox,fixed
981,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1239/artifact/cov_build/html/index.html
982,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1240/artifact/cov_build/html/index.html
983,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1241/artifact/cov_build/html/index.html
984,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1242/artifact/cov_build/html/index.html
985,nitzancarmi,bot:retest
986,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1243/artifact/cov_build/html/index.html
987,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1244/artifact/cov_build/html/index.html
988,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1245/artifact/cov_build/html/index.html
989,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1246/artifact/cov_build/html/index.html
990,vasilyMellanox,bot:retest
991,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1247/artifact/cov_build/html/index.html
992,vasilyMellanox,fixed
993,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1248/artifact/cov_build/html/index.html
994,vasilyMellanox,bot:retest
995,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1249/artifact/cov_build/html/index.html
996,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1250/artifact/cov_build/html/index.html
997,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1251/artifact/cov_build/html/index.html
998,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1252/artifact/cov_build/html/index.html
999,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1253/artifact/cov_build/html/index.html
1000,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1254/artifact/cov_build/html/index.html
1001,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1255/artifact/cov_build/html/index.html
1002,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1256/artifact/cov_build/html/index.html
1003,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1257/artifact/cov_build/html/index.html
1004,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1258/artifact/cov_build/html/index.html
1005,yshestakov,bot:retest
1006,yshestakov,bot:retest  
1007,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1259/artifact/cov_build/html/index.html
1008,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1260/artifact/cov_build/html/index.html
1009,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1261/artifact/cov_build/html/index.html
1010,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1262/artifact/cov_build/html/index.html
1011,nitzancarmi,bot:retest
1012,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1264/artifact/cov_build/html/index.html
1013,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1263/artifact/cov_build/html/index.html
1014,nitzancarmi,bot:retest
1015,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1265/artifact/cov_build/html/index.html
1016,yshestakov,bot:retest
1017,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1266/artifact/cov_build/html/index.html
1018,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1267/artifact/cov_build/html/index.html
1019,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1268/artifact/cov_build/html/index.html
1020,nitzancarmi,bot:retest
1021,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1269/artifact/cov_build/html/index.html
1022,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1270/artifact/cov_build/html/index.html
1023,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1271/artifact/cov_build/html/index.html
1024,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1272/artifact/cov_build/html/index.html
1025,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1273/artifact/cov_build/html/index.html
1026,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1274/artifact/cov_build/html/index.html
1027,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1275/artifact/cov_build/html/index.html
1028,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1276/artifact/cov_build/html/index.html
1029,nitzancarmi,bot:retest
1030,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1277/artifact/cov_build/html/index.html
1031,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1278/artifact/cov_build/html/index.html
1032,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1279/artifact/cov_build/html/index.html
1033,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1280/artifact/cov_build/html/index.html
1034,yshestakov,bot:retest
1035,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1281/artifact/cov_build/html/index.html
1036,yshestakov,bot:retest
1037,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1282/artifact/cov_build/html/index.html
1038,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1284/artifact/cov_build/html/index.html
1039,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1286/artifact/cov_build/html/index.html
1040,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1289/artifact/cov_build/html/index.html
1041,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1290/artifact/cov_build/html/index.html
1042,nitzancarmi,bot:retest
1043,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1291/artifact/cov_build/html/index.html
1044,nitzancarmi,bot:retest
1045,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1292/artifact/cov_build/html/index.html
1046,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1293/artifact/cov_build/html/index.html
1047,umanskymax,bot:retest
1048,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1294/artifact/cov_build/html/index.html
1049,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1295/artifact/cov_build/html/index.html
1050,vasilyMellanox,some issue with GutHub  the commit is already merged by AlexM.  commit 2c04e80f2146daafd17164957a1650616ac72823  Merge: 4eff0d5 59007fd  Author: Alex Mikheev <alexm@mellanox.com>  Date:   Thu Jun 20 11:49:18 2019 +0300        Merge pull request  432 from vasilyMellanox/multi-global-id        Add a possibility to define a several NS global IDs    commit 59007fd6dcbe9d5016c39a108047d9fb99f12304  Author: Vasily Philipov <vasilyf@mellanox.com>  Date:   Sun Jun 2 17:15:11 2019 +0300        A multiple NS global ID support  
1051,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1296/artifact/cov_build/html/index.html
1052,nitzancarmi,bot:retest
1053,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1297/artifact/cov_build/html/index.html
1054,umanskymax,bot:retest
1055,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1298/artifact/cov_build/html/index.html
1056,umanskymax,bot:retest
1057,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1299/artifact/cov_build/html/index.html
1058,umanskymax,bot:retest
1059,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1300/artifact/cov_build/html/index.html
1060,umanskymax,bot:retest
1061,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1301/artifact/cov_build/html/index.html
1062,umanskymax,bot:retest
1063,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1302/artifact/cov_build/html/index.html
1064,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1303/artifact/cov_build/html/index.html
1065,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1304/artifact/cov_build/html/index.html
1066,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1305/artifact/cov_build/html/index.html
1067,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1306/artifact/cov_build/html/index.html
1068,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1307/artifact/cov_build/html/index.html
1069,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1308/artifact/cov_build/html/index.html
1070,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1309/artifact/cov_build/html/index.html
1071,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1310/artifact/cov_build/html/index.html
1072,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1312/artifact/cov_build/html/index.html
1073,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1314/artifact/cov_build/html/index.html
1074,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1315/artifact/cov_build/html/index.html
1075,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1316/artifact/cov_build/html/index.html
1076,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1317/artifact/cov_build/html/index.html
1077,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1318/artifact/cov_build/html/index.html
1078,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1319/artifact/cov_build/html/index.html
1079,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1320/artifact/cov_build/html/index.html
1080,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1321/artifact/cov_build/html/index.html
1081,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1322/artifact/cov_build/html/index.html
1082,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1323/artifact/cov_build/html/index.html
1083,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1324/artifact/cov_build/html/index.html
1084,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1325/artifact/cov_build/html/index.html
1085,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1326/artifact/cov_build/html/index.html
1086,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1327/artifact/cov_build/html/index.html
1087,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1328/artifact/cov_build/html/index.html
1088,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1329/artifact/cov_build/html/index.html
1089,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1330/artifact/cov_build/html/index.html
1090,nitzancarmi,bot:retest
1091,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1331/artifact/cov_build/html/index.html
1092,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1332/artifact/cov_build/html/index.html
1093,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1333/artifact/cov_build/html/index.html
1094,yshestakov,bot:retest
1095,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1334/artifact/cov_build/html/index.html
1096,yshestakov,bot:retest
1097,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1335/artifact/cov_build/html/index.html
1098,yshestakov,bot:retest
1099,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1336/artifact/cov_build/html/index.html
1100,yshestakov,bot:retest
1101,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1337/artifact/cov_build/html/index.html
1102,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1338/artifact/cov_build/html/index.html
1103,nitzancarmi,bot:retest
1104,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1339/artifact/cov_build/html/index.html
1105,yshestakov,bot:retest
1106,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1340/artifact/cov_build/html/index.html
1107,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1341/artifact/cov_build/html/index.html
1108,yshestakov,bot:retest
1109,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1342/artifact/cov_build/html/index.html
1110,yshestakov,bot:retest
1111,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1343/artifact/cov_build/html/index.html
1112,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1344/artifact/cov_build/html/index.html
1113,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1345/artifact/cov_build/html/index.html
1114,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1346/artifact/cov_build/html/index.html
1115,nitzancarmi,bot:retest
1116,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1347/artifact/cov_build/html/index.html
1117,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1348/artifact/cov_build/html/index.html
1118,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1349/artifact/cov_build/html/index.html
1119,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1350/artifact/cov_build/html/index.html
1120,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1351/artifact/cov_build/html/index.html
1121,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1352/artifact/cov_build/html/index.html
1122,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1353/artifact/cov_build/html/index.html
1123,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1354/artifact/cov_build/html/index.html
1124,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1355/artifact/cov_build/html/index.html
1125,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1356/artifact/cov_build/html/index.html
1126,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1357/artifact/cov_build/html/index.html
1127,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1358/artifact/cov_build/html/index.html
1128,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1359/artifact/cov_build/html/index.html
1129,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1360/artifact/cov_build/html/index.html
1130,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1361/artifact/cov_build/html/index.html
1131,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1362/artifact/cov_build/html/index.html
1132,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1363/artifact/cov_build/html/index.html
1133,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1364/artifact/cov_build/html/index.html
1134,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1365/artifact/cov_build/html/index.html
1135,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1366/artifact/cov_build/html/index.html
1136,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1367/artifact/cov_build/html/index.html
1137,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1368/artifact/cov_build/html/index.html
1138,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1369/artifact/cov_build/html/index.html
1139,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1370/artifact/cov_build/html/index.html
1140,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1371/artifact/cov_build/html/index.html
1141,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1372/artifact/cov_build/html/index.html
1142,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1373/artifact/cov_build/html/index.html
1143,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1374/artifact/cov_build/html/index.html
1144,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1375/artifact/cov_build/html/index.html
1145,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1376/artifact/cov_build/html/index.html
1146,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1377/artifact/cov_build/html/index.html
1147,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1378/artifact/cov_build/html/index.html
1148,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1379/artifact/cov_build/html/index.html
1149,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1380/artifact/cov_build/html/index.html
1150,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1381/artifact/cov_build/html/index.html
1151,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1382/artifact/cov_build/html/index.html
1152,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1383/artifact/cov_build/html/index.html
1153,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1384/artifact/cov_build/html/index.html
1154,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1385/artifact/cov_build/html/index.html
1155,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1386/artifact/cov_build/html/index.html
1156,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1387/artifact/cov_build/html/index.html
1157,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1388/artifact/cov_build/html/index.html
1158,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1389/artifact/cov_build/html/index.html
1159,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1390/artifact/cov_build/html/index.html
1160,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1391/artifact/cov_build/html/index.html
1161,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1392/artifact/cov_build/html/index.html
1162,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1393/artifact/cov_build/html/index.html
1163,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1394/artifact/cov_build/html/index.html
1164,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1395/artifact/cov_build/html/index.html
1165,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1396/artifact/cov_build/html/index.html
1166,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1397/artifact/cov_build/html/index.html
1167,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1398/artifact/cov_build/html/index.html
1168,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1399/artifact/cov_build/html/index.html
1169,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1400/artifact/cov_build/html/index.html
1170,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1401/artifact/cov_build/html/index.html
1171,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1402/artifact/cov_build/html/index.html
1172,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1405/artifact/cov_build/html/index.html
1173,yshestakov,Could you please tell me why to push into nvme_snap_next?     8  . 2019  05:56   Max Gurtovoy <notifications@github.com>  :  > *@maxgurtovoy* requested changes on this pull request. > > please push this to the nvme_snap_next branch and also to master (usually > I don't do this  but now we must). > Please build the rpm from nvme_snap_next branch. > ------------------------------ > > In build_scripts/build_and_unittest.sh > <https://github.com/Mellanox/nvmx/pull/489 discussion_r311838039>: > > > @@ -28 7 +28 7 @@ function do_build > >       configure >    ./configure --prefix=/usr/local > -  make  {?_smp_mflags} > +  make > > this is not related to the commit. > lets push only related stuff now. > >   > You are receiving this because you were assigned. > Reply to this email directly  view it on GitHub > <https://github.com/Mellanox/nvmx/pull/489?email_source=notifications email_token=AABIOTTQKZOKCTE3QXFW22LQDODL7A5CNFSM4IKBLVF2YY3PNVWWK3TUL52HS4DFWFIHK3DMKJSXC5LFON2FEZLWNFSXPKTDN5WW2ZLOORPWSZGOCA5UUSY pullrequestreview-272321099>  > or mute the thread > <https://github.com/notifications/unsubscribe-auth/AABIOTUZIYRL23ERHW7LJIDQDODL7ANCNFSM4IKBLVFQ> > . > 
1174,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1406/artifact/cov_build/html/index.html
1175,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1407/artifact/cov_build/html/index.html
1176,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1408/artifact/cov_build/html/index.html
1177,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1409/artifact/cov_build/html/index.html
1178,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1410/artifact/cov_build/html/index.html
1179,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1411/artifact/cov_build/html/index.html
1180,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1412/artifact/cov_build/html/index.html
1181,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1413/artifact/cov_build/html/index.html
1182,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1414/artifact/cov_build/html/index.html
1183,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1415/artifact/cov_build/html/index.html
1184,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1416/artifact/cov_build/html/index.html
1185,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1417/artifact/cov_build/html/index.html
1186,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1418/artifact/cov_build/html/index.html
1187,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1419/artifact/cov_build/html/index.html
1188,yshestakov,bot:retest
1189,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1420/artifact/cov_build/html/index.html
1190,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1421/artifact/cov_build/html/index.html
1191,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1422/artifact/cov_build/html/index.html
1192,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1423/artifact/cov_build/html/index.html
1193,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1424/artifact/cov_build/html/index.html
1194,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1425/artifact/cov_build/html/index.html
1195,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1426/artifact/cov_build/html/index.html
1196,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1427/artifact/cov_build/html/index.html
1197,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1428/artifact/cov_build/html/index.html
1198,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1429/artifact/cov_build/html/index.html
1199,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1430/artifact/cov_build/html/index.html
1200,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1431/artifact/cov_build/html/index.html
1201,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1432/artifact/cov_build/html/index.html
1202,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1433/artifact/cov_build/html/index.html
1203,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1434/artifact/cov_build/html/index.html
1204,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1435/artifact/cov_build/html/index.html
1205,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1436/artifact/cov_build/html/index.html
1206,yshestakov,bot:retest
1207,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1437/artifact/cov_build/html/index.html
1208,yshestakov,bot:retest
1209,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1438/artifact/cov_build/html/index.html
1210,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1439/artifact/cov_build/html/index.html
1211,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1440/artifact/cov_build/html/index.html
1212,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1441/artifact/cov_build/html/index.html
1213,yshestakov,bot:retest
1214,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1442/artifact/cov_build/html/index.html
1215,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1443/artifact/cov_build/html/index.html
1216,yshestakov,bot:retest
1217,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1444/artifact/cov_build/html/index.html
1218,alex-mikheev,bot:retest
1219,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1445/artifact/cov_build/html/index.html
1220,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1446/artifact/cov_build/html/index.html
1221,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1447/artifact/cov_build/html/index.html
1222,maxgurtovoy,bot:retest
1223,yshestakov,bot:retest
1224,hellerguyh,due to upcoming refactoring in mlx_dev_emu it is pointless to do this refactoring at this time.
1225,yshestakov,bot:retest
1226,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1472/artifact/cov_build/html/index.html
1227,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1473/artifact/cov_build/html/index.html
1228,yshestakov,pushed with changes requested by @maxgurtovoy 
1229,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1474/artifact/cov_build/html/index.html
1230,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1475/artifact/cov_build/html/index.html
1231,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1476/artifact/cov_build/html/index.html
1232,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1477/artifact/cov_build/html/index.html
1233,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1478/artifact/cov_build/html/index.html
1234,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1479/artifact/cov_build/html/index.html
1235,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1480/artifact/cov_build/html/index.html
1236,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1481/artifact/cov_build/html/index.html
1237,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1482/artifact/cov_build/html/index.html
1238,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1483/artifact/cov_build/html/index.html
1239,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1484/artifact/cov_build/html/index.html
1240,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1485/artifact/cov_build/html/index.html
1241,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1486/artifact/cov_build/html/index.html
1242,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1487/artifact/cov_build/html/index.html
1243,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1488/artifact/cov_build/html/index.html
1244,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1489/artifact/cov_build/html/index.html
1245,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1490/artifact/cov_build/html/index.html
1246,yshestakov,@maxgurtovoy  JFYI  PR 519 passed 'full regression' tests as well:  http://dev-r-vrt-079-019.mtr.labs.mlnx/blue/organizations/jenkins/nvme-snap-pr-verification/detail/nvme-snap-pr-verification/75/pipeline
1247,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1491/artifact/cov_build/html/index.html
1248,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1492/artifact/cov_build/html/index.html
1249,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1493/artifact/cov_build/html/index.html
1250,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1494/artifact/cov_build/html/index.html
1251,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1495/artifact/cov_build/html/index.html
1252,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1496/artifact/cov_build/html/index.html
1253,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1497/artifact/cov_build/html/index.html
1254,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1498/artifact/cov_build/html/index.html
1255,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1499/artifact/cov_build/html/index.html
1256,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1500/artifact/cov_build/html/index.html
1257,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1501/artifact/cov_build/html/index.html
1258,alex-mikheev,:bot:retest
1259,alex-mikheev,bot:retest
1260,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1502/artifact/cov_build/html/index.html
1261,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1503/artifact/cov_build/html/index.html
1262,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1504/artifact/cov_build/html/index.html
1263,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1505/artifact/cov_build/html/index.html
1264,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1506/artifact/cov_build/html/index.html
1265,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1507/artifact/cov_build/html/index.html
1266,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1508/artifact/cov_build/html/index.html
1267,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1509/artifact/cov_build/html/index.html
1268,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1510/artifact/cov_build/html/index.html
1269,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1511/artifact/cov_build/html/index.html
1270,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1512/artifact/cov_build/html/index.html
1271,swx-jenkins2,Can one of the admins verify this patch?
1272,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1513/artifact/cov_build/html/index.html
1273,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1514/artifact/cov_build/html/index.html
1274,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1515/artifact/cov_build/html/index.html
1275,nitzancarmi,Duplicate
1276,nitzancarmi,Duplicate
1277,nitzancarmi,Old - Will be renewed over the new code
1278,nitzancarmi,Duplicate
1279,nitzancarmi,Duplicate
1280,nitzancarmi,Old - no dummy controllers anymore
1281,nitzancarmi,old - no dummy controllers anymore
1282,nitzancarmi,old code
1283,nitzancarmi,old
1284,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1516/artifact/cov_build/html/index.html
1285,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1517/artifact/cov_build/html/index.html
1286,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1518/artifact/cov_build/html/index.html
1287,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1519/artifact/cov_build/html/index.html
1288,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1520/artifact/cov_build/html/index.html
1289,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1521/artifact/cov_build/html/index.html
1290,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1522/artifact/cov_build/html/index.html
1291,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1523/artifact/cov_build/html/index.html
1292,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1524/artifact/cov_build/html/index.html
1293,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1525/artifact/cov_build/html/index.html
1294,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1526/artifact/cov_build/html/index.html
1295,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1527/artifact/cov_build/html/index.html
1296,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1528/artifact/cov_build/html/index.html
1297,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1529/artifact/cov_build/html/index.html
1298,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1530/artifact/cov_build/html/index.html
1299,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1531/artifact/cov_build/html/index.html
1300,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1532/artifact/cov_build/html/index.html
1301,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1533/artifact/cov_build/html/index.html
1302,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1534/artifact/cov_build/html/index.html
1303,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1535/artifact/cov_build/html/index.html
1304,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1536/artifact/cov_build/html/index.html
1305,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1537/artifact/cov_build/html/index.html
1306,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1538/artifact/cov_build/html/index.html
1307,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1539/artifact/cov_build/html/index.html
1308,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1540/artifact/cov_build/html/index.html
1309,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1541/artifact/cov_build/html/index.html
1310,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1542/artifact/cov_build/html/index.html
1311,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1543/artifact/cov_build/html/index.html
1312,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1544/artifact/cov_build/html/index.html
1313,nitzancarmi,@snimrod Tell me if anything is still missing in the explanation.
1314,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1545/artifact/cov_build/html/index.html
1315,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1546/artifact/cov_build/html/index.html
1316,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1547/artifact/cov_build/html/index.html
1317,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1548/artifact/cov_build/html/index.html
1318,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1549/artifact/cov_build/html/index.html
1319,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1550/artifact/cov_build/html/index.html
1320,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1551/artifact/cov_build/html/index.html
1321,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1552/artifact/cov_build/html/index.html
1322,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1553/artifact/cov_build/html/index.html
1323,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1554/artifact/cov_build/html/index.html
1324,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1555/artifact/cov_build/html/index.html
1325,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1556/artifact/cov_build/html/index.html
1326,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1557/artifact/cov_build/html/index.html
1327,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1558/artifact/cov_build/html/index.html
1328,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1559/artifact/cov_build/html/index.html
1329,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1560/artifact/cov_build/html/index.html
1330,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1561/artifact/cov_build/html/index.html
1331,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1562/artifact/cov_build/html/index.html
1332,nitzancarmi,Will be rebased over snap-3.0 branch
1333,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1563/artifact/cov_build/html/index.html
1334,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1564/artifact/cov_build/html/index.html
1335,nitzancarmi,@alex-mikheev I added  Tests:  section and fixed the issues. Please review when available :-)
1336,umanskymax,bot:retest
1337,alex-mikheev,bot:retest  
1338,yshestakov,bot:retest
1339,maxgurtovoy,bot:retest
1340,yshestakov,bot:retest
1341,yshestakov,bot:retest
1342,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1578/artifact/cov_build/html/index.html
1343,alex-mikheev,bot:retest
1344,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1579/artifact/cov_build/html/index.html
1345,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1580/artifact/cov_build/html/index.html
1346,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1581/artifact/cov_build/html/index.html
1347,alex-mikheev,bot:retest
1348,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1584/artifact/cov_build/html/index.html
1349,alex-mikheev,bot:retest
1350,alex-mikheev,@nitzancarmi  now need to fix tests
1351,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1586/artifact/cov_build/html/index.html
1352,nitzancarmi,bot:retest
1353,nitzancarmi,@maxgurtovoy I added a seperate commit for fixinf main.c error flows for easier reading   and also squashed the  dummy  nvme_config commit (with only a pointer to the old json file)  with the  ctrl.*  params implementations.  In addition  fixed all coverity issues.
1354,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1593/artifact/cov_build/html/index.html
1355,nitzancarmi,bot:retest
1356,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1595/artifact/cov_build/html/index.html
1357,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1596/artifact/cov_build/html/index.html
1358,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1597/artifact/cov_build/html/index.html
1359,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1598/artifact/cov_build/html/index.html
1360,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1601/artifact/cov_build/html/index.html
1361,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1602/artifact/cov_build/html/index.html
1362,swx-jenkins2,Can one of the admins verify this patch?
1363,swx-jenkins2,Can one of the admins verify this patch?
1364,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1603/artifact/cov_build/html/index.html
1365,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1604/artifact/cov_build/html/index.html
1366,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1606/artifact/cov_build/html/index.html
1367,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1607/artifact/cov_build/html/index.html
1368,alex-mikheev,you have to fix test config file. They set emu_type db_only. Checkout tests/README and in particular  Invocation of gtest_nvme_ctrl 
1369,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1608/artifact/cov_build/html/index.html
1370,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1609/artifact/cov_build/html/index.html
1371,swx-jenkins2,Can one of the admins verify this patch?
1372,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1612/artifact/cov_build/html/index.html
1373,nitzancarmi,@maxgurtovoy  Can you review this patch please?
1374,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1613/artifact/cov_build/html/index.html
1375,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1614/artifact/cov_build/html/index.html
1376,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1615/artifact/cov_build/html/index.html
1377,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1616/artifact/cov_build/html/index.html
1378,swx-jenkins2,Can one of the admins verify this patch?
1379,swx-jenkins2,Can one of the admins verify this patch?
1380,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1617/artifact/cov_build/html/index.html
1381,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1619/artifact/cov_build/html/index.html
1382,umanskymax,bot:retest
1383,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1620/artifact/cov_build/html/index.html
1384,umanskymax,bot:retest
1385,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1621/artifact/cov_build/html/index.html
1386,Tom-Wu-2019,bot:retest
1387,Tom-Wu-2019,bot:retest
1388,Tom-Wu-2019,bot:retest
1389,Tom-Wu-2019,bot:retest
1390,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1622/artifact/cov_build/html/index.html
1391,yshestakov,Need to create separate PRs for C code fixes and debian/* 
1392,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1623/artifact/cov_build/html/index.html
1393,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1624/artifact/cov_build/html/index.html
1394,swx-jenkins2,Can one of the admins verify this patch?
1395,umanskymax,bot:retest
1396,umanskymax,bot:retest
1397,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1625/artifact/cov_build/html/index.html
1398,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1626/artifact/cov_build/html/index.html
1399,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1627/artifact/cov_build/html/index.html
1400,Tom-Wu-2019,bot:retest
1401,Tom-Wu-2019,bot:retest
1402,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1628/artifact/cov_build/html/index.html
1403,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1629/artifact/cov_build/html/index.html
1404,umanskymax,bot:retest
1405,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1630/artifact/cov_build/html/index.html
1406,nitzancarmi,We should add device_id 0x1042 to scripts/pci_ids.h.  This is the PCI device ID for non-transactional devices  and it is the only device ID that we support for now.
1407,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1631/artifact/cov_build/html/index.html
1408,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1632/artifact/cov_build/html/index.html
1409,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1633/artifact/cov_build/html/index.html
1410,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1634/artifact/cov_build/html/index.html
1411,alex-mikheev,bot:retest
1412,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1635/artifact/cov_build/html/index.html
1413,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1636/artifact/cov_build/html/index.html
1414,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1637/artifact/cov_build/html/index.html
1415,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1638/artifact/cov_build/html/index.html
1416,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1639/artifact/cov_build/html/index.html
1417,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1640/artifact/cov_build/html/index.html
1418,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1641/artifact/cov_build/html/index.html
1419,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1642/artifact/cov_build/html/index.html
1420,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1643/artifact/cov_build/html/index.html
1421,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1644/artifact/cov_build/html/index.html
1422,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1645/artifact/cov_build/html/index.html
1423,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1646/artifact/cov_build/html/index.html
1424,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1647/artifact/cov_build/html/index.html
1425,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1649/artifact/cov_build/html/index.html
1426,nitzancarmi,bot:retest
1427,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1650/artifact/cov_build/html/index.html
1428,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1651/artifact/cov_build/html/index.html
1429,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1652/artifact/cov_build/html/index.html
1430,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1653/artifact/cov_build/html/index.html
1431,nitzancarmi,bot:retest
1432,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1654/artifact/cov_build/html/index.html
1433,nitzancarmi,bot:retest
1434,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1655/artifact/cov_build/html/index.html
1435,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1656/artifact/cov_build/html/index.html
1436,yshestakov,bot:retest
1437,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1657/artifact/cov_build/html/index.html
1438,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1658/artifact/cov_build/html/index.html
1439,umanskymax,bot:retest
1440,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1659/artifact/cov_build/html/index.html
1441,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1660/artifact/cov_build/html/index.html
1442,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1661/artifact/cov_build/html/index.html
1443,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1662/artifact/cov_build/html/index.html
1444,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1663/artifact/cov_build/html/index.html
1445,nitzancarmi,bot:retest
1446,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1664/artifact/cov_build/html/index.html
1447,nitzancarmi,bot:retest
1448,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1665/artifact/cov_build/html/index.html
1449,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1666/artifact/cov_build/html/index.html
1450,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1667/artifact/cov_build/html/index.html
1451,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1668/artifact/cov_build/html/index.html
1452,alex-mikheev,dma lib is going to be a part of the snap-rdma. PR is obsolete
1453,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1669/artifact/cov_build/html/index.html
1454,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1670/artifact/cov_build/html/index.html
1455,nitzancarmi,@maxgurtovoy @alex-mikheev After Yurii's fix to snap-3.0 build  the commits passed test.
1456,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1672/artifact/cov_build/html/index.html
1457,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1673/artifact/cov_build/html/index.html
1458,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1674/artifact/cov_build/html/index.html
1459,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1675/artifact/cov_build/html/index.html
1460,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1676/artifact/cov_build/html/index.html
1461,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1677/artifact/cov_build/html/index.html
1462,maxgurtovoy,Yurii  please mention in the commit message why you remove it ? I don't understand where it come from ?
1463,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1678/artifact/cov_build/html/index.html
1464,yshestakov,> Yurii  please mention in the commit message why you remove it ? I don't understand where it come from ?    The `nvmx-fw` package is (was) used to bring FW images to SNIC before we get it provided by BFB images
1465,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1679/artifact/cov_build/html/index.html
1466,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1680/artifact/cov_build/html/index.html
1467,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1681/artifact/cov_build/html/index.html
1468,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1682/artifact/cov_build/html/index.html
1469,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1683/artifact/cov_build/html/index.html
1470,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1684/artifact/cov_build/html/index.html
1471,alex-mikheev,bot:retest
1472,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1686/artifact/cov_build/html/index.html
1473,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1687/artifact/cov_build/html/index.html
1474,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/4/artifact/cov_build/html/index.html
1475,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/2/artifact/cov_build/html/index.html
1476,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/9/artifact/cov_build/html/index.html
1477,yshestakov,bot:retest
1478,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1688/artifact/cov_build/html/index.html
1479,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/22/artifact/cov_build/html/index.html
1480,yshestakov,bot:retest
1481,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1689/artifact/cov_build/html/index.html
1482,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/23/artifact/cov_build/html/index.html
1483,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1690/artifact/cov_build/html/index.html
1484,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/24/artifact/cov_build/html/index.html
1485,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1691/artifact/cov_build/html/index.html
1486,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/25/artifact/cov_build/html/index.html
1487,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1692/artifact/cov_build/html/index.html
1488,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/26/artifact/cov_build/html/index.html
1489,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1693/artifact/cov_build/html/index.html
1490,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/27/artifact/cov_build/html/index.html
1491,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1694/artifact/cov_build/html/index.html
1492,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/28/artifact/cov_build/html/index.html
1493,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/29/artifact/cov_build/html/index.html
1494,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1695/artifact/cov_build/html/index.html
1495,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1696/artifact/cov_build/html/index.html
1496,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/30/artifact/cov_build/html/index.html
1497,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1697/artifact/cov_build/html/index.html
1498,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/31/artifact/cov_build/html/index.html
1499,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1698/artifact/cov_build/html/index.html
1500,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/32/artifact/cov_build/html/index.html
1501,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1699/artifact/cov_build/html/index.html
1502,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/33/artifact/cov_build/html/index.html
1503,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1700/artifact/cov_build/html/index.html
1504,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/34/artifact/cov_build/html/index.html
1505,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1701/artifact/cov_build/html/index.html
1506,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/35/artifact/cov_build/html/index.html
1507,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1702/artifact/cov_build/html/index.html
1508,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/36/artifact/cov_build/html/index.html
1509,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1703/artifact/cov_build/html/index.html
1510,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/37/artifact/cov_build/html/index.html
1511,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1704/artifact/cov_build/html/index.html
1512,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/38/artifact/cov_build/html/index.html
1513,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1705/artifact/cov_build/html/index.html
1514,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/39/artifact/cov_build/html/index.html
1515,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1706/artifact/cov_build/html/index.html
1516,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/40/artifact/cov_build/html/index.html
1517,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1707/artifact/cov_build/html/index.html
1518,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1708/artifact/cov_build/html/index.html
1519,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/41/artifact/cov_build/html/index.html
1520,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1709/artifact/cov_build/html/index.html
1521,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/42/artifact/cov_build/html/index.html
1522,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1710/artifact/cov_build/html/index.html
1523,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/43/artifact/cov_build/html/index.html
1524,alex-mikheev,bot:retest
1525,alex-mikheev,bot:retest
1526,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1711/artifact/cov_build/html/index.html
1527,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1712/artifact/cov_build/html/index.html
1528,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/44/artifact/cov_build/html/index.html
1529,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/45/artifact/cov_build/html/index.html
1530,alex-mikheev,bot:retest
1531,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1713/artifact/cov_build/html/index.html
1532,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/46/artifact/cov_build/html/index.html
1533,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1714/artifact/cov_build/html/index.html
1534,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/47/artifact/cov_build/html/index.html
1535,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1715/artifact/cov_build/html/index.html
1536,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/48/artifact/cov_build/html/index.html
1537,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1716/artifact/cov_build/html/index.html
1538,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/49/artifact/cov_build/html/index.html
1539,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1717/artifact/cov_build/html/index.html
1540,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/50/artifact/cov_build/html/index.html
1541,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/51/artifact/cov_build/html/index.html
1542,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1718/artifact/cov_build/html/index.html
1543,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1719/artifact/cov_build/html/index.html
1544,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/52/artifact/cov_build/html/index.html
1545,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1720/artifact/cov_build/html/index.html
1546,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/53/artifact/cov_build/html/index.html
1547,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1721/artifact/cov_build/html/index.html
1548,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/54/artifact/cov_build/html/index.html
1549,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1722/artifact/cov_build/html/index.html
1550,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/55/artifact/cov_build/html/index.html
1551,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/56/artifact/cov_build/html/index.html
1552,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1723/artifact/cov_build/html/index.html
1553,nitzancarmi,@alex-mikheev  @maxgurtovoy - This is a re-work for some needed preparations before I push the virtio-blk Application implementation. Please review :-).
1554,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1724/artifact/cov_build/html/index.html
1555,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/57/artifact/cov_build/html/index.html
1556,nitzancarmi,Won't fix for BF1
1557,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1725/artifact/cov_build/html/index.html
1558,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/58/artifact/cov_build/html/index.html
1559,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1726/artifact/cov_build/html/index.html
1560,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/59/artifact/cov_build/html/index.html
1561,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1727/artifact/cov_build/html/index.html
1562,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/60/artifact/cov_build/html/index.html
1563,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1730/artifact/cov_build/html/index.html
1564,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/63/artifact/cov_build/html/index.html
1565,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/64/artifact/cov_build/html/index.html
1566,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1731/artifact/cov_build/html/index.html
1567,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/65/artifact/cov_build/html/index.html
1568,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/66/artifact/cov_build/html/index.html
1569,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1733/artifact/cov_build/html/index.html
1570,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/68/artifact/cov_build/html/index.html
1571,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1735/artifact/cov_build/html/index.html
1572,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1736/artifact/cov_build/html/index.html
1573,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1737/artifact/cov_build/html/index.html
1574,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/69/artifact/cov_build/html/index.html
1575,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/70/artifact/cov_build/html/index.html
1576,alex-mikheev,bot:retest
1577,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1738/artifact/cov_build/html/index.html
1578,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/71/artifact/cov_build/html/index.html
1579,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1739/artifact/cov_build/html/index.html
1580,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/72/artifact/cov_build/html/index.html
1581,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1741/artifact/cov_build/html/index.html
1582,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/74/artifact/cov_build/html/index.html
1583,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1742/artifact/cov_build/html/index.html
1584,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/75/artifact/cov_build/html/index.html
1585,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1743/artifact/cov_build/html/index.html
1586,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/76/artifact/cov_build/html/index.html
1587,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1744/artifact/cov_build/html/index.html
1588,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/77/artifact/cov_build/html/index.html
1589,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/78/artifact/cov_build/html/index.html
1590,nitzancarmi,bot:retest
1591,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/79/artifact/cov_build/html/index.html
1592,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/80/artifact/cov_build/html/index.html
1593,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/81/artifact/cov_build/html/index.html
1594,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/82/artifact/cov_build/html/index.html
1595,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/83/artifact/cov_build/html/index.html
1596,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/84/artifact/cov_build/html/index.html
1597,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/85/artifact/cov_build/html/index.html
1598,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/86/artifact/cov_build/html/index.html
1599,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/89/artifact/cov_build/html/index.html
1600,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/91/artifact/cov_build/html/index.html
1601,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/93/artifact/cov_build/html/index.html
1602,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1745/artifact/cov_build/html/index.html
1603,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1746/artifact/cov_build/html/index.html
1604,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1747/artifact/cov_build/html/index.html
1605,nitzancarmi,@umanskymax Can you please re-push it to snap-3.0 branch too and verify there are no changes needed there?
1606,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/96/artifact/cov_build/html/index.html
1607,nitzancarmi,bot:retest
1608,umanskymax,> @umanskymax Can you please re-push it to snap-3.0 branch too and verify there are no changes needed there?    Ok
1609,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/98/artifact/cov_build/html/index.html
1610,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/99/artifact/cov_build/html/index.html
1611,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/101/artifact/cov_build/html/index.html
1612,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/104/artifact/cov_build/html/index.html
1613,Tom-Wu-2019,bot:retest
1614,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/105/artifact/cov_build/html/index.html
1615,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/106/artifact/cov_build/html/index.html
1616,Tom-Wu-2019,bot:retest
1617,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/107/artifact/cov_build/html/index.html
1618,nitzancarmi,bot:retest
1619,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/108/artifact/cov_build/html/index.html
1620,nitzancarmi,@yshestakov Can you please approve?
1621,alex-mikheev,bot:retest
1622,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/111/artifact/cov_build/html/index.html
1623,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/112/artifact/cov_build/html/index.html
1624,nitzancarmi,bot:retest
1625,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/113/artifact/cov_build/html/index.html
1626,nitzancarmi,bot:retest
1627,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/114/artifact/cov_build/html/index.html
1628,nitzancarmi,bot:retest
1629,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/115/artifact/cov_build/html/index.html
1630,nitzancarmi,bot:retest
1631,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/117/artifact/cov_build/html/index.html
1632,alex-mikheev,bot:retest
1633,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/119/artifact/cov_build/html/index.html
1634,alex-mikheev,bot:retest
1635,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/122/artifact/cov_build/html/index.html
1636,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/123/artifact/cov_build/html/index.html
1637,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/124/artifact/cov_build/html/index.html
1638,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/126/artifact/cov_build/html/index.html
1639,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/127/artifact/cov_build/html/index.html
1640,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/128/artifact/cov_build/html/index.html
1641,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/130/artifact/cov_build/html/index.html
1642,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/131/artifact/cov_build/html/index.html
1643,Tom-Wu-2019,bot:retest
1644,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/132/artifact/cov_build/html/index.html
1645,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/133/artifact/cov_build/html/index.html
1646,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1749/artifact/cov_build/html/index.html
1647,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1750/artifact/cov_build/html/index.html
1648,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1751/artifact/cov_build/html/index.html
1649,nitzancarmi,We don't want to use json file anyway
1650,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/136/artifact/cov_build/html/index.html
1651,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/137/artifact/cov_build/html/index.html
1652,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/138/artifact/cov_build/html/index.html
1653,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/139/artifact/cov_build/html/index.html
1654,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/140/artifact/cov_build/html/index.html
1655,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/141/artifact/cov_build/html/index.html
1656,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/142/artifact/cov_build/html/index.html
1657,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/144/artifact/cov_build/html/index.html
1658,umanskymax,bot:retest
1659,umanskymax,bot:retest
1660,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/145/artifact/cov_build/html/index.html
1661,umanskymax,bot:retest
1662,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1752/artifact/cov_build/html/index.html
1663,umanskymax,bot:retest
1664,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1754/artifact/cov_build/html/index.html
1665,andrii-holovchenko,bot:retest
1666,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1755/artifact/cov_build/html/index.html
1667,andrii-holovchenko,bot:retest
1668,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1756/artifact/cov_build/html/index.html
1669,andrii-holovchenko,bot:retest
1670,andrii-holovchenko,bot:retest
1671,andrii-holovchenko,bot:retest
1672,andrii-holovchenko,bot:retest
1673,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1760/artifact/cov_build/html/index.html
1674,andrii-holovchenko,bot:retest
1675,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/146/artifact/cov_build/html/index.html
1676,andrii-holovchenko,bot:retest
1677,andrii-holovchenko,bot:retest
1678,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/149/artifact/cov_build/html/index.html
1679,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1762/artifact/cov_build/html/index.html
1680,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1763/artifact/cov_build/html/index.html
1681,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/151/artifact/cov_build/html/index.html
1682,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/154/artifact/cov_build/html/index.html
1683,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/155/artifact/cov_build/html/index.html
1684,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/156/artifact/cov_build/html/index.html
1685,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/157/artifact/cov_build/html/index.html
1686,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/158/artifact/cov_build/html/index.html
1687,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/159/artifact/cov_build/html/index.html
1688,mike-dubman,need to add flag to jenkins tests
1689,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/160/artifact/cov_build/html/index.html
1690,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/161/artifact/cov_build/html/index.html
1691,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/162/artifact/cov_build/html/index.html
1692,yshestakov,bot:retest  `centos75-nvme-snap-3.0:latest` was updated to include spdk 
1693,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/163/artifact/cov_build/html/index.html
1694,yshestakov,bot:retest
1695,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/165/artifact/cov_build/html/index.html
1696,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/166/artifact/cov_build/html/index.html
1697,andrii-holovchenko,bot:retest
1698,yshestakov,> why needed? script is executed after autogen anyway    `./autogen.sh` does `git submodule init ; git submodule update`. But I don't want to compile `googletest` framework when I need to build binary RPM only or run COVERITY check  
1699,andrii-holovchenko,bot:retest
1700,mike-dubman,> > why needed? script is executed after autogen anyway  >   > `./autogen.sh` does `git submodule init ; git submodule update`. But I don't want to compile `googletest` framework when I need to build binary RPM only or run COVERITY check    git submodule stuff should be part of autogen.sh
1701,andrii-holovchenko,bot:retest
1702,andrii-holovchenko,bot:retest
1703,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/173/artifact/cov_build/html/index.html
1704,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/174/artifact/cov_build/html/index.html
1705,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/175/artifact/cov_build/html/index.html
1706,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/176/artifact/cov_build/html/index.html
1707,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/177/artifact/cov_build/html/index.html
1708,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/178/artifact/cov_build/html/index.html
1709,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1764/artifact/cov_build/html/index.html
1710,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/179/artifact/cov_build/html/index.html
1711,yshestakov,bot:retest
1712,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1765/artifact/cov_build/html/index.html
1713,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/181/artifact/cov_build/html/index.html
1714,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/182/artifact/cov_build/html/index.html
1715,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/183/artifact/cov_build/html/index.html
1716,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1766/artifact/cov_build/html/index.html
1717,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/184/artifact/cov_build/html/index.html
1718,yshestakov,bot:retest
1719,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/185/artifact/cov_build/html/index.html
1720,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/186/artifact/cov_build/html/index.html
1721,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/187/artifact/cov_build/html/index.html
1722,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/188/artifact/cov_build/html/index.html
1723,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/189/artifact/cov_build/html/index.html
1724,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/190/artifact/cov_build/html/index.html
1725,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1767/artifact/cov_build/html/index.html
1726,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/191/artifact/cov_build/html/index.html
1727,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/192/artifact/cov_build/html/index.html
1728,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/193/artifact/cov_build/html/index.html
1729,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/194/artifact/cov_build/html/index.html
1730,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/195/artifact/cov_build/html/index.html
1731,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/196/artifact/cov_build/html/index.html
1732,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/197/artifact/cov_build/html/index.html
1733,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/198/artifact/cov_build/html/index.html
1734,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/199/artifact/cov_build/html/index.html
1735,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/200/artifact/cov_build/html/index.html
1736,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/201/artifact/cov_build/html/index.html
1737,nitzancarmi,@umanskymax Shlomi approved this naming convention.  Can you please approve this patch?
1738,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/202/artifact/cov_build/html/index.html
1739,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/203/artifact/cov_build/html/index.html
1740,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/204/artifact/cov_build/html/index.html
1741,andrii-holovchenko,bot:retest
1742,andrii-holovchenko,bot:retest
1743,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1768/artifact/cov_build/html/index.html
1744,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/206/artifact/cov_build/html/index.html
1745,nitzancarmi,There are some compilation issues with modify_sq. Please solve them
1746,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1769/artifact/cov_build/html/index.html
1747,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1770/artifact/cov_build/html/index.html
1748,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1771/artifact/cov_build/html/index.html
1749,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/208/artifact/cov_build/html/index.html
1750,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1772/artifact/cov_build/html/index.html
1751,yshestakov,bot:retest
1752,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1773/artifact/cov_build/html/index.html
1753,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/209/artifact/cov_build/html/index.html
1754,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/210/artifact/cov_build/html/index.html
1755,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/211/artifact/cov_build/html/index.html
1756,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/212/artifact/cov_build/html/index.html
1757,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1774/artifact/cov_build/html/index.html
1758,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1775/artifact/cov_build/html/index.html
1759,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1776/artifact/cov_build/html/index.html
1760,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/213/artifact/cov_build/html/index.html
1761,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/214/artifact/cov_build/html/index.html
1762,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/215/artifact/cov_build/html/index.html
1763,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/216/artifact/cov_build/html/index.html
1764,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/217/artifact/cov_build/html/index.html
1765,nitzancarmi,Replaced with  650 
1766,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/218/artifact/cov_build/html/index.html
1767,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/219/artifact/cov_build/html/index.html
1768,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/220/artifact/cov_build/html/index.html
1769,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/222/artifact/cov_build/html/index.html
1770,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/223/artifact/cov_build/html/index.html
1771,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/224/artifact/cov_build/html/index.html
1772,yshestakov,bot:retest
1773,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/225/artifact/cov_build/html/index.html
1774,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/226/artifact/cov_build/html/index.html
1775,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/227/artifact/cov_build/html/index.html
1776,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/228/artifact/cov_build/html/index.html
1777,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/229/artifact/cov_build/html/index.html
1778,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvme-snap-3.0-pipeline/230/artifact/cov_build/html/index.html
1779,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/1/artifact/cov_build/html/index.html
1780,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/5/artifact/cov_build/html/index.html
1781,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/6/artifact/cov_build/html/index.html
1782,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/2/artifact/cov_build/html/index.html
1783,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/3/artifact/cov_build/html/index.html
1784,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/7/artifact/cov_build/html/index.html
1785,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/8/artifact/cov_build/html/index.html
1786,nitzancarmi,bot:retest
1787,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/9/artifact/cov_build/html/index.html
1788,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/10/artifact/cov_build/html/index.html
1789,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/11/artifact/cov_build/html/index.html
1790,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/12/artifact/cov_build/html/index.html
1791,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/13/artifact/cov_build/html/index.html
1792,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/14/artifact/cov_build/html/index.html
1793,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/16/artifact/cov_build/html/index.html
1794,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/17/artifact/cov_build/html/index.html
1795,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/18/artifact/cov_build/html/index.html
1796,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/19/artifact/cov_build/html/index.html
1797,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/20/artifact/cov_build/html/index.html
1798,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/21/artifact/cov_build/html/index.html
1799,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/22/artifact/cov_build/html/index.html
1800,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/23/artifact/cov_build/html/index.html
1801,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/24/artifact/cov_build/html/index.html
1802,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/25/artifact/cov_build/html/index.html
1803,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/26/artifact/cov_build/html/index.html
1804,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/27/artifact/cov_build/html/index.html
1805,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/28/artifact/cov_build/html/index.html
1806,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/29/artifact/cov_build/html/index.html
1807,andrii-holovchenko,bot:retest
1808,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/30/artifact/cov_build/html/index.html
1809,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/31/artifact/cov_build/html/index.html
1810,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline-andriih/1/artifact/cov_build/html/index.html
1811,andrii-holovchenko,bot:retest
1812,andrii-holovchenko,bot:retest
1813,andrii-holovchenko,bot:retest
1814,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/32/artifact/cov_build/html/index.html
1815,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/34/artifact/cov_build/html/index.html
1816,swx-jenkins2,Can one of the admins verify this patch?
1817,andrii-holovchenko,bot:retest
1818,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/35/artifact/cov_build/html/index.html
1819,nitzancarmi,Seems good. Just please add  Tests  section to commit messages and rebase over latest code  and it's ok.
1820,andrii-holovchenko,bot:retest
1821,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/36/artifact/cov_build/html/index.html
1822,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/37/artifact/cov_build/html/index.html
1823,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/38/artifact/cov_build/html/index.html
1824,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/39/artifact/cov_build/html/index.html
1825,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/40/artifact/cov_build/html/index.html
1826,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/41/artifact/cov_build/html/index.html
1827,nitzancarmi,bot:retest
1828,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/43/artifact/cov_build/html/index.html
1829,nitzancarmi,bot:retest
1830,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/44/artifact/cov_build/html/index.html
1831,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/45/artifact/cov_build/html/index.html
1832,nitzancarmi,bot:retest
1833,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/46/artifact/cov_build/html/index.html
1834,snimrod,bot:retest
1835,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/47/artifact/cov_build/html/index.html
1836,alex-mikheev,:bot:retest
1837,alex-mikheev,bot:retest
1838,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/48/artifact/cov_build/html/index.html
1839,nitzancarmi,bot:retest
1840,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/50/artifact/cov_build/html/index.html
1841,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/51/artifact/cov_build/html/index.html
1842,alex-mikheev,bot:retest
1843,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/52/artifact/cov_build/html/index.html
1844,nitzancarmi,bot:retest
1845,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/53/artifact/cov_build/html/index.html
1846,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/54/artifact/cov_build/html/index.html
1847,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/55/artifact/cov_build/html/index.html
1848,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/56/artifact/cov_build/html/index.html
1849,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/57/artifact/cov_build/html/index.html
1850,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/58/artifact/cov_build/html/index.html
1851,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/59/artifact/cov_build/html/index.html
1852,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/60/artifact/cov_build/html/index.html
1853,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/61/artifact/cov_build/html/index.html
1854,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/62/artifact/cov_build/html/index.html
1855,andrii-holovchenko,bot:retest
1856,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/63/artifact/cov_build/html/index.html
1857,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline-andriih/1/artifact/cov_build/html/index.html
1858,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline-andriih/4/artifact/cov_build/html/index.html
1859,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/64/artifact/cov_build/html/index.html
1860,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline-andriih/9/artifact/cov_build/html/index.html
1861,andrii-holovchenko,bot:retest
1862,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/65/artifact/cov_build/html/index.html
1863,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline-andriih/10/artifact/cov_build/html/index.html
1864,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/66/artifact/cov_build/html/index.html
1865,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline-andriih/11/artifact/cov_build/html/index.html
1866,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/68/artifact/cov_build/html/index.html
1867,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline-andriih/13/artifact/cov_build/html/index.html
1868,swx-jenkins2,Can one of the admins verify this patch?
1869,nitzancarmi,bot:retest
1870,umanskymax,bot:retest
1871,andrii-holovchenko,bot:retest
1872,andrii-holovchenko,bot:retest
1873,andrii-holovchenko,bot:retest
1874,andrii-holovchenko,bot:retest
1875,snimrod,bot:retest
1876,nitzancarmi,bot:retest
1877,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/77/artifact/cov_build/html/index.html
1878,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/78/artifact/cov_build/html/index.html
1879,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/79/artifact/cov_build/html/index.html
1880,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/80/artifact/cov_build/html/index.html
1881,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/81/artifact/cov_build/html/index.html
1882,nitzancarmi,bot:retest
1883,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/82/artifact/cov_build/html/index.html
1884,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/83/artifact/cov_build/html/index.html
1885,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/84/artifact/cov_build/html/index.html
1886,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/85/artifact/cov_build/html/index.html
1887,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/86/artifact/cov_build/html/index.html
1888,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/87/artifact/cov_build/html/index.html
1889,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/88/artifact/cov_build/html/index.html
1890,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/89/artifact/cov_build/html/index.html
1891,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/90/artifact/cov_build/html/index.html
1892,hellerguyh,> It is better not to merge before release.  >   > and we should test (offload  spdk) modes on both eth and ib  >   > devx_compat/devx_verbs should not be needed anymore.    Done Ethernet checks  why IB is needed as well?   If need to remove more code we can do it in another commit
1893,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/92/artifact/cov_build/html/index.html
1894,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/93/artifact/cov_build/html/index.html
1895,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/94/artifact/cov_build/html/index.html
1896,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/95/artifact/cov_build/html/index.html
1897,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/96/artifact/cov_build/html/index.html
1898,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/97/artifact/cov_build/html/index.html
1899,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/98/artifact/cov_build/html/index.html
1900,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/99/artifact/cov_build/html/index.html
1901,nitzancarmi,@alex-mikheev fixed. Please review again.
1902,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/100/artifact/cov_build/html/index.html
1903,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/101/artifact/cov_build/html/index.html
1904,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/102/artifact/cov_build/html/index.html
1905,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/103/artifact/cov_build/html/index.html
1906,nitzancarmi,@andrii-holovchenko please see you don't have any objections  and let's merge this
1907,alex-mikheev,+Yuri @yshestakov 
1908,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1778/artifact/cov_build/html/index.html
1909,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1779/artifact/cov_build/html/index.html
1910,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1780/artifact/cov_build/html/index.html
1911,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1781/artifact/cov_build/html/index.html
1912,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1783/artifact/cov_build/html/index.html
1913,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1784/artifact/cov_build/html/index.html
1914,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/104/artifact/cov_build/html/index.html
1915,andrii-holovchenko,bot:retest
1916,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1785/artifact/cov_build/html/index.html
1917,andrii-holovchenko,bot:retest
1918,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1787/artifact/cov_build/html/index.html
1919,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/105/artifact/cov_build/html/index.html
1920,andrii-holovchenko,bot:retest
1921,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/106/artifact/cov_build/html/index.html
1922,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/107/artifact/cov_build/html/index.html
1923,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1789/artifact/cov_build/html/index.html
1924,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/108/artifact/cov_build/html/index.html
1925,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/109/artifact/cov_build/html/index.html
1926,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/110/artifact/cov_build/html/index.html
1927,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/111/artifact/cov_build/html/index.html
1928,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-mlnx-snap-3.0-pipeline/112/artifact/cov_build/html/index.html
1929,nitzancarmi,> @nitzancarmi @snimrod i wonder if we should add compiletion time check for he spdk_bdev_open_ext() to preserve compatibility with older spdk versions    You mean  backporting  SPDK so we can use spdk_bdev_open() in case spdk_bdev_open_ext() doesn't exist?
1930,swx-jenkins3,Coverity failed. Check the report please: http://hpc-master.lab.mtl.com:8080/job/pr-nvmx-pipeline/1790/artifact/cov_build/html/index.html
1931,nitzancarmi,@Tom-Wu-2019 Please add compilation time check in autoconf for the existence of spdk_bdev_open_ext() as Alex suggested (just like we do backports on drivers). JD already wants to work with us  vs. SPDK v19.04  so we better be ready for such cases.    You can use AC_CHECK_FUNC() macro in m4/spdk.m4 (probably in the action-if-found section od AC_CHECK_LIB for spdk library).    Other than that - patch looks good.  
1932,Tom-Wu-2019,> @Tom-Wu-2019 Please add compilation time check in autoconf for the existence of spdk_bdev_open_ext() as Alex suggested (just like we do backports on drivers). JD already wants to work with us  vs. SPDK v19.04  so we better be ready for such cases.  >   > You can use AC_CHECK_FUNC() macro in m4/spdk.m4 (probably in the action-if-found section od AC_CHECK_LIB for spdk library).  >   > Other than that - patch looks good.    sure  I will add this kind of check. and thinks for your instruction.
1933,nitzancarmi,Looks good  I just wait for release to be over before merging this.
1934,Tom-Wu-2019,> Looks good  I just wait for release to be over before merging this.    wait a moment  I found HAVE_SPDK_BDEV_OPEN_EXT is not defined on spdk-20.04.1-17.aarch64. I am checking on this.
1935,Tom-Wu-2019,> Also please submit another PR that checks for bdev_open_ext in the virtio_blk controller.    sure will do it after this PR was merged.
1936,nitzancarmi,bot:retest
1937,nitzancarmi,Please just fix commit message so CI will pass.  The lines in the  tests  section are too long. :-)
1938,Tom-Wu-2019,I am wondering if we should replace 'bctx->bdev = spdk_bdev_desc_get_bdev(bctx->desc);' by 'bctx->bdev = spdk_bdev_get_by_name(attr->bdev_name);`  and move it to in front of spdk_bdev_open_ext/spdk_bedv_open.  Becasue  since we need to get the `bdev` anyway  and if we do this  it will reduce to only once to get `bdev` in `spdk_bedv_open` case.
1939,Tom-Wu-2019,> Note that soon I will push some change to snap-rdma  which allows virtio-blk resize option too. Then we should add handler here too.    good to know this  please told me after you did the push to snap-rdma  then I can add the resize event support to virtio-blk ctrl.
1940,alex-mikheev,:bot:retest
1941,alex-mikheev,bot:retest
1942,alex-mikheev,bot:retest
1943,alex-mikheev,:bot:retest
1944,alex-mikheev,bot:retest
1945,andrii-holovchenko,bot:retest
1946,alex-mikheev,bot:retest
1947,Tom-Wu-2019,I met a error during I use tests/fio_iops.sh script did the test  when use '--verify=md5' option to fio command  below error log can observed:  read: (g=0): rw=read  bs=(R) 4096B-4096B  (W) 4096B-4096B  (T) 4096B-4096B  ioengine=libaio  iodepth=512  fio-3.1  Starting 1 process  verify: bad magic header da7c  wanted acca at file /dev/nvme0n1 offset 0  length 4096  verify: bad magic header 1000  wanted acca at file /dev/nvme0n1 offset 4096  length 4096  read: No I/O performed by libaio  perhaps try --debug=io option for details?  fio: pid=163905  err=84/file:io_u.c:2030  func=io_u_queued_complete  error=Invalid or incomplete multibyte or wide character    read: (groupid=0  jobs=1): err=84 (file:io_u.c:2030  func=io_u_queued_complete  error=Invalid or incomplete multibyte or wide character): pid=163905: Fri Sep 25 09:45:59 2020     read: IOPS=102k  BW=1600KiB/s (1638kB/s)(8192B/5msec)      slat (nsec): min=3477  max=88444  avg=7579.32  stdev=11320.90      clat (usec): min=4097  max=4196  avg=4147.10  stdev=70.08       lat (usec): min=4123  max=4200  avg=4162.01  stdev=54.90      clat percentiles (usec):       |  1.00th=[ 4113]   5.00th=[ 4113]  10.00th=[ 4113]  20.00th=[ 4113]     when this error happened  no any error log came from SNAP or Host driver. I do not know what`s reason may cause to this issue. and expect this error  no other error observed during the test.
1948,Tom-Wu-2019,did a test with `sqe_only` mode  which will use async prp API  and got the same error during the tests/fio_iops.sh test.
1949,nitzancarmi,@mgerdts Please review this commit. It should solve the windows async events issue.
1950,nitzancarmi,@alex-mikheev  Done.
1951,nitzancarmi,will be part of larger windowd support PR
1952,nitzancarmi,bot:retest
1953,alex-mikheev,bot:retest
1954,alex-mikheev,:bot:retest
1955,andrii-holovchenko,bot:retest
1956,nitzancarmi,bot:retest
1957,nitzancarmi,bot:retest
1958,Tom-Wu-2019,> There are following problem with this approach:  >   > * spdk passthough requires a bdev that supports this functionality  > * passtrhough commands must be associated with the passthrough bdev when added. You cannot deduce nsid from the command itself  > * if passthrough is requested for commands such as create_cq or create_sq it will kill the controller.    to problem1  we can use spdk_bdev_io_type_supported(bdev  SPDK_BDEV_IO_TYPE_NVME_ADMIN) to chck if bdev is support passthrough  to problem2  it is a realy big problem to us. spdk_bdev_nvme_admin_passthru() API require a 'struct spdk_bdev_desc *' parameter  we need to get it from a bdev  and we only can get a bdev by nsid during handle the nvme admin/io request.  this is from my understanding. If we cannot get a nsid from a nvme admin command  then we need to maintain a  mapping between passthrough opcode and nsid(or bdev_name)  this kind of mapping only can build during set passthrough opcode by RPC.  But the user need to take the responsibility to do this.  to problem3  we can to prevent this by add a passthrough opcodes blacklist.  ---
1959,nitzancarmi,> > There are following problem with this approach:  > >   > > * spdk passthough requires a bdev that supports this functionality  > > * passtrhough commands must be associated with the passthrough bdev when added. You cannot deduce nsid from the command itself  > > * if passthrough is requested for commands such as create_cq or create_sq it will kill the controller.  >   >   to problem1  we can use spdk_bdev_io_type_supported(bdev  SPDK_BDEV_IO_TYPE_NVME_ADMIN) to chck if bdev is support passthrough  > to problem2  it is a realy big problem to us. spdk_bdev_nvme_admin_passthru() API require a 'struct spdk_bdev_desc *' parameter  we need to get it from a bdev  and we only can get a bdev by nsid during handle the nvme admin/io request.  > this is from my understanding. If we cannot get a nsid from a nvme admin command  then we need to maintain a  > mapping between passthrough opcode and nsid(or bdev_name)  this kind of mapping only can build during set passthrough opcode by RPC.  > But the user need to take the responsibility to do this.  > to problem3  we can to prevent this by add a passthrough opcodes blacklist.    You're right about problem  2. In snap we may aggregate multiple remote targets into single controller at host side  on the other side  I think most customers which will use that feature will use their own *single* remote target  so asking them to configure passthru commands per bdev might be a lot of work.    I think we can rely on the fact that NSID is a mandatory field in nvme command:  For invalid nsid - we fail.  For a valid nsid - send to that NSID only.  For special value 0xFFFFFFFF (broadcast) - send to ALL namespaces (and complete when ALL cqes are recieved).  For special value 0 (no nsid needed) - send to the first namespace only (since we assume there is only one controller) and raise a matching warning on the logs if num_ns > 1.    @alex-mikheev @Tom-Wu-2019 sounds good?
1960,Tom-Wu-2019,> > > There are following problem with this approach:  > > >   > > > * spdk passthough requires a bdev that supports this functionality  > > > * passtrhough commands must be associated with the passthrough bdev when added. You cannot deduce nsid from the command itself  > > > * if passthrough is requested for commands such as create_cq or create_sq it will kill the controller.  > >   > >   > >   to problem1  we can use spdk_bdev_io_type_supported(bdev  SPDK_BDEV_IO_TYPE_NVME_ADMIN) to chck if bdev is support passthrough  > > to problem2  it is a realy big problem to us. spdk_bdev_nvme_admin_passthru() API require a 'struct spdk_bdev_desc *' parameter  we need to get it from a bdev  and we only can get a bdev by nsid during handle the nvme admin/io request.  > > this is from my understanding. If we cannot get a nsid from a nvme admin command  then we need to maintain a  > > mapping between passthrough opcode and nsid(or bdev_name)  this kind of mapping only can build during set passthrough opcode by RPC.  > > But the user need to take the responsibility to do this.  > > to problem3  we can to prevent this by add a passthrough opcodes blacklist.  >   > You're right about problem  2. In snap we may aggregate multiple remote targets into single controller at host side  on the other side  I think most customers which will use that feature will use their own _single_ remote target  so asking them to configure passthru commands per bdev might be a lot of work.  >   > I think we can rely on the fact that NSID is a mandatory field in nvme command:  > For invalid nsid - we fail.  > For a valid nsid - send to that NSID only.  > For special value 0xFFFFFFFF (broadcast) - send to ALL namespaces (and complete when ALL cqes are recieved).  > For special value 0 (no nsid needed) - send to the first namespace only (since we assume there is only one controller) and raise a matching warning on the logs if num_ns > 1.  >   > @alex-mikheev @Tom-Wu-2019 sounds good?    I think it is a feasible solution  expect for 'special value 0' case.  NVMe Sepc speciflied 'If the namespace identifier is not used for the command  then this field shall be cleared to 0h'  I understand this as command with NSID 0 is target to controller not to namespace/bdev  and lots admin command will not  use NSID and set this value to 0  as Alex mentioned. if we treat nsid 0 as you suggestted  then we will passthrough all those  command with nsid 0 to first namespace un-anticipatory. I think we should treat nsid 0 as a blacklist passthrough opcode  not do passthrough for it  even user set it.  Aslo  we need to communicate with customer that nsid is mandatory field to any passthrough command.
1961,alex-mikheev,It is quite possible that the vendor defined admin command will use nsid for something else. This the reason why I prefer static mapping between pass - through command and its handler.  Let the spdk bdev handle nsid or the lack of it.
1962,nitzancarmi,bot:retest
1963,andrii-holovchenko,bot:retest
1964,andrii-holovchenko,bot:retest
1965,nitzancarmi,bot:retest
